{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leoqaz12/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "from keras import backend as K\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.activations import *\n",
    "from keras.callbacks import *\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from keras.utils import *\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras import *\n",
    "from keras.engine.topology import *\n",
    "from keras.optimizers import *\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sklearn\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** method 2 to read data with merge **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train = pd.read_csv('data/train_selected_features.csv')\n",
    "train = pd.read_csv('data/train_all_features.csv')\n",
    "predictBB = pd.read_csv('data/test_all_features.csv')\n",
    "ans = pd.read_csv('data/train_answers.csv')\n",
    "df = train.merge(ans,how='left',on='fid')\n",
    "df2 = predictBB.merge(ans,how='left',on='fid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##STACKING##\n",
    "# cv = KFold(len(df),n_folds = 10, shuffle = True)\n",
    "train_10=[]\n",
    "predict_10=[]\n",
    "predictBB_10 =[]\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "kf2 = KFold(n_splits=10, shuffle=True)\n",
    "count=1\n",
    "for (traincv, testcv),( _ ,testcv2) in zip(kf.split(df),kf2.split(df2)):\n",
    "    if count==1:\n",
    "        train = df.iloc[traincv]\n",
    "        predict = df.iloc[testcv]\n",
    "        train_10.append(train)\n",
    "        predict_10.append(predict)\n",
    "        pb = df2.iloc[testcv2]\n",
    "        predictBB_10.append(pb)\n",
    "        count +=1\n",
    "    elif count ==2:\n",
    "        train = df.iloc[traincv]\n",
    "        predict = df.iloc[testcv]\n",
    "        train_10.append(train)\n",
    "        predict_10.append(predict)\n",
    "        pb = df2.iloc[testcv2]\n",
    "        predictBB_10.append(pb)\n",
    "        count +=1\n",
    "    elif count ==10:\n",
    "        train = df.iloc[traincv]\n",
    "        predict = df.iloc[testcv]\n",
    "        train_10.append(train)\n",
    "        predict_10.append(predict)\n",
    "        pb = df2.iloc[testcv2]\n",
    "        predictBB_10.append(pb)\n",
    "    elif count ==3:\n",
    "        train = df.iloc[traincv]\n",
    "        predict = df.iloc[testcv]\n",
    "        train_10.append(train)\n",
    "        predict_10.append(predict)\n",
    "        pb = df2.iloc[testcv2]\n",
    "        predictBB_10.append(pb)\n",
    "        count +=1\n",
    "    elif count ==4:\n",
    "        train = df.iloc[traincv]\n",
    "        predict = df.iloc[testcv]\n",
    "        train_10.append(train)\n",
    "        predict_10.append(predict)\n",
    "        pb = df2.iloc[testcv2]\n",
    "        predictBB_10.append(pb)\n",
    "        count +=1\n",
    "    elif count ==5:\n",
    "        train = df.iloc[traincv]\n",
    "        predict = df.iloc[testcv]\n",
    "        train_10.append(train)\n",
    "        predict_10.append(predict)\n",
    "        pb = df2.iloc[testcv2]\n",
    "        predictBB_10.append(pb)\n",
    "        count +=1\n",
    "    elif count ==6:\n",
    "        train = df.iloc[traincv]\n",
    "        predict = df.iloc[testcv]\n",
    "        train_10.append(train)\n",
    "        predict_10.append(predict)\n",
    "        pb = df2.iloc[testcv2]\n",
    "        predictBB_10.append(pb)\n",
    "        count +=1\n",
    "    elif count ==7:\n",
    "        train = df.iloc[traincv]\n",
    "        predict = df.iloc[testcv]\n",
    "        train_10.append(train)\n",
    "        predict_10.append(predict)\n",
    "        pb = df2.iloc[testcv2]\n",
    "        predictBB_10.append(pb)\n",
    "        count +=1\n",
    "    elif count ==8:\n",
    "        train = df.iloc[traincv]\n",
    "        predict = df.iloc[testcv]\n",
    "        train_10.append(train)\n",
    "        predict_10.append(predict)\n",
    "        pb = df2.iloc[testcv2]\n",
    "        predictBB_10.append(pb)\n",
    "        count +=1\n",
    "    elif count ==9:\n",
    "        train = df.iloc[traincv]\n",
    "        predict = df.iloc[testcv]\n",
    "        train_10.append(train)\n",
    "        predict_10.append(predict)\n",
    "        pb = df2.iloc[testcv2]\n",
    "        predictBB_10.append(pb)\n",
    "        count +=1\n",
    "    \n",
    "#     train = df.iloc[testcv]\n",
    "#     print(train)\n",
    "\n",
    "# print(len(train_10))\n",
    "# print(len(predict_10))\n",
    "# kf = KFold(n_splits = 10, shuffle = True, random_state = 2)\n",
    "\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##STACKING##\n",
    "def model_best(X_T,y_T,x_valid,y_valid):    \n",
    "    model = Sequential()\n",
    "    '''\n",
    "    model.add(Conv1D(16,2,padding='same',input_shape=(1,658)))\n",
    "    model.add(Conv1D(32,2,padding='same'))\n",
    "    model.add(Conv1D(32,2,padding='same'))\n",
    "    model.add(Conv1D(64,2,padding='same'))\n",
    "    model.add(Conv1D(64,2,padding='same'))  #AF?\n",
    "    model.add(Flatten())\n",
    "    '''\n",
    "    #dropout 0.001 0.5 decay\n",
    "    # l2: 0.0001~0.01 decay\n",
    "    model.add(Dense(8192,kernel_initializer='lecun_normal',input_dim=827,kernel_regularizer=regularizers.l2(0.00001))) #, kernel_regularizer=regularizers.l2(0.0001)\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('selu'))\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero'))\n",
    "    model.add(Dense(4096,kernel_initializer='lecun_normal', kernel_regularizer=regularizers.l2(0.00001)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Activation('selu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dense(2048,kernel_initializer='lecun_normal', kernel_regularizer=regularizers.l2(0.00001)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Activation('selu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero'))\n",
    "    model.add(Dense(1024,kernel_initializer='lecun_normal', kernel_regularizer=regularizers.l2(0.00001)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Activation('selu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Dense(512,kernel_initializer='lecun_normal', kernel_regularizer=regularizers.l2(0.00001))) #,input_dim = 442\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Activation('selu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero')) #, weights=None\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    # model.add(Dense(512,activation='relu',kernel_initializer='lecun_uniform'))#,activation='relu'))\n",
    "    model.add(Dense(256,kernel_initializer='lecun_normal', kernel_regularizer=regularizers.l2(0.00001)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Activation('selu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    # model.add(Dense(256,activation='relu',kernel_initializer='lecun_uniform'))\n",
    "    model.add(Dense(128,kernel_initializer='lecun_normal', kernel_regularizer=regularizers.l2(0.00001)))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Activation('selu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Dense(64,kernel_initializer='uniform', kernel_regularizer=regularizers.l2(0.00001)))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Dense(32,kernel_initializer='uniform'))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Activation('relu'))\n",
    "    # model.add(Dropout(0.005))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Dense(16,kernel_initializer='uniform'))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Activation('relu'))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Dense(8,kernel_initializer='uniform'))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Activation('relu'))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Dense(4,kernel_initializer='uniform'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    # model.add(Dropout(0.2))\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Dense(2,kernel_initializer='uniform'))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Activation('relu'))\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    # model.add(Dense(128,kernel_initializer='lecun_uniform'))\n",
    "    # model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "    model.add(Dense(1,kernel_initializer='uniform')) #,activation='sigmoid\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('sigmoid'))\n",
    "    ADAM = Adam()\n",
    "    model.compile(loss='binary_crossentropy', optimizer=ADAM, metrics=['accuracy']) #binary_crossentropy\n",
    "#     model.summary()\n",
    "    \n",
    "    batch_size = 32#128 \n",
    "    early_stopping_monitor = EarlyStopping(patience=5) #,monitor='val_acc',mode='auto' #15\n",
    "    # keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "    callback = GetBest(monitor='val_loss', verbose=1, mode='auto')\n",
    "    # CW = {0:8.312,1:1.}\n",
    "    model.fit(X_T, y_T, batch_size=batch_size, epochs=500, validation_data=(x_valid,y_valid), callbacks=[callback,early_stopping_monitor],shuffle=True)#,class_weight=CW)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "class GetBest(Callback):\n",
    "    \"\"\"Get the best model at the end of training.\n",
    "\t# Arguments\n",
    "        monitor: quantity to monitor.\n",
    "        verbose: verbosity mode, 0 or 1.\n",
    "        mode: one of {auto, min, max}.\n",
    "            The decision\n",
    "            to overwrite the current stored weights is made\n",
    "            based on either the maximization or the\n",
    "            minimization of the monitored quantity. For `val_acc`,\n",
    "            this should be `max`, for `val_loss` this should\n",
    "            be `min`, etc. In `auto` mode, the direction is\n",
    "            automatically inferred from the name of the monitored quantity.\n",
    "        period: Interval (number of epochs) between checkpoints.\n",
    "\t# Example\n",
    "\t\tcallbacks = [GetBest(monitor='val_acc', verbose=1, mode='max')]\n",
    "\t\tmode.fit(X, y, validation_data=(X_eval, Y_eval),\n",
    "                 callbacks=callbacks)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, monitor='val_loss', verbose=0,\n",
    "                 mode='auto', period=1):\n",
    "        super(GetBest, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.verbose = verbose\n",
    "        self.period = period\n",
    "        self.best_epochs = 0\n",
    "        self.epochs_since_last_save = 0\n",
    "\n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn('GetBest mode %s is unknown, '\n",
    "                          'fallback to auto mode.' % (mode),\n",
    "                          RuntimeWarning)\n",
    "            mode = 'auto'\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
    "                self.monitor_op = np.greater\n",
    "                self.best = -np.Inf\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "                self.best = np.Inf\n",
    "                \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epochs_since_last_save += 1\n",
    "        if self.epochs_since_last_save >= self.period:\n",
    "            self.epochs_since_last_save = 0\n",
    "            #filepath = self.filepath.format(epoch=epoch + 1, **logs)\n",
    "            current = logs.get(self.monitor)\n",
    "            if current is None:\n",
    "                warnings.warn('Can pick best model only with %s available, '\n",
    "                              'skipping.' % (self.monitor), RuntimeWarning)\n",
    "            else:\n",
    "                if self.monitor_op(current, self.best):\n",
    "                    if self.verbose > 0:\n",
    "                        print('\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n",
    "                              ' storing weights.'\n",
    "                              % (epoch + 1, self.monitor, self.best,\n",
    "                                 current))\n",
    "                    self.best = current\n",
    "                    self.best_epochs = epoch + 1\n",
    "                    self.best_weights = self.model.get_weights()\n",
    "                else:\n",
    "                    if self.verbose > 0:\n",
    "                        print('\\nEpoch %05d: %s did not improve' %\n",
    "                              (epoch + 1, self.monitor))            \n",
    "                    \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.verbose > 0:\n",
    "            print('Using epoch %05d with %s: %0.5f' % (self.best_epochs, self.monitor,\n",
    "                                                       self.best))\n",
    "        self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #SAVE##\n",
    "# with open('history/train_10.pkl','wb') as f:\n",
    "#     pickle.dump(train_10,f)\n",
    "# f.close()\n",
    "# with open('history/predict_10.pkl','wb') as f:\n",
    "#     pickle.dump(predict_10,f)\n",
    "# f.close()\n",
    "# with open('history/predictBB_10.pkl','wb') as f:\n",
    "#     pickle.dump(predictBB_10,f)\n",
    "# f.close()\n",
    "# allAnswer = pd.DataFrame()\n",
    "# iterC=0 \n",
    "\n",
    "##RESUME##\n",
    "with open('history/train_10.pkl','rb') as f:\n",
    "    train_10 = pickle.load(f)\n",
    "f.close()\n",
    "with open('history/predict_10.pkl','rb') as f:\n",
    "    predict_10 = pickle.load(f)\n",
    "f.close()\n",
    "with open('history/predictBB_10.pkl','rb') as f:\n",
    "    predictBB_10 = pickle.load(f)\n",
    "f.close()\n",
    "allAnswer = pd.read_pickle('history/allAnswer.pkl')\n",
    "iterC=2 #set to 0~9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 74620 samples, validate on 4727 samples\n",
      "Epoch 1/500\n",
      "74620/74620 [==============================] - 91s 1ms/step - loss: 0.5630 - acc: 0.7940 - val_loss: 0.4926 - val_acc: 0.8147\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.49259, storing weights.\n",
      "Epoch 2/500\n",
      "74620/74620 [==============================] - 89s 1ms/step - loss: 0.4791 - acc: 0.8189 - val_loss: 0.4203 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.49259 to 0.42029, storing weights.\n",
      "Epoch 3/500\n",
      "74620/74620 [==============================] - 89s 1ms/step - loss: 0.4696 - acc: 0.8236 - val_loss: 0.4530 - val_acc: 0.8517\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/500\n",
      "74620/74620 [==============================] - 89s 1ms/step - loss: 0.4709 - acc: 0.8274 - val_loss: 0.4685 - val_acc: 0.8710\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/500\n",
      "74620/74620 [==============================] - 89s 1ms/step - loss: 0.4619 - acc: 0.8326 - val_loss: 0.4531 - val_acc: 0.8504\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/500\n",
      "74620/74620 [==============================] - 90s 1ms/step - loss: 0.4326 - acc: 0.8460 - val_loss: 0.4359 - val_acc: 0.8145\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/500\n",
      "74620/74620 [==============================] - 89s 1ms/step - loss: 0.4088 - acc: 0.8557 - val_loss: 0.3810 - val_acc: 0.8735\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.42029 to 0.38104, storing weights.\n",
      "Epoch 8/500\n",
      "74620/74620 [==============================] - 90s 1ms/step - loss: 0.3882 - acc: 0.8654 - val_loss: 0.3405 - val_acc: 0.8860\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.38104 to 0.34047, storing weights.\n",
      "Epoch 9/500\n",
      "74620/74620 [==============================] - 89s 1ms/step - loss: 0.3714 - acc: 0.8729 - val_loss: 0.4218 - val_acc: 0.8234\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/500\n",
      "74620/74620 [==============================] - 90s 1ms/step - loss: 0.3555 - acc: 0.8814 - val_loss: 0.4622 - val_acc: 0.8007\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/500\n",
      "74620/74620 [==============================] - 89s 1ms/step - loss: 0.3455 - acc: 0.8887 - val_loss: 0.3210 - val_acc: 0.9150\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.34047 to 0.32100, storing weights.\n",
      "Epoch 12/500\n",
      "74620/74620 [==============================] - 90s 1ms/step - loss: 0.3362 - acc: 0.8960 - val_loss: 0.4573 - val_acc: 0.8187\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/500\n",
      "74620/74620 [==============================] - 90s 1ms/step - loss: 0.3267 - acc: 0.9023 - val_loss: 0.2971 - val_acc: 0.9063\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.32100 to 0.29715, storing weights.\n",
      "Epoch 14/500\n",
      "74620/74620 [==============================] - 90s 1ms/step - loss: 0.3222 - acc: 0.9062 - val_loss: 0.2974 - val_acc: 0.9166\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/500\n",
      "74620/74620 [==============================] - 90s 1ms/step - loss: 0.3229 - acc: 0.9104 - val_loss: 0.3791 - val_acc: 0.8820\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/500\n",
      "74620/74620 [==============================] - 90s 1ms/step - loss: 0.3210 - acc: 0.9142 - val_loss: 0.3071 - val_acc: 0.9213\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/500\n",
      "74620/74620 [==============================] - 90s 1ms/step - loss: 0.3275 - acc: 0.9140 - val_loss: 0.4371 - val_acc: 0.8576\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/500\n",
      "74620/74620 [==============================] - 90s 1ms/step - loss: 0.3116 - acc: 0.9208 - val_loss: 0.3171 - val_acc: 0.9217\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Using epoch 00013 with val_loss: 0.29715\n",
      "=============DONE: 2 ===============need Set iterC to: 3\n",
      "0\n",
      "Train on 74830 samples, validate on 4727 samples\n",
      "Epoch 1/500\n",
      "74830/74830 [==============================] - 92s 1ms/step - loss: 0.5620 - acc: 0.7864 - val_loss: 0.6037 - val_acc: 0.7360\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.60366, storing weights.\n",
      "Epoch 2/500\n",
      "74830/74830 [==============================] - 90s 1ms/step - loss: 0.4817 - acc: 0.8094 - val_loss: 0.4203 - val_acc: 0.8684\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.60366 to 0.42033, storing weights.\n",
      "Epoch 3/500\n",
      "74830/74830 [==============================] - 90s 1ms/step - loss: 0.4700 - acc: 0.8184 - val_loss: 0.3958 - val_acc: 0.8832\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.42033 to 0.39582, storing weights.\n",
      "Epoch 4/500\n",
      "74830/74830 [==============================] - 90s 1ms/step - loss: 0.4620 - acc: 0.8275 - val_loss: 0.3948 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.39582 to 0.39485, storing weights.\n",
      "Epoch 5/500\n",
      "74830/74830 [==============================] - 90s 1ms/step - loss: 0.4410 - acc: 0.8403 - val_loss: 0.4107 - val_acc: 0.8570\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/500\n",
      "74830/74830 [==============================] - 90s 1ms/step - loss: 0.4154 - acc: 0.8498 - val_loss: 0.4034 - val_acc: 0.8682\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/500\n",
      "74830/74830 [==============================] - 90s 1ms/step - loss: 0.3907 - acc: 0.8610 - val_loss: 0.4160 - val_acc: 0.8212\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/500\n",
      "74830/74830 [==============================] - 90s 1ms/step - loss: 0.3741 - acc: 0.8690 - val_loss: 0.3513 - val_acc: 0.8593\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.39485 to 0.35126, storing weights.\n",
      "Epoch 9/500\n",
      "74830/74830 [==============================] - 90s 1ms/step - loss: 0.3608 - acc: 0.8761 - val_loss: 0.2687 - val_acc: 0.9336\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35126 to 0.26874, storing weights.\n",
      "Epoch 10/500\n",
      "74830/74830 [==============================] - 90s 1ms/step - loss: 0.3508 - acc: 0.8825 - val_loss: 0.3232 - val_acc: 0.8856\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/500\n",
      "74830/74830 [==============================] - 90s 1ms/step - loss: 0.3376 - acc: 0.8908 - val_loss: 0.3189 - val_acc: 0.8915\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/500\n",
      "74830/74830 [==============================] - 90s 1ms/step - loss: 0.3337 - acc: 0.8944 - val_loss: 0.3725 - val_acc: 0.8572\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/500\n",
      "74830/74830 [==============================] - 90s 1ms/step - loss: 0.3275 - acc: 0.8988 - val_loss: 0.3823 - val_acc: 0.8439\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/500\n",
      "74830/74830 [==============================] - 90s 1ms/step - loss: 0.3290 - acc: 0.9007 - val_loss: 0.4079 - val_acc: 0.8470\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Using epoch 00009 with val_loss: 0.26874\n",
      "=============DONE: 3 ===============need Set iterC to: 4\n",
      "0\n",
      "Train on 74277 samples, validate on 4727 samples\n",
      "Epoch 1/500\n",
      "74277/74277 [==============================] - 92s 1ms/step - loss: 0.5747 - acc: 0.7872 - val_loss: 0.4615 - val_acc: 0.8847\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.46146, storing weights.\n",
      "Epoch 2/500\n",
      "74277/74277 [==============================] - 90s 1ms/step - loss: 0.4856 - acc: 0.8133 - val_loss: 0.5075 - val_acc: 0.8157\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/500\n",
      "74277/74277 [==============================] - 90s 1ms/step - loss: 0.4678 - acc: 0.8206 - val_loss: 0.4217 - val_acc: 0.8710\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.46146 to 0.42171, storing weights.\n",
      "Epoch 4/500\n",
      "74277/74277 [==============================] - 90s 1ms/step - loss: 0.4587 - acc: 0.8295 - val_loss: 0.4347 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/500\n",
      "74277/74277 [==============================] - 90s 1ms/step - loss: 0.4434 - acc: 0.8390 - val_loss: 0.3768 - val_acc: 0.8938\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.42171 to 0.37677, storing weights.\n",
      "Epoch 6/500\n",
      "74277/74277 [==============================] - 90s 1ms/step - loss: 0.4184 - acc: 0.8491 - val_loss: 0.4312 - val_acc: 0.8701\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/500\n",
      "74277/74277 [==============================] - 90s 1ms/step - loss: 0.3970 - acc: 0.8590 - val_loss: 0.4001 - val_acc: 0.8619\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/500\n",
      "74277/74277 [==============================] - 90s 1ms/step - loss: 0.3811 - acc: 0.8669 - val_loss: 0.3739 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.37677 to 0.37389, storing weights.\n",
      "Epoch 9/500\n",
      "74277/74277 [==============================] - 90s 1ms/step - loss: 0.3596 - acc: 0.8747 - val_loss: 0.3725 - val_acc: 0.8623\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.37389 to 0.37248, storing weights.\n",
      "Epoch 10/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74277/74277 [==============================] - 90s 1ms/step - loss: 0.3486 - acc: 0.8805 - val_loss: 0.5451 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/500\n",
      "74277/74277 [==============================] - 90s 1ms/step - loss: 0.3389 - acc: 0.8879 - val_loss: 0.3058 - val_acc: 0.9021\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.37248 to 0.30576, storing weights.\n",
      "Epoch 12/500\n",
      "74277/74277 [==============================] - 90s 1ms/step - loss: 0.3338 - acc: 0.8914 - val_loss: 0.3472 - val_acc: 0.9029\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/500\n",
      "74277/74277 [==============================] - 90s 1ms/step - loss: 0.3262 - acc: 0.8951 - val_loss: 0.4537 - val_acc: 0.8456\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/500\n",
      "74277/74277 [==============================] - 90s 1ms/step - loss: 0.3187 - acc: 0.9006 - val_loss: 0.3543 - val_acc: 0.8940\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/500\n",
      "74277/74277 [==============================] - 90s 1ms/step - loss: 0.3181 - acc: 0.9037 - val_loss: 0.3229 - val_acc: 0.9033\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/500\n",
      "74277/74277 [==============================] - 90s 1ms/step - loss: 0.3230 - acc: 0.9042 - val_loss: 0.3247 - val_acc: 0.9023\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Using epoch 00011 with val_loss: 0.30576\n",
      "=============DONE: 4 ===============need Set iterC to: 5\n",
      "0\n",
      "Train on 74438 samples, validate on 4727 samples\n",
      "Epoch 1/500\n",
      "74438/74438 [==============================] - 92s 1ms/step - loss: 0.5511 - acc: 0.7936 - val_loss: 0.4742 - val_acc: 0.8248\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.47418, storing weights.\n",
      "Epoch 2/500\n",
      "74438/74438 [==============================] - 90s 1ms/step - loss: 0.4772 - acc: 0.8129 - val_loss: 0.4571 - val_acc: 0.8073\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.47418 to 0.45710, storing weights.\n",
      "Epoch 3/500\n",
      "74438/74438 [==============================] - 90s 1ms/step - loss: 0.4681 - acc: 0.8189 - val_loss: 0.4402 - val_acc: 0.8132\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.45710 to 0.44019, storing weights.\n",
      "Epoch 4/500\n",
      "74438/74438 [==============================] - 90s 1ms/step - loss: 0.4605 - acc: 0.8273 - val_loss: 0.4238 - val_acc: 0.8765\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.44019 to 0.42381, storing weights.\n",
      "Epoch 5/500\n",
      "74438/74438 [==============================] - 90s 1ms/step - loss: 0.4405 - acc: 0.8360 - val_loss: 0.4596 - val_acc: 0.8212\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/500\n",
      "74438/74438 [==============================] - 90s 1ms/step - loss: 0.4150 - acc: 0.8463 - val_loss: 0.4586 - val_acc: 0.8659\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/500\n",
      "74438/74438 [==============================] - 90s 1ms/step - loss: 0.3952 - acc: 0.8545 - val_loss: 0.3690 - val_acc: 0.8691\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.42381 to 0.36905, storing weights.\n",
      "Epoch 8/500\n",
      "74438/74438 [==============================] - 90s 1ms/step - loss: 0.3780 - acc: 0.8625 - val_loss: 0.4003 - val_acc: 0.8468\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/500\n",
      "74438/74438 [==============================] - 90s 1ms/step - loss: 0.3642 - acc: 0.8719 - val_loss: 0.3818 - val_acc: 0.8580\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/500\n",
      "74438/74438 [==============================] - 90s 1ms/step - loss: 0.3524 - acc: 0.8790 - val_loss: 0.3024 - val_acc: 0.9037\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.36905 to 0.30243, storing weights.\n",
      "Epoch 11/500\n",
      "74438/74438 [==============================] - 90s 1ms/step - loss: 0.3447 - acc: 0.8831 - val_loss: 0.3650 - val_acc: 0.8729\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/500\n",
      "74438/74438 [==============================] - 90s 1ms/step - loss: 0.3325 - acc: 0.8911 - val_loss: 0.3244 - val_acc: 0.8691\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/500\n",
      "74438/74438 [==============================] - 90s 1ms/step - loss: 0.3261 - acc: 0.8967 - val_loss: 0.3269 - val_acc: 0.8801\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/500\n",
      "74438/74438 [==============================] - 89s 1ms/step - loss: 0.3208 - acc: 0.8991 - val_loss: 0.4000 - val_acc: 0.8790\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/500\n",
      "74438/74438 [==============================] - 89s 1ms/step - loss: 0.3187 - acc: 0.9029 - val_loss: 0.3111 - val_acc: 0.9029\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Using epoch 00010 with val_loss: 0.30243\n",
      "=============DONE: 5 ===============need Set iterC to: 6\n",
      "0\n",
      "Train on 74382 samples, validate on 4727 samples\n",
      "Epoch 1/500\n",
      "74382/74382 [==============================] - 91s 1ms/step - loss: 0.5635 - acc: 0.7887 - val_loss: 0.4196 - val_acc: 0.8716\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.41961, storing weights.\n",
      "Epoch 2/500\n",
      "74382/74382 [==============================] - 89s 1ms/step - loss: 0.4816 - acc: 0.8171 - val_loss: 0.4142 - val_acc: 0.8671\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.41961 to 0.41424, storing weights.\n",
      "Epoch 3/500\n",
      "74382/74382 [==============================] - 89s 1ms/step - loss: 0.4710 - acc: 0.8221 - val_loss: 0.4179 - val_acc: 0.8559\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/500\n",
      "74382/74382 [==============================] - 89s 1ms/step - loss: 0.4692 - acc: 0.8285 - val_loss: 0.5204 - val_acc: 0.7874\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/500\n",
      "74382/74382 [==============================] - 89s 1ms/step - loss: 0.4534 - acc: 0.8383 - val_loss: 0.4321 - val_acc: 0.8555\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/500\n",
      "74382/74382 [==============================] - 89s 1ms/step - loss: 0.4321 - acc: 0.8481 - val_loss: 0.3540 - val_acc: 0.8638\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.41424 to 0.35402, storing weights.\n",
      "Epoch 7/500\n",
      "74382/74382 [==============================] - 89s 1ms/step - loss: 0.4027 - acc: 0.8592 - val_loss: 0.3851 - val_acc: 0.8697\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/500\n",
      "74382/74382 [==============================] - 89s 1ms/step - loss: 0.3798 - acc: 0.8687 - val_loss: 0.4115 - val_acc: 0.8282\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/500\n",
      "74382/74382 [==============================] - 89s 1ms/step - loss: 0.3661 - acc: 0.8764 - val_loss: 0.4116 - val_acc: 0.8212\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/500\n",
      "74382/74382 [==============================] - 89s 1ms/step - loss: 0.3495 - acc: 0.8834 - val_loss: 0.3351 - val_acc: 0.9025\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.35402 to 0.33510, storing weights.\n",
      "Epoch 11/500\n",
      "74382/74382 [==============================] - 90s 1ms/step - loss: 0.3380 - acc: 0.8907 - val_loss: 0.3262 - val_acc: 0.8902\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.33510 to 0.32622, storing weights.\n",
      "Epoch 12/500\n",
      "74382/74382 [==============================] - 89s 1ms/step - loss: 0.3293 - acc: 0.8980 - val_loss: 0.3977 - val_acc: 0.8557\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/500\n",
      "74382/74382 [==============================] - 89s 1ms/step - loss: 0.3232 - acc: 0.9010 - val_loss: 0.4270 - val_acc: 0.8316\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/500\n",
      "74382/74382 [==============================] - 89s 1ms/step - loss: 0.3174 - acc: 0.9052 - val_loss: 0.3404 - val_acc: 0.8898\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/500\n",
      "74382/74382 [==============================] - 89s 1ms/step - loss: 0.3137 - acc: 0.9113 - val_loss: 0.3501 - val_acc: 0.8987\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/500\n",
      "74382/74382 [==============================] - 89s 1ms/step - loss: 0.3065 - acc: 0.9159 - val_loss: 0.3943 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Using epoch 00011 with val_loss: 0.32622\n",
      "=============DONE: 6 ===============need Set iterC to: 7\n",
      "0\n",
      "Train on 74718 samples, validate on 4727 samples\n",
      "Epoch 1/500\n",
      "74718/74718 [==============================] - 92s 1ms/step - loss: 0.5617 - acc: 0.7918 - val_loss: 0.5303 - val_acc: 0.7925\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53031, storing weights.\n",
      "Epoch 2/500\n",
      "74718/74718 [==============================] - 90s 1ms/step - loss: 0.4786 - acc: 0.8164 - val_loss: 0.4985 - val_acc: 0.8661\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.53031 to 0.49850, storing weights.\n",
      "Epoch 3/500\n",
      "74718/74718 [==============================] - 90s 1ms/step - loss: 0.4677 - acc: 0.8208 - val_loss: 0.4698 - val_acc: 0.8327\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.49850 to 0.46980, storing weights.\n",
      "Epoch 4/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74718/74718 [==============================] - 90s 1ms/step - loss: 0.4647 - acc: 0.8270 - val_loss: 0.5215 - val_acc: 0.8411\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/500\n",
      "74718/74718 [==============================] - 90s 1ms/step - loss: 0.4433 - acc: 0.8385 - val_loss: 0.4591 - val_acc: 0.8896\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.46980 to 0.45907, storing weights.\n",
      "Epoch 6/500\n",
      "74718/74718 [==============================] - 90s 1ms/step - loss: 0.4196 - acc: 0.8480 - val_loss: 0.4101 - val_acc: 0.8466\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.45907 to 0.41007, storing weights.\n",
      "Epoch 7/500\n",
      "74718/74718 [==============================] - 90s 1ms/step - loss: 0.3962 - acc: 0.8575 - val_loss: 0.4224 - val_acc: 0.8517\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/500\n",
      "74718/74718 [==============================] - 90s 1ms/step - loss: 0.3724 - acc: 0.8687 - val_loss: 0.3304 - val_acc: 0.8773\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.41007 to 0.33041, storing weights.\n",
      "Epoch 9/500\n",
      "74718/74718 [==============================] - 90s 1ms/step - loss: 0.3572 - acc: 0.8777 - val_loss: 0.3249 - val_acc: 0.8832\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.33041 to 0.32486, storing weights.\n",
      "Epoch 10/500\n",
      "74718/74718 [==============================] - 90s 1ms/step - loss: 0.3453 - acc: 0.8848 - val_loss: 0.3923 - val_acc: 0.8862\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/500\n",
      "74718/74718 [==============================] - 90s 1ms/step - loss: 0.3348 - acc: 0.8918 - val_loss: 0.3446 - val_acc: 0.8972\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/500\n",
      "74718/74718 [==============================] - 90s 1ms/step - loss: 0.3277 - acc: 0.8975 - val_loss: 0.3430 - val_acc: 0.8722\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/500\n",
      "74718/74718 [==============================] - 90s 1ms/step - loss: 0.3165 - acc: 0.9027 - val_loss: 0.3488 - val_acc: 0.8853\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/500\n",
      "74718/74718 [==============================] - 90s 1ms/step - loss: 0.3111 - acc: 0.9081 - val_loss: 0.3472 - val_acc: 0.8944\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Using epoch 00009 with val_loss: 0.32486\n",
      "=============DONE: 7 ===============need Set iterC to: 8\n",
      "0\n",
      "Train on 74327 samples, validate on 4727 samples\n",
      "Epoch 1/500\n",
      "74327/74327 [==============================] - 92s 1ms/step - loss: 0.5513 - acc: 0.7934 - val_loss: 0.4894 - val_acc: 0.8468\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.48938, storing weights.\n",
      "Epoch 2/500\n",
      "74327/74327 [==============================] - 90s 1ms/step - loss: 0.4736 - acc: 0.8176 - val_loss: 0.4694 - val_acc: 0.8352\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.48938 to 0.46940, storing weights.\n",
      "Epoch 3/500\n",
      "74327/74327 [==============================] - 90s 1ms/step - loss: 0.4651 - acc: 0.8241 - val_loss: 0.4456 - val_acc: 0.7959\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.46940 to 0.44560, storing weights.\n",
      "Epoch 4/500\n",
      "74327/74327 [==============================] - 90s 1ms/step - loss: 0.4501 - acc: 0.8349 - val_loss: 0.5156 - val_acc: 0.8333\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/500\n",
      "74327/74327 [==============================] - 90s 1ms/step - loss: 0.4267 - acc: 0.8443 - val_loss: 0.3386 - val_acc: 0.9014\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.44560 to 0.33863, storing weights.\n",
      "Epoch 6/500\n",
      "74327/74327 [==============================] - 90s 1ms/step - loss: 0.4035 - acc: 0.8557 - val_loss: 0.3752 - val_acc: 0.8693\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/500\n",
      "74327/74327 [==============================] - 89s 1ms/step - loss: 0.3832 - acc: 0.8642 - val_loss: 0.5633 - val_acc: 0.7652\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/500\n",
      "74327/74327 [==============================] - 90s 1ms/step - loss: 0.3667 - acc: 0.8731 - val_loss: 0.5213 - val_acc: 0.7766\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/500\n",
      "74327/74327 [==============================] - 90s 1ms/step - loss: 0.3511 - acc: 0.8811 - val_loss: 0.2815 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.33863 to 0.28152, storing weights.\n",
      "Epoch 10/500\n",
      "74327/74327 [==============================] - 90s 1ms/step - loss: 0.3432 - acc: 0.8866 - val_loss: 0.3650 - val_acc: 0.8807\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/500\n",
      "74327/74327 [==============================] - 90s 1ms/step - loss: 0.3320 - acc: 0.8952 - val_loss: 0.3761 - val_acc: 0.8705\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/500\n",
      "74327/74327 [==============================] - 90s 1ms/step - loss: 0.3220 - acc: 0.8987 - val_loss: 0.3273 - val_acc: 0.8985\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/500\n",
      "74327/74327 [==============================] - 90s 1ms/step - loss: 0.3144 - acc: 0.9043 - val_loss: 0.3280 - val_acc: 0.8919\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/500\n",
      "74327/74327 [==============================] - 90s 1ms/step - loss: 0.3095 - acc: 0.9095 - val_loss: 0.3366 - val_acc: 0.8966\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Using epoch 00009 with val_loss: 0.28152\n",
      "=============DONE: 8 ===============need Set iterC to: 9\n",
      "0\n",
      "Train on 74173 samples, validate on 4727 samples\n",
      "Epoch 1/500\n",
      "74173/74173 [==============================] - 92s 1ms/step - loss: 0.5728 - acc: 0.7808 - val_loss: 0.5041 - val_acc: 0.7859\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.50408, storing weights.\n",
      "Epoch 2/500\n",
      "74173/74173 [==============================] - 90s 1ms/step - loss: 0.4895 - acc: 0.8083 - val_loss: 0.4617 - val_acc: 0.8195\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.50408 to 0.46169, storing weights.\n",
      "Epoch 3/500\n",
      "74173/74173 [==============================] - 90s 1ms/step - loss: 0.4736 - acc: 0.8133 - val_loss: 0.4487 - val_acc: 0.8134\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.46169 to 0.44867, storing weights.\n",
      "Epoch 4/500\n",
      "74173/74173 [==============================] - 90s 1ms/step - loss: 0.4665 - acc: 0.8215 - val_loss: 0.4808 - val_acc: 0.8794\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/500\n",
      "74173/74173 [==============================] - 90s 1ms/step - loss: 0.4509 - acc: 0.8319 - val_loss: 0.5623 - val_acc: 0.7557\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/500\n",
      "74173/74173 [==============================] - 90s 1ms/step - loss: 0.4268 - acc: 0.8399 - val_loss: 0.4222 - val_acc: 0.8257\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.44867 to 0.42218, storing weights.\n",
      "Epoch 7/500\n",
      "74173/74173 [==============================] - 90s 1ms/step - loss: 0.4065 - acc: 0.8465 - val_loss: 0.3747 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.42218 to 0.37470, storing weights.\n",
      "Epoch 8/500\n",
      "74173/74173 [==============================] - 90s 1ms/step - loss: 0.3869 - acc: 0.8566 - val_loss: 0.3866 - val_acc: 0.9099\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/500\n",
      "74173/74173 [==============================] - 90s 1ms/step - loss: 0.3804 - acc: 0.8626 - val_loss: 0.3807 - val_acc: 0.8724\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/500\n",
      "74173/74173 [==============================] - 90s 1ms/step - loss: 0.3693 - acc: 0.8674 - val_loss: 0.5104 - val_acc: 0.8107\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/500\n",
      "74173/74173 [==============================] - 90s 1ms/step - loss: 0.3621 - acc: 0.8740 - val_loss: 0.4113 - val_acc: 0.8435\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/500\n",
      "74173/74173 [==============================] - 90s 1ms/step - loss: 0.3540 - acc: 0.8789 - val_loss: 0.4140 - val_acc: 0.8468\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Using epoch 00007 with val_loss: 0.37470\n",
      "=============DONE: 9 ===============need Set iterC to: 10\n",
      "0\n",
      "finish with fid:  81894\n"
     ]
    }
   ],
   "source": [
    "##STACKING##\n",
    "\n",
    "\n",
    "# iterC=0\n",
    "for df,dfp,predictB in zip(train_10[iterC::],predict_10[iterC::],predictBB_10[iterC::]):\n",
    "    predict_Best = predictB.drop(columns=['fid','ans'])\n",
    "    predict = dfp.drop(columns=['fid','ans'])\n",
    "    predictN = dfp.filter(['fid'])\n",
    "    predictN = predictN.reset_index(drop=True)\n",
    "    predictNN = predictB.filter(['fid'])\n",
    "    predictNN = predictNN.reset_index(drop=True)\n",
    "    train = df.drop(columns=['fid','ans'])\n",
    "    ans = df.filter(['ans'])\n",
    "    ans_p = dfp.filter(['ans'])\n",
    "    P_B = np.array(pd.concat([predict_Best],axis=1))\n",
    "    X_T = np.array(pd.concat([train],axis=1))\n",
    "    X_P = np.array(pd.concat([predict],axis=1))\n",
    "    X_T = np.expand_dims(X_T,axis=1)\n",
    "    X_P = np.expand_dims(X_P,axis=1)\n",
    "    P_B = np.expand_dims(P_B,axis=1)\n",
    "    # y_T = np.array(pd.concat([ans],axis=1))\n",
    "    # y_T = np.expand_dims(y_T,axis=1)\n",
    "#     print(X_T.shape)\n",
    "    # print(y_T.shape)\n",
    "\n",
    "    # regression\n",
    "    # ans = df.filter(['y'])\n",
    "    y_T = np.array(pd.concat([ans],axis=1))\n",
    "    y_P = np.array(pd.concat([ans_p],axis=1))\n",
    "    # y_T = np.expand_dims(y_T,axis=1)\n",
    "#     y_T.shape\n",
    "\n",
    "    #split valid train set\n",
    "#     X_T = \n",
    "    X_T = X_T.reshape(-1,X_T.shape[2])    \n",
    "    P_B = P_B.reshape(-1,P_B.shape[2])\n",
    "    X_P = X_P.reshape(-1,X_P.shape[2])\n",
    "#     print(X_T.shape, P_B.shape)\n",
    "    eps = 0.00001\n",
    "    x_mean = np.mean(np.vstack([X_T,P_B]),axis=0)\n",
    "    x_std = np.std(np.vstack([X_T,P_B]),axis=0)+eps\n",
    "\n",
    "    train_valid_ratio = 0.9\n",
    "    indices = np.random.permutation(X_T.shape[0])\n",
    "    train_idx, valid_idx = indices[:int(X_T.shape[0] * train_valid_ratio)], indices[int(X_T.shape[0] * train_valid_ratio):]\n",
    "    x_train, x_valid = X_T[train_idx,:], X_T[valid_idx,:]\n",
    "    y_train, y_valid = y_T[train_idx], y_T[valid_idx]\n",
    "\n",
    "    x_train = x_train - np.tile(x_mean,(len(x_train),1))\n",
    "    x_train = x_train/np.tile(x_std,(len(x_train),1))\n",
    "    x_valid = x_valid - np.tile(x_mean,(len(x_valid),1))\n",
    "    x_valid = x_valid/np.tile(x_std,(len(x_valid),1))\n",
    "    P_B = P_B - np.tile(x_mean,(len(P_B),1))\n",
    "    P_B = P_B/np.tile(x_std,(len(P_B),1))\n",
    "    X_P = X_P - np.tile(x_mean,(len(X_P),1))\n",
    "    X_P = X_P/np.tile(x_std,(len(X_P),1))\n",
    "\n",
    "#     ## normalization\n",
    "#     x_train = x_train.reshape(-1,X_T.shape[2])\n",
    "#     X_P = X_P.reshape(-1,X_T.shape[2])\n",
    "#     P_B = P_B.reshape(-1,P_B.shape[2])\n",
    "#     x_valid = x_valid.reshape(-1,X_T.shape[2])\n",
    "#     ### TEST\n",
    "\n",
    "#     eps = 0.00001\n",
    "#     x_train_mean = np.mean(x_train, axis=0)\n",
    "#     X_P_mean = np.mean(X_P,axis=0)\n",
    "#     P_B_mean = np.mean(P_B,axis=0)\n",
    "#     x_train_std = np.std(x_train, axis=0) + eps\n",
    "#     X_P_std = np.std(X_P,axis=0)+eps\n",
    "#     P_B_std = np.std(P_B,axis=0)+eps\n",
    "\n",
    "#     x_train = x_train - np.tile(x_train_mean,(len(x_train),1))\n",
    "#     X_P = X_P - np.tile(X_P_mean,(len(X_P),1))\n",
    "#     P_B = P_B - np.tile(P_B_mean,(len(P_B),1))\n",
    "#     x_train = x_train/np.tile(x_train_std,(len(x_train),1))\n",
    "#     X_P = X_P/np.tile(X_P_std,(len(X_P),1))\n",
    "#     P_B = P_B/np.tile(P_B_std,(len(P_B),1))\n",
    "\n",
    "\n",
    "#     x_valid = x_valid - np.tile(x_train_mean,(len(x_valid),1))\n",
    "#     x_valid = x_valid/np.tile(x_train_std,(len(x_valid),1))\n",
    "\n",
    "\n",
    "\n",
    "#     print(x_train.shape, x_valid.shape)\n",
    "#     print(y_train.shape, y_valid.shape)\n",
    "\n",
    "\n",
    "    dfT = pd.DataFrame(x_train,columns=None)\n",
    "    dfA = pd.DataFrame(y_train,columns=None)\n",
    "    dfM = dfT.merge(dfA,how='left',left_index=True,right_index=True)\n",
    "#     dfM\n",
    "\n",
    "    #only DENSE regression\n",
    "    dfMM = dfM[dfM['0_y'] == 1 ]\n",
    "    for i in range(7):\n",
    "        dfM = dfM.append(dfMM)\n",
    "    dfT = dfM.drop(columns='0_y')\n",
    "    dfA = dfM.filter(['0_y'])\n",
    "\n",
    "    X_T = np.array(pd.concat([dfT],axis=1))\n",
    "    y_T = np.array(pd.concat([dfA],axis=1))\n",
    "\n",
    "#     print(X_T.shape, x_valid.shape)\n",
    "#     print(y_T.shape, y_valid.shape)\n",
    "\n",
    "    model = model_best(X_T,y_T,x_valid,y_valid)\n",
    "\n",
    "    answer = model.predict(X_P)\n",
    "#     model2 = load_model('model/0.9332_regression_DNN.h5')\n",
    "    answer2 = model.predict(P_B)\n",
    "#     answer = answer*0.95 + answer2*0.05\n",
    "    answer = pd.DataFrame(answer,columns=None)\n",
    "    answer2 = pd.DataFrame(answer2,columns=None)\n",
    "    merges = predictN.merge(answer,how='left',left_index=True,right_index=True)\n",
    "    merges2 = predictNN.merge(answer2,how='left',left_index=True,right_index=True)\n",
    "    allAnswer = allAnswer.append(merges)\n",
    "    allAnswer = allAnswer.append(merges2)\n",
    "    allAnswer.to_pickle('history/allAnswer.pkl')\n",
    "#     with open('history/allAnswer.pkl','wb') as f:\n",
    "#         pickle.dump(allAnswer,f)\n",
    "    print('=============DONE: '+str(iterC)+' ===============need Set iterC to: '+str(iterC+1))\n",
    "    print(allAnswer.isnull().values.sum())\n",
    "    iterC+=1\n",
    "    #AUC\n",
    "allAnswers = allAnswer.sort_values(by=['fid'])\n",
    "# allAnswers.to_csv('csv/stacking3.csv')\n",
    "allAnswers = allAnswers.reset_index(drop=True)\n",
    "allAnswers.to_csv('csv/stacking3.csv',header=False,index=False)\n",
    "print(\"finish with fid: \",len(allAnswers))\n",
    "# anss = pd.read_csv('data/train_answers.csv')\n",
    "# anss = anss.drop(columns='fid')\n",
    "# y_valid = np.array(pd.concat([anss],axis=1))\n",
    "# allAnswer = allAnswers.drop(columns='fid')\n",
    "# valid_pred = np.array(pd.concat([allAnswer],axis=1))\n",
    "# from sklearn import metrics\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# # valid_pred = model.predict(x_valid)\n",
    "# fpr, tpr, thresholds = metrics.roc_curve(y_valid, valid_pred, pos_label=1)\n",
    "# valid_auc = metrics.auc(fpr, tpr)\n",
    "# print('Valid AUC:', valid_auc)\n",
    "# conf_mat = confusion_matrix(y_valid,np.round(valid_pred),labels=[1,0])\n",
    "# print('confusion matrix of valid:',conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fid</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0015734d9e317de07623fd04ddaef05a</td>\n",
       "      <td>0.069228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>004225ae888890c798f8cbdbb36b75d5</td>\n",
       "      <td>0.026529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00450228a40e22a5dbdf455d95183f47</td>\n",
       "      <td>0.019942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004b141ca2ca4ad89f97eef26fb10fef</td>\n",
       "      <td>0.089469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00548cf8a412e19debe1eff9897091e7</td>\n",
       "      <td>0.010397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0058fec008d105f7948a116e9f86dec8</td>\n",
       "      <td>0.012357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00606292a9ae11820c171ac20f5d74df</td>\n",
       "      <td>0.004830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0071303ea5067af92f0569a0e8214f24</td>\n",
       "      <td>0.441820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00779c47016f882bbf960bff35ce8c49</td>\n",
       "      <td>0.015774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00908d7e1814e3e29ba727cc4b2d8563</td>\n",
       "      <td>0.304542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>00dbb1cc42124292b3998ebcc0134943</td>\n",
       "      <td>0.104966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0105890a2f1f0214585c3888aebeae30</td>\n",
       "      <td>0.904952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>010fe9b243ec235d7fcbda0c85c73d50</td>\n",
       "      <td>0.074725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0110c30ab2ba888a43cacbfb16b7cd97</td>\n",
       "      <td>0.071782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>01179e527d1a629ec0309c9ce714726b</td>\n",
       "      <td>0.257932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0118c2f1cfe6c8061284a364e7b911fb</td>\n",
       "      <td>0.758171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>011c5523dbbec44679fe542c50eeb282</td>\n",
       "      <td>0.022702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0120c63cb633cc17c0488d945a238679</td>\n",
       "      <td>0.076780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>013778e26384caee7f7ff3c986ab1f40</td>\n",
       "      <td>0.628039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>013aecfd14b2680cde44af2716882f1f</td>\n",
       "      <td>0.255056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0146e2ff1ad8421ec98ecd0bcb1ddc8b</td>\n",
       "      <td>0.034548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>014c7622d54466bf4e3de06c068f3ce1</td>\n",
       "      <td>0.410899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>014f96c80c1accb029ec3a50a14ddadc</td>\n",
       "      <td>0.020553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>015576e5bc552d5d26e926c6c8056424</td>\n",
       "      <td>0.005280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0155b94db8f34a68fdfbd98a4867594b</td>\n",
       "      <td>0.044165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>015c8b8654428533d012a81ed55254ed</td>\n",
       "      <td>0.575707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>015f652a574821653af17128729ae868</td>\n",
       "      <td>0.071205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>016903d94dfa6806a0acb81eb011a5ca</td>\n",
       "      <td>0.014967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>016f13e8ceb1ecdabb46a19bd126a767</td>\n",
       "      <td>0.061517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0178350e303fc50ac270da78f0a1ee49</td>\n",
       "      <td>0.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2908</th>\n",
       "      <td>fdc8b3983aec1c2bc0c053e1e2e3d7cf</td>\n",
       "      <td>0.006228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2909</th>\n",
       "      <td>fdd3d35a881203b275c8f23b4646bb7a</td>\n",
       "      <td>0.864416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2910</th>\n",
       "      <td>fddfc243ec3af937a260a55155d80143</td>\n",
       "      <td>0.006228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2911</th>\n",
       "      <td>fe012c5923e4b45ec5e43fe5bb913b98</td>\n",
       "      <td>0.146143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2912</th>\n",
       "      <td>fe15cce7a9fd36a2ca9f4ee6e5482c34</td>\n",
       "      <td>0.144900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2913</th>\n",
       "      <td>fe448067afb34a5c12cc2a77dfc36d05</td>\n",
       "      <td>0.012858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2914</th>\n",
       "      <td>fe5a620ff6e7f0b30421400615d6dca7</td>\n",
       "      <td>0.155756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>fe70099103942d5e3bff9c29db819764</td>\n",
       "      <td>0.081104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>fe896f127b7a90cc0a02172eaa312367</td>\n",
       "      <td>0.006646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>fea1302aeb52a1e55f724f2441ab219a</td>\n",
       "      <td>0.006228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>fea76ff3f8afdf0ec07b2aa848a242ff</td>\n",
       "      <td>0.006987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>feb7b7a09ad3c01bde105ddf2d80103e</td>\n",
       "      <td>0.517495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2920</th>\n",
       "      <td>ff05ef969f071d5be24f1f4b059e3462</td>\n",
       "      <td>0.015123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2921</th>\n",
       "      <td>ff1b5da028d8f99b65a908d1184ba6b1</td>\n",
       "      <td>0.037720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2922</th>\n",
       "      <td>ff1bc66824a568a49aa6887a85fc1614</td>\n",
       "      <td>0.006228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2923</th>\n",
       "      <td>ff1dcdbfbcda40cab465e61eb0b324cb</td>\n",
       "      <td>0.192366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2924</th>\n",
       "      <td>ff226d37b82523182dbeab6b19a3cc47</td>\n",
       "      <td>0.064267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925</th>\n",
       "      <td>ff2e2bd95541841e2c74f8a6c6249fa5</td>\n",
       "      <td>0.027272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2926</th>\n",
       "      <td>ff4e3d8bd9d41438c69d8aa552d2989d</td>\n",
       "      <td>0.037854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2927</th>\n",
       "      <td>ff720d661fbb2b39090d4d47a542bcad</td>\n",
       "      <td>0.006624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2928</th>\n",
       "      <td>ff869729cd81a9607399326039b4ba81</td>\n",
       "      <td>0.160175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2929</th>\n",
       "      <td>ff9525fefa5e7908bb42efb702234010</td>\n",
       "      <td>0.006228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2930</th>\n",
       "      <td>ff9c89a0104c1779a714090368187840</td>\n",
       "      <td>0.401608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2931</th>\n",
       "      <td>ffa957024274ea71c5f50808ffb15859</td>\n",
       "      <td>0.007767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2932</th>\n",
       "      <td>ffc2226cf8d0e814f318d23e95524dcd</td>\n",
       "      <td>0.009208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2933</th>\n",
       "      <td>ffde8dba3bc591ef4c3b7fc6be0ee230</td>\n",
       "      <td>0.082195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2934</th>\n",
       "      <td>ffe734ba47080c062a0c3a107142605b</td>\n",
       "      <td>0.694779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2935</th>\n",
       "      <td>ffea91ac33d536b569049298bbcc41c2</td>\n",
       "      <td>0.006461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2936</th>\n",
       "      <td>ffeea6827b985bc9cd5e84866f833dbf</td>\n",
       "      <td>0.334213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>fff12a45cd80cab1c89e4099b9c535dd</td>\n",
       "      <td>0.006228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16380 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   fid         0\n",
       "0     0015734d9e317de07623fd04ddaef05a  0.069228\n",
       "1     004225ae888890c798f8cbdbb36b75d5  0.026529\n",
       "2     00450228a40e22a5dbdf455d95183f47  0.019942\n",
       "3     004b141ca2ca4ad89f97eef26fb10fef  0.089469\n",
       "4     00548cf8a412e19debe1eff9897091e7  0.010397\n",
       "5     0058fec008d105f7948a116e9f86dec8  0.012357\n",
       "6     00606292a9ae11820c171ac20f5d74df  0.004830\n",
       "7     0071303ea5067af92f0569a0e8214f24  0.441820\n",
       "8     00779c47016f882bbf960bff35ce8c49  0.015774\n",
       "9     00908d7e1814e3e29ba727cc4b2d8563  0.304542\n",
       "10    00dbb1cc42124292b3998ebcc0134943  0.104966\n",
       "11    0105890a2f1f0214585c3888aebeae30  0.904952\n",
       "12    010fe9b243ec235d7fcbda0c85c73d50  0.074725\n",
       "13    0110c30ab2ba888a43cacbfb16b7cd97  0.071782\n",
       "14    01179e527d1a629ec0309c9ce714726b  0.257932\n",
       "15    0118c2f1cfe6c8061284a364e7b911fb  0.758171\n",
       "16    011c5523dbbec44679fe542c50eeb282  0.022702\n",
       "17    0120c63cb633cc17c0488d945a238679  0.076780\n",
       "18    013778e26384caee7f7ff3c986ab1f40  0.628039\n",
       "19    013aecfd14b2680cde44af2716882f1f  0.255056\n",
       "20    0146e2ff1ad8421ec98ecd0bcb1ddc8b  0.034548\n",
       "21    014c7622d54466bf4e3de06c068f3ce1  0.410899\n",
       "22    014f96c80c1accb029ec3a50a14ddadc  0.020553\n",
       "23    015576e5bc552d5d26e926c6c8056424  0.005280\n",
       "24    0155b94db8f34a68fdfbd98a4867594b  0.044165\n",
       "25    015c8b8654428533d012a81ed55254ed  0.575707\n",
       "26    015f652a574821653af17128729ae868  0.071205\n",
       "27    016903d94dfa6806a0acb81eb011a5ca  0.014967\n",
       "28    016f13e8ceb1ecdabb46a19bd126a767  0.061517\n",
       "29    0178350e303fc50ac270da78f0a1ee49  0.213500\n",
       "...                                ...       ...\n",
       "2908  fdc8b3983aec1c2bc0c053e1e2e3d7cf  0.006228\n",
       "2909  fdd3d35a881203b275c8f23b4646bb7a  0.864416\n",
       "2910  fddfc243ec3af937a260a55155d80143  0.006228\n",
       "2911  fe012c5923e4b45ec5e43fe5bb913b98  0.146143\n",
       "2912  fe15cce7a9fd36a2ca9f4ee6e5482c34  0.144900\n",
       "2913  fe448067afb34a5c12cc2a77dfc36d05  0.012858\n",
       "2914  fe5a620ff6e7f0b30421400615d6dca7  0.155756\n",
       "2915  fe70099103942d5e3bff9c29db819764  0.081104\n",
       "2916  fe896f127b7a90cc0a02172eaa312367  0.006646\n",
       "2917  fea1302aeb52a1e55f724f2441ab219a  0.006228\n",
       "2918  fea76ff3f8afdf0ec07b2aa848a242ff  0.006987\n",
       "2919  feb7b7a09ad3c01bde105ddf2d80103e  0.517495\n",
       "2920  ff05ef969f071d5be24f1f4b059e3462  0.015123\n",
       "2921  ff1b5da028d8f99b65a908d1184ba6b1  0.037720\n",
       "2922  ff1bc66824a568a49aa6887a85fc1614  0.006228\n",
       "2923  ff1dcdbfbcda40cab465e61eb0b324cb  0.192366\n",
       "2924  ff226d37b82523182dbeab6b19a3cc47  0.064267\n",
       "2925  ff2e2bd95541841e2c74f8a6c6249fa5  0.027272\n",
       "2926  ff4e3d8bd9d41438c69d8aa552d2989d  0.037854\n",
       "2927  ff720d661fbb2b39090d4d47a542bcad  0.006624\n",
       "2928  ff869729cd81a9607399326039b4ba81  0.160175\n",
       "2929  ff9525fefa5e7908bb42efb702234010  0.006228\n",
       "2930  ff9c89a0104c1779a714090368187840  0.401608\n",
       "2931  ffa957024274ea71c5f50808ffb15859  0.007767\n",
       "2932  ffc2226cf8d0e814f318d23e95524dcd  0.009208\n",
       "2933  ffde8dba3bc591ef4c3b7fc6be0ee230  0.082195\n",
       "2934  ffe734ba47080c062a0c3a107142605b  0.694779\n",
       "2935  ffea91ac33d536b569049298bbcc41c2  0.006461\n",
       "2936  ffeea6827b985bc9cd5e84866f833dbf  0.334213\n",
       "2937  fff12a45cd80cab1c89e4099b9c535dd  0.006228\n",
       "\n",
       "[16380 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# allAnswers = allAnswer.sort_values(by=['fid'])\n",
    "# # ans\n",
    "# ans = pd.read_csv('data/train_answers.csv')\n",
    "# # ans\n",
    "# allAnswer = allAnswers.drop(columns='fid')\n",
    "# np = np.array(pd.concat([allAnswer],axis=1))\n",
    "# ans = ans.drop(columns='fid')\n",
    "# npa = np.array(pd.concat([ans],axis=1))\n",
    "\n",
    "# from sklearn import metrics\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# fpr, tpr, thresholds = metrics.roc_curve(y_valid, valid_pred, pos_label=1)\n",
    "# valid_auc = metrics.auc(fpr, tpr)\n",
    "# print('Valid AUC:', valid_auc)\n",
    "# conf_mat = confusion_matrix(y_valid,np.round(valid_pred),labels=[1,0])\n",
    "# print('confusion matrix of valid:',conf_mat)\n",
    "# predictNN\n",
    "# answer\n",
    "# merges\n",
    "# allAnswer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fid</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001fe8dce14ce099aa6ca8ea5026ea7</td>\n",
       "      <td>0.029317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0003dc8130969abe688cadf5f14ea19f</td>\n",
       "      <td>0.001412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000f15bf6a8f0b51126ece0ce93af65c</td>\n",
       "      <td>0.001584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001543427ff6300877e6a44e354588a6</td>\n",
       "      <td>0.008175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0021c7f072ad0e8068ab8c611700bfe1</td>\n",
       "      <td>0.322728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0029bf6bbac4cb8ef325828408ca8721</td>\n",
       "      <td>0.013068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>002a80d574a26583900e0b1e199c4ef8</td>\n",
       "      <td>0.011408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00300d8402fbd9d8a74af5478b1326d6</td>\n",
       "      <td>0.080735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>004749a2597c0155e436662ee6ba7892</td>\n",
       "      <td>0.026965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>004e57a598c4e1f392e62d2ce7cac527</td>\n",
       "      <td>0.003673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>006ff22c6e01a5888dcb964c32d82512</td>\n",
       "      <td>0.005683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00a92b584bfcf58b70d5e965e91da03c</td>\n",
       "      <td>0.028433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00aaa21cc049207866b35707ab105a86</td>\n",
       "      <td>0.003845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00bf11cf3da869516c332fb98a52713e</td>\n",
       "      <td>0.722706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00c636c3cb406c9403f81a0ab8286a9d</td>\n",
       "      <td>0.004344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00ce7cccc7b8c9639860ffa19e355254</td>\n",
       "      <td>0.074695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00dbb1cc42124292b3998ebcc0134943</td>\n",
       "      <td>0.012075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00f5aecb9b058eebbe7f90aa3e86b60f</td>\n",
       "      <td>0.214857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00fd343e99d6a93f719b6580394a64a7</td>\n",
       "      <td>0.018557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0117c95e850a62ec44ea8893056ecfef</td>\n",
       "      <td>0.062221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>01248c3d45ab196ca920fbed2ba533aa</td>\n",
       "      <td>0.006161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0129be0c2613520f29527881dc11dae4</td>\n",
       "      <td>0.275587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>012b0d44cfbdc570e580683e0d293f3a</td>\n",
       "      <td>0.578619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>01307772476d9751cacda4635f75f99a</td>\n",
       "      <td>0.005667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>013493db180f2f8f53d8fe3aabf10660</td>\n",
       "      <td>0.642469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>01365e65b4ef9881e29ba4541ff7c837</td>\n",
       "      <td>0.002054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>015576e5bc552d5d26e926c6c8056424</td>\n",
       "      <td>0.002772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>015f4d2f39f3cea90b9e4ea24aac0e31</td>\n",
       "      <td>0.008712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>016f1bc272c32459f6c82217d1e41cfb</td>\n",
       "      <td>0.013188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0178350e303fc50ac270da78f0a1ee49</td>\n",
       "      <td>0.002081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5222</th>\n",
       "      <td>fe66c481573ed6ff7add29bf33626b8f</td>\n",
       "      <td>0.009074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5223</th>\n",
       "      <td>fe6a2b3bb4d1fe0485376ee93d5c003f</td>\n",
       "      <td>0.006348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5224</th>\n",
       "      <td>fe8a5b84c43767bac99dc9cbf3710dff</td>\n",
       "      <td>0.015226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5225</th>\n",
       "      <td>fe927ceda5d5a892e356ea8ba48713ec</td>\n",
       "      <td>0.003615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>feb70d56093cf4e9ac7f61c49793d05a</td>\n",
       "      <td>0.031720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5227</th>\n",
       "      <td>fec3ab8d4724f0ae4ee2fb238712cba5</td>\n",
       "      <td>0.004587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5228</th>\n",
       "      <td>fed2bb1d491839ddf620eefcb0dfc3c4</td>\n",
       "      <td>0.001382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5229</th>\n",
       "      <td>fed3e8bdf57315636c6a4790bb8e4235</td>\n",
       "      <td>0.003253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5230</th>\n",
       "      <td>fed94b13993dda74b6888c1ebcf0aa3c</td>\n",
       "      <td>0.055193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5231</th>\n",
       "      <td>fed96bd2d1c23d4fc100a0dfbf6a889a</td>\n",
       "      <td>0.013818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5232</th>\n",
       "      <td>fee125f7acba89a05a492a4bb828c6da</td>\n",
       "      <td>0.248894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5233</th>\n",
       "      <td>feecec128970ba3aa186f568691f2a9c</td>\n",
       "      <td>0.044705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5234</th>\n",
       "      <td>ff09824d43aad02905eedb5993e03117</td>\n",
       "      <td>0.015886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5235</th>\n",
       "      <td>ff1522447a9127d9383db0786c5dfc3c</td>\n",
       "      <td>0.000778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5236</th>\n",
       "      <td>ff19640ca45edef75352998ab757314b</td>\n",
       "      <td>0.002150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237</th>\n",
       "      <td>ff1a4bb4e995a201cf83faa438e0a9f5</td>\n",
       "      <td>0.006621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5238</th>\n",
       "      <td>ff46496dfea2bc7067bb160fefa5b65a</td>\n",
       "      <td>0.005563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5239</th>\n",
       "      <td>ff4de230b23ffa0cb5025f12b5bed121</td>\n",
       "      <td>0.014446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5240</th>\n",
       "      <td>ff5246ce75eed0563df1b1c10d99eda4</td>\n",
       "      <td>0.007019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5241</th>\n",
       "      <td>ff53fe1df7afd397201acdd20a97593b</td>\n",
       "      <td>0.407832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5242</th>\n",
       "      <td>ff60949b42fe6f08db76fbdd99192859</td>\n",
       "      <td>0.019399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5243</th>\n",
       "      <td>ff9325a20da8202d9a296a6861b40a27</td>\n",
       "      <td>0.172701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5244</th>\n",
       "      <td>ff93f5944941d3eebf5bd1ebdf1f710f</td>\n",
       "      <td>0.005749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5245</th>\n",
       "      <td>ffae628759280a57199409737aa36c00</td>\n",
       "      <td>0.011675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5246</th>\n",
       "      <td>ffc2e9c3e0b1e6f18f1d2ea5e711b50c</td>\n",
       "      <td>0.006444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5247</th>\n",
       "      <td>ffcf53ef4238ab0b175bc7b5ec37d6d6</td>\n",
       "      <td>0.002953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5248</th>\n",
       "      <td>ffe023042f06cb48603058b6245fc338</td>\n",
       "      <td>0.720568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5249</th>\n",
       "      <td>ffee6396874030bbef0de892b28ec929</td>\n",
       "      <td>0.060451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5250</th>\n",
       "      <td>ffee6d88debec239c941df9afeff430d</td>\n",
       "      <td>0.834134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5251</th>\n",
       "      <td>ffeff485a68762c5c44a0c75fe4ae655</td>\n",
       "      <td>0.833129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5252 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   fid         0\n",
       "0     0001fe8dce14ce099aa6ca8ea5026ea7  0.029317\n",
       "1     0003dc8130969abe688cadf5f14ea19f  0.001412\n",
       "2     000f15bf6a8f0b51126ece0ce93af65c  0.001584\n",
       "3     001543427ff6300877e6a44e354588a6  0.008175\n",
       "4     0021c7f072ad0e8068ab8c611700bfe1  0.322728\n",
       "5     0029bf6bbac4cb8ef325828408ca8721  0.013068\n",
       "6     002a80d574a26583900e0b1e199c4ef8  0.011408\n",
       "7     00300d8402fbd9d8a74af5478b1326d6  0.080735\n",
       "8     004749a2597c0155e436662ee6ba7892  0.026965\n",
       "9     004e57a598c4e1f392e62d2ce7cac527  0.003673\n",
       "10    006ff22c6e01a5888dcb964c32d82512  0.005683\n",
       "11    00a92b584bfcf58b70d5e965e91da03c  0.028433\n",
       "12    00aaa21cc049207866b35707ab105a86  0.003845\n",
       "13    00bf11cf3da869516c332fb98a52713e  0.722706\n",
       "14    00c636c3cb406c9403f81a0ab8286a9d  0.004344\n",
       "15    00ce7cccc7b8c9639860ffa19e355254  0.074695\n",
       "16    00dbb1cc42124292b3998ebcc0134943  0.012075\n",
       "17    00f5aecb9b058eebbe7f90aa3e86b60f  0.214857\n",
       "18    00fd343e99d6a93f719b6580394a64a7  0.018557\n",
       "19    0117c95e850a62ec44ea8893056ecfef  0.062221\n",
       "20    01248c3d45ab196ca920fbed2ba533aa  0.006161\n",
       "21    0129be0c2613520f29527881dc11dae4  0.275587\n",
       "22    012b0d44cfbdc570e580683e0d293f3a  0.578619\n",
       "23    01307772476d9751cacda4635f75f99a  0.005667\n",
       "24    013493db180f2f8f53d8fe3aabf10660  0.642469\n",
       "25    01365e65b4ef9881e29ba4541ff7c837  0.002054\n",
       "26    015576e5bc552d5d26e926c6c8056424  0.002772\n",
       "27    015f4d2f39f3cea90b9e4ea24aac0e31  0.008712\n",
       "28    016f1bc272c32459f6c82217d1e41cfb  0.013188\n",
       "29    0178350e303fc50ac270da78f0a1ee49  0.002081\n",
       "...                                ...       ...\n",
       "5222  fe66c481573ed6ff7add29bf33626b8f  0.009074\n",
       "5223  fe6a2b3bb4d1fe0485376ee93d5c003f  0.006348\n",
       "5224  fe8a5b84c43767bac99dc9cbf3710dff  0.015226\n",
       "5225  fe927ceda5d5a892e356ea8ba48713ec  0.003615\n",
       "5226  feb70d56093cf4e9ac7f61c49793d05a  0.031720\n",
       "5227  fec3ab8d4724f0ae4ee2fb238712cba5  0.004587\n",
       "5228  fed2bb1d491839ddf620eefcb0dfc3c4  0.001382\n",
       "5229  fed3e8bdf57315636c6a4790bb8e4235  0.003253\n",
       "5230  fed94b13993dda74b6888c1ebcf0aa3c  0.055193\n",
       "5231  fed96bd2d1c23d4fc100a0dfbf6a889a  0.013818\n",
       "5232  fee125f7acba89a05a492a4bb828c6da  0.248894\n",
       "5233  feecec128970ba3aa186f568691f2a9c  0.044705\n",
       "5234  ff09824d43aad02905eedb5993e03117  0.015886\n",
       "5235  ff1522447a9127d9383db0786c5dfc3c  0.000778\n",
       "5236  ff19640ca45edef75352998ab757314b  0.002150\n",
       "5237  ff1a4bb4e995a201cf83faa438e0a9f5  0.006621\n",
       "5238  ff46496dfea2bc7067bb160fefa5b65a  0.005563\n",
       "5239  ff4de230b23ffa0cb5025f12b5bed121  0.014446\n",
       "5240  ff5246ce75eed0563df1b1c10d99eda4  0.007019\n",
       "5241  ff53fe1df7afd397201acdd20a97593b  0.407832\n",
       "5242  ff60949b42fe6f08db76fbdd99192859  0.019399\n",
       "5243  ff9325a20da8202d9a296a6861b40a27  0.172701\n",
       "5244  ff93f5944941d3eebf5bd1ebdf1f710f  0.005749\n",
       "5245  ffae628759280a57199409737aa36c00  0.011675\n",
       "5246  ffc2e9c3e0b1e6f18f1d2ea5e711b50c  0.006444\n",
       "5247  ffcf53ef4238ab0b175bc7b5ec37d6d6  0.002953\n",
       "5248  ffe023042f06cb48603058b6245fc338  0.720568\n",
       "5249  ffee6396874030bbef0de892b28ec929  0.060451\n",
       "5250  ffee6d88debec239c941df9afeff430d  0.834134\n",
       "5251  ffeff485a68762c5c44a0c75fe4ae655  0.833129\n",
       "\n",
       "[5252 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "valid_pred = model.predict(x_valid)\n",
    "# valid_pred = np.reshape(-1,1)\n",
    "print(valid_pred.shape)\n",
    "# y_valid = np.reshape(-1,1)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_valid, valid_pred, pos_label=1)\n",
    "valid_auc = metrics.auc(fpr, tpr)\n",
    "print('Valid AUC:', valid_auc)\n",
    "\n",
    "conf_mat = confusion_matrix(y_valid,np.round(valid_pred),labels=[1,0])\n",
    "print('confusion matrix of valid:',conf_mat)\n",
    "\n",
    "valid_pred = model.predict(X_T)\n",
    "# valid_pred = np.reshape(-1,1)\n",
    "print(valid_pred.shape)\n",
    "# y_valid = np.reshape(-1,1)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_T, valid_pred, pos_label=1)\n",
    "valid_auc = metrics.auc(fpr, tpr)\n",
    "print('Train AUC:', valid_auc)\n",
    "\n",
    "conf_mat = confusion_matrix(y_T,np.round(valid_pred),labels=[1,0])\n",
    "print('confusion matrix of train:',conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model/0.9332_regression_DNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fid</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000462baadff8a02f7f6fef84d242eed</td>\n",
       "      <td>0.020994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>000a00e6939f803bbf4aaf51b39048d0</td>\n",
       "      <td>0.011523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>000f15bf6a8f0b51126ece0ce93af65c</td>\n",
       "      <td>0.003075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0011d5f78c56a18dbb60a455c084ecca</td>\n",
       "      <td>0.010115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0023276a212ce8efa247fc1bca44b853</td>\n",
       "      <td>0.853587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>00245e6ba06c7142c6c8e9b60b0b4afc</td>\n",
       "      <td>0.013539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>004e57a598c4e1f392e62d2ce7cac527</td>\n",
       "      <td>0.221938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>005abdfb0f6588fcb04c03b6b94becbe</td>\n",
       "      <td>0.004996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>007cb2b19304d60879ea9f794ddc8375</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>008e0adaa5f026a92d949f90efb9f0f0</td>\n",
       "      <td>0.005371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>008f43f81999afb64e1bae5e5721afd3</td>\n",
       "      <td>0.003884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>009baf7faae650778cf96b8b4a572aa0</td>\n",
       "      <td>0.007319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>00a1d10d97c2ec77aaa4845b5ca19029</td>\n",
       "      <td>0.006005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>00aaa21cc049207866b35707ab105a86</td>\n",
       "      <td>0.011294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>00baa8960e771066b3929d8d121e5770</td>\n",
       "      <td>0.328890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>00bb0d62470340c81cc56348abe47b51</td>\n",
       "      <td>0.005137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>00c0a22f61b1b165d773997909a28f71</td>\n",
       "      <td>0.002622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>00c636c3cb406c9403f81a0ab8286a9d</td>\n",
       "      <td>0.001783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>00cd4a26a4b6db4736b447d93863b18f</td>\n",
       "      <td>0.003258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>00f282dfd3362865b4612da162ceff71</td>\n",
       "      <td>0.004115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0100f1e9d8207a46ebc89b48173d7804</td>\n",
       "      <td>0.003109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>01038b3c68f64a439322619445685903</td>\n",
       "      <td>0.265113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0105958355ed34ecb88d8089f61d65ee</td>\n",
       "      <td>0.301272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>011362d80367aa2902e85b2c7dfb3815</td>\n",
       "      <td>0.014997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>012b0d44cfbdc570e580683e0d293f3a</td>\n",
       "      <td>0.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>012f7033e5adfa827853d0592482b7d8</td>\n",
       "      <td>0.006723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0133c7e7efd5f8e28281c825cba301f6</td>\n",
       "      <td>0.072164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>013778e26384caee7f7ff3c986ab1f40</td>\n",
       "      <td>0.070011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0146e2ff1ad8421ec98ecd0bcb1ddc8b</td>\n",
       "      <td>0.196625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>014f96c80c1accb029ec3a50a14ddadc</td>\n",
       "      <td>0.035009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52164</th>\n",
       "      <td>fe270fdfc916ee0f5617b540f1d782a3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52173</th>\n",
       "      <td>fe30e75e874d7c7f32c94480802f46bf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52177</th>\n",
       "      <td>fe37b1b3368a8c4fb0ab7ea32e141d6e</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52184</th>\n",
       "      <td>fe45be483dd95f98760f7501aa06ba85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52197</th>\n",
       "      <td>fe53057b6fb2424390d0d191eb3b9a82</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52198</th>\n",
       "      <td>fe5328df803f9963262ed73e4a309a54</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52199</th>\n",
       "      <td>fe53e29156fc8bae50d0b96c9f4d7657</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52237</th>\n",
       "      <td>fe927ceda5d5a892e356ea8ba48713ec</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52246</th>\n",
       "      <td>fe9dda5a681f129fca55f4611c4ac3fc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52265</th>\n",
       "      <td>fec03b5b330f339c069c7e6e1d5e7d95</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52274</th>\n",
       "      <td>fec8aebca8b2fde334b55dd3fba94cdd</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52280</th>\n",
       "      <td>fed2bb1d491839ddf620eefcb0dfc3c4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52283</th>\n",
       "      <td>fed3e8bdf57315636c6a4790bb8e4235</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52296</th>\n",
       "      <td>fee125f7acba89a05a492a4bb828c6da</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52300</th>\n",
       "      <td>fee65f231817211cc22169416c800fa3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52313</th>\n",
       "      <td>fef84dfad6a6d6d8a084bfc86838e635</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52317</th>\n",
       "      <td>fefb9c2fc3e8f5d4540f95eab041ae70</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52324</th>\n",
       "      <td>ff02d051ba93d372cde7a757a6d81f08</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52330</th>\n",
       "      <td>ff0d3737a231b1b35e11f6b2767a92bc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52353</th>\n",
       "      <td>ff2f67bb95e04ff63f4b9da595229914</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52362</th>\n",
       "      <td>ff3d0ea54d8df8c5a21c5f5602f98ea3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52377</th>\n",
       "      <td>ff4b41112d7a06ebfe4d6cd6c103194d</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52412</th>\n",
       "      <td>ff8027c3c1358d9941c3a872469f9bda</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52417</th>\n",
       "      <td>ff82d17a18b0b9d737f68669052bdcaa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52420</th>\n",
       "      <td>ff84a57dfaa1ab4ce5bc7803bc550974</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52424</th>\n",
       "      <td>ff8a739ce369543bc59b28ffac725869</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52437</th>\n",
       "      <td>ff9d6c746be37b7ce3939e5e81bf288e</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52469</th>\n",
       "      <td>ffccd83a4df2bf783530ceadedfc1c25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52502</th>\n",
       "      <td>ffeff485a68762c5c44a0c75fe4ae655</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52503</th>\n",
       "      <td>fff08b1c7eb9c2884a3a36ae307c9e77</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5252 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    fid         0\n",
       "5      000462baadff8a02f7f6fef84d242eed  0.020994\n",
       "11     000a00e6939f803bbf4aaf51b39048d0  0.011523\n",
       "15     000f15bf6a8f0b51126ece0ce93af65c  0.003075\n",
       "18     0011d5f78c56a18dbb60a455c084ecca  0.010115\n",
       "28     0023276a212ce8efa247fc1bca44b853  0.853587\n",
       "30     00245e6ba06c7142c6c8e9b60b0b4afc  0.013539\n",
       "55     004e57a598c4e1f392e62d2ce7cac527  0.221938\n",
       "62     005abdfb0f6588fcb04c03b6b94becbe  0.004996\n",
       "90     007cb2b19304d60879ea9f794ddc8375  0.006300\n",
       "99     008e0adaa5f026a92d949f90efb9f0f0  0.005371\n",
       "102    008f43f81999afb64e1bae5e5721afd3  0.003884\n",
       "112    009baf7faae650778cf96b8b4a572aa0  0.007319\n",
       "118    00a1d10d97c2ec77aaa4845b5ca19029  0.006005\n",
       "133    00aaa21cc049207866b35707ab105a86  0.011294\n",
       "138    00baa8960e771066b3929d8d121e5770  0.328890\n",
       "139    00bb0d62470340c81cc56348abe47b51  0.005137\n",
       "145    00c0a22f61b1b165d773997909a28f71  0.002622\n",
       "152    00c636c3cb406c9403f81a0ab8286a9d  0.001783\n",
       "158    00cd4a26a4b6db4736b447d93863b18f  0.003258\n",
       "176    00f282dfd3362865b4612da162ceff71  0.004115\n",
       "190    0100f1e9d8207a46ebc89b48173d7804  0.003109\n",
       "193    01038b3c68f64a439322619445685903  0.265113\n",
       "196    0105958355ed34ecb88d8089f61d65ee  0.301272\n",
       "202    011362d80367aa2902e85b2c7dfb3815  0.014997\n",
       "228    012b0d44cfbdc570e580683e0d293f3a  0.014700\n",
       "233    012f7033e5adfa827853d0592482b7d8  0.006723\n",
       "238    0133c7e7efd5f8e28281c825cba301f6  0.072164\n",
       "244    013778e26384caee7f7ff3c986ab1f40  0.070011\n",
       "257    0146e2ff1ad8421ec98ecd0bcb1ddc8b  0.196625\n",
       "267    014f96c80c1accb029ec3a50a14ddadc  0.035009\n",
       "...                                 ...       ...\n",
       "52164  fe270fdfc916ee0f5617b540f1d782a3       NaN\n",
       "52173  fe30e75e874d7c7f32c94480802f46bf       NaN\n",
       "52177  fe37b1b3368a8c4fb0ab7ea32e141d6e       NaN\n",
       "52184  fe45be483dd95f98760f7501aa06ba85       NaN\n",
       "52197  fe53057b6fb2424390d0d191eb3b9a82       NaN\n",
       "52198  fe5328df803f9963262ed73e4a309a54       NaN\n",
       "52199  fe53e29156fc8bae50d0b96c9f4d7657       NaN\n",
       "52237  fe927ceda5d5a892e356ea8ba48713ec       NaN\n",
       "52246  fe9dda5a681f129fca55f4611c4ac3fc       NaN\n",
       "52265  fec03b5b330f339c069c7e6e1d5e7d95       NaN\n",
       "52274  fec8aebca8b2fde334b55dd3fba94cdd       NaN\n",
       "52280  fed2bb1d491839ddf620eefcb0dfc3c4       NaN\n",
       "52283  fed3e8bdf57315636c6a4790bb8e4235       NaN\n",
       "52296  fee125f7acba89a05a492a4bb828c6da       NaN\n",
       "52300  fee65f231817211cc22169416c800fa3       NaN\n",
       "52313  fef84dfad6a6d6d8a084bfc86838e635       NaN\n",
       "52317  fefb9c2fc3e8f5d4540f95eab041ae70       NaN\n",
       "52324  ff02d051ba93d372cde7a757a6d81f08       NaN\n",
       "52330  ff0d3737a231b1b35e11f6b2767a92bc       NaN\n",
       "52353  ff2f67bb95e04ff63f4b9da595229914       NaN\n",
       "52362  ff3d0ea54d8df8c5a21c5f5602f98ea3       NaN\n",
       "52377  ff4b41112d7a06ebfe4d6cd6c103194d       NaN\n",
       "52412  ff8027c3c1358d9941c3a872469f9bda       NaN\n",
       "52417  ff82d17a18b0b9d737f68669052bdcaa       NaN\n",
       "52420  ff84a57dfaa1ab4ce5bc7803bc550974       NaN\n",
       "52424  ff8a739ce369543bc59b28ffac725869       NaN\n",
       "52437  ff9d6c746be37b7ce3939e5e81bf288e       NaN\n",
       "52469  ffccd83a4df2bf783530ceadedfc1c25       NaN\n",
       "52502  ffeff485a68762c5c44a0c75fe4ae655       NaN\n",
       "52503  fff08b1c7eb9c2884a3a36ae307c9e77       NaN\n",
       "\n",
       "[5252 rows x 2 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictN = predict_10[0].filter(['fid'])\n",
    "dfP = pd.DataFrame(ans,columns=None)\n",
    "PA = predictN.merge(dfP,how='left',left_index=True,right_index=True)\n",
    "# dfL = df\n",
    "PA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fid</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000462baadff8a02f7f6fef84d242eed</td>\n",
       "      <td>0.020994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>000a00e6939f803bbf4aaf51b39048d0</td>\n",
       "      <td>0.011523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>000f15bf6a8f0b51126ece0ce93af65c</td>\n",
       "      <td>0.003075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0011d5f78c56a18dbb60a455c084ecca</td>\n",
       "      <td>0.010115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0023276a212ce8efa247fc1bca44b853</td>\n",
       "      <td>0.853587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>00245e6ba06c7142c6c8e9b60b0b4afc</td>\n",
       "      <td>0.013539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>004e57a598c4e1f392e62d2ce7cac527</td>\n",
       "      <td>0.221938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>005abdfb0f6588fcb04c03b6b94becbe</td>\n",
       "      <td>0.004996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>007cb2b19304d60879ea9f794ddc8375</td>\n",
       "      <td>0.006300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>008e0adaa5f026a92d949f90efb9f0f0</td>\n",
       "      <td>0.005371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>008f43f81999afb64e1bae5e5721afd3</td>\n",
       "      <td>0.003884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>009baf7faae650778cf96b8b4a572aa0</td>\n",
       "      <td>0.007319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>00a1d10d97c2ec77aaa4845b5ca19029</td>\n",
       "      <td>0.006005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>00aaa21cc049207866b35707ab105a86</td>\n",
       "      <td>0.011294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>00baa8960e771066b3929d8d121e5770</td>\n",
       "      <td>0.328890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>00bb0d62470340c81cc56348abe47b51</td>\n",
       "      <td>0.005137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>00c0a22f61b1b165d773997909a28f71</td>\n",
       "      <td>0.002622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>00c636c3cb406c9403f81a0ab8286a9d</td>\n",
       "      <td>0.001783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>00cd4a26a4b6db4736b447d93863b18f</td>\n",
       "      <td>0.003258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>00f282dfd3362865b4612da162ceff71</td>\n",
       "      <td>0.004115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0100f1e9d8207a46ebc89b48173d7804</td>\n",
       "      <td>0.003109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>01038b3c68f64a439322619445685903</td>\n",
       "      <td>0.265113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0105958355ed34ecb88d8089f61d65ee</td>\n",
       "      <td>0.301272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>011362d80367aa2902e85b2c7dfb3815</td>\n",
       "      <td>0.014997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>012b0d44cfbdc570e580683e0d293f3a</td>\n",
       "      <td>0.014700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>012f7033e5adfa827853d0592482b7d8</td>\n",
       "      <td>0.006723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0133c7e7efd5f8e28281c825cba301f6</td>\n",
       "      <td>0.072164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>013778e26384caee7f7ff3c986ab1f40</td>\n",
       "      <td>0.070011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0146e2ff1ad8421ec98ecd0bcb1ddc8b</td>\n",
       "      <td>0.196625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>014f96c80c1accb029ec3a50a14ddadc</td>\n",
       "      <td>0.035009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52164</th>\n",
       "      <td>fe270fdfc916ee0f5617b540f1d782a3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52173</th>\n",
       "      <td>fe30e75e874d7c7f32c94480802f46bf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52177</th>\n",
       "      <td>fe37b1b3368a8c4fb0ab7ea32e141d6e</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52184</th>\n",
       "      <td>fe45be483dd95f98760f7501aa06ba85</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52197</th>\n",
       "      <td>fe53057b6fb2424390d0d191eb3b9a82</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52198</th>\n",
       "      <td>fe5328df803f9963262ed73e4a309a54</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52199</th>\n",
       "      <td>fe53e29156fc8bae50d0b96c9f4d7657</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52237</th>\n",
       "      <td>fe927ceda5d5a892e356ea8ba48713ec</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52246</th>\n",
       "      <td>fe9dda5a681f129fca55f4611c4ac3fc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52265</th>\n",
       "      <td>fec03b5b330f339c069c7e6e1d5e7d95</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52274</th>\n",
       "      <td>fec8aebca8b2fde334b55dd3fba94cdd</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52280</th>\n",
       "      <td>fed2bb1d491839ddf620eefcb0dfc3c4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52283</th>\n",
       "      <td>fed3e8bdf57315636c6a4790bb8e4235</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52296</th>\n",
       "      <td>fee125f7acba89a05a492a4bb828c6da</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52300</th>\n",
       "      <td>fee65f231817211cc22169416c800fa3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52313</th>\n",
       "      <td>fef84dfad6a6d6d8a084bfc86838e635</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52317</th>\n",
       "      <td>fefb9c2fc3e8f5d4540f95eab041ae70</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52324</th>\n",
       "      <td>ff02d051ba93d372cde7a757a6d81f08</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52330</th>\n",
       "      <td>ff0d3737a231b1b35e11f6b2767a92bc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52353</th>\n",
       "      <td>ff2f67bb95e04ff63f4b9da595229914</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52362</th>\n",
       "      <td>ff3d0ea54d8df8c5a21c5f5602f98ea3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52377</th>\n",
       "      <td>ff4b41112d7a06ebfe4d6cd6c103194d</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52412</th>\n",
       "      <td>ff8027c3c1358d9941c3a872469f9bda</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52417</th>\n",
       "      <td>ff82d17a18b0b9d737f68669052bdcaa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52420</th>\n",
       "      <td>ff84a57dfaa1ab4ce5bc7803bc550974</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52424</th>\n",
       "      <td>ff8a739ce369543bc59b28ffac725869</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52437</th>\n",
       "      <td>ff9d6c746be37b7ce3939e5e81bf288e</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52469</th>\n",
       "      <td>ffccd83a4df2bf783530ceadedfc1c25</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52502</th>\n",
       "      <td>ffeff485a68762c5c44a0c75fe4ae655</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52503</th>\n",
       "      <td>fff08b1c7eb9c2884a3a36ae307c9e77</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5252 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    fid         0\n",
       "5      000462baadff8a02f7f6fef84d242eed  0.020994\n",
       "11     000a00e6939f803bbf4aaf51b39048d0  0.011523\n",
       "15     000f15bf6a8f0b51126ece0ce93af65c  0.003075\n",
       "18     0011d5f78c56a18dbb60a455c084ecca  0.010115\n",
       "28     0023276a212ce8efa247fc1bca44b853  0.853587\n",
       "30     00245e6ba06c7142c6c8e9b60b0b4afc  0.013539\n",
       "55     004e57a598c4e1f392e62d2ce7cac527  0.221938\n",
       "62     005abdfb0f6588fcb04c03b6b94becbe  0.004996\n",
       "90     007cb2b19304d60879ea9f794ddc8375  0.006300\n",
       "99     008e0adaa5f026a92d949f90efb9f0f0  0.005371\n",
       "102    008f43f81999afb64e1bae5e5721afd3  0.003884\n",
       "112    009baf7faae650778cf96b8b4a572aa0  0.007319\n",
       "118    00a1d10d97c2ec77aaa4845b5ca19029  0.006005\n",
       "133    00aaa21cc049207866b35707ab105a86  0.011294\n",
       "138    00baa8960e771066b3929d8d121e5770  0.328890\n",
       "139    00bb0d62470340c81cc56348abe47b51  0.005137\n",
       "145    00c0a22f61b1b165d773997909a28f71  0.002622\n",
       "152    00c636c3cb406c9403f81a0ab8286a9d  0.001783\n",
       "158    00cd4a26a4b6db4736b447d93863b18f  0.003258\n",
       "176    00f282dfd3362865b4612da162ceff71  0.004115\n",
       "190    0100f1e9d8207a46ebc89b48173d7804  0.003109\n",
       "193    01038b3c68f64a439322619445685903  0.265113\n",
       "196    0105958355ed34ecb88d8089f61d65ee  0.301272\n",
       "202    011362d80367aa2902e85b2c7dfb3815  0.014997\n",
       "228    012b0d44cfbdc570e580683e0d293f3a  0.014700\n",
       "233    012f7033e5adfa827853d0592482b7d8  0.006723\n",
       "238    0133c7e7efd5f8e28281c825cba301f6  0.072164\n",
       "244    013778e26384caee7f7ff3c986ab1f40  0.070011\n",
       "257    0146e2ff1ad8421ec98ecd0bcb1ddc8b  0.196625\n",
       "267    014f96c80c1accb029ec3a50a14ddadc  0.035009\n",
       "...                                 ...       ...\n",
       "52164  fe270fdfc916ee0f5617b540f1d782a3       NaN\n",
       "52173  fe30e75e874d7c7f32c94480802f46bf       NaN\n",
       "52177  fe37b1b3368a8c4fb0ab7ea32e141d6e       NaN\n",
       "52184  fe45be483dd95f98760f7501aa06ba85       NaN\n",
       "52197  fe53057b6fb2424390d0d191eb3b9a82       NaN\n",
       "52198  fe5328df803f9963262ed73e4a309a54       NaN\n",
       "52199  fe53e29156fc8bae50d0b96c9f4d7657       NaN\n",
       "52237  fe927ceda5d5a892e356ea8ba48713ec       NaN\n",
       "52246  fe9dda5a681f129fca55f4611c4ac3fc       NaN\n",
       "52265  fec03b5b330f339c069c7e6e1d5e7d95       NaN\n",
       "52274  fec8aebca8b2fde334b55dd3fba94cdd       NaN\n",
       "52280  fed2bb1d491839ddf620eefcb0dfc3c4       NaN\n",
       "52283  fed3e8bdf57315636c6a4790bb8e4235       NaN\n",
       "52296  fee125f7acba89a05a492a4bb828c6da       NaN\n",
       "52300  fee65f231817211cc22169416c800fa3       NaN\n",
       "52313  fef84dfad6a6d6d8a084bfc86838e635       NaN\n",
       "52317  fefb9c2fc3e8f5d4540f95eab041ae70       NaN\n",
       "52324  ff02d051ba93d372cde7a757a6d81f08       NaN\n",
       "52330  ff0d3737a231b1b35e11f6b2767a92bc       NaN\n",
       "52353  ff2f67bb95e04ff63f4b9da595229914       NaN\n",
       "52362  ff3d0ea54d8df8c5a21c5f5602f98ea3       NaN\n",
       "52377  ff4b41112d7a06ebfe4d6cd6c103194d       NaN\n",
       "52412  ff8027c3c1358d9941c3a872469f9bda       NaN\n",
       "52417  ff82d17a18b0b9d737f68669052bdcaa       NaN\n",
       "52420  ff84a57dfaa1ab4ce5bc7803bc550974       NaN\n",
       "52424  ff8a739ce369543bc59b28ffac725869       NaN\n",
       "52437  ff9d6c746be37b7ce3939e5e81bf288e       NaN\n",
       "52469  ffccd83a4df2bf783530ceadedfc1c25       NaN\n",
       "52502  ffeff485a68762c5c44a0c75fe4ae655       NaN\n",
       "52503  fff08b1c7eb9c2884a3a36ae307c9e77       NaN\n",
       "\n",
       "[5252 rows x 2 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PAA =pd.DataFrame()\n",
    "PAA = PAA.append(PA)\n",
    "PAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52518, 1, 680)\n"
     ]
    }
   ],
   "source": [
    "train = df.drop(columns=['fid','ans'])\n",
    "ans = df.filter(['ans'])\n",
    "X_T = np.array(pd.concat([train],axis=1))\n",
    "X_T = np.expand_dims(X_T,axis=1)\n",
    "# y_T = np.array(pd.concat([ans],axis=1))\n",
    "# y_T = np.expand_dims(y_T,axis=1)\n",
    "print(X_T.shape)\n",
    "# print(y_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52518, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regression\n",
    "# ans = df.filter(['y'])\n",
    "y_T = np.array(pd.concat([ans],axis=1))\n",
    "# y_T = np.expand_dims(y_T,axis=1)\n",
    "y_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_T.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52518, 1, 680) (52518, 1)\n",
      "(47266, 680) (5252, 680)\n",
      "(47266, 1) (5252, 1)\n"
     ]
    }
   ],
   "source": [
    "#split valid train set\n",
    "\n",
    "print(X_T.shape, y_T.shape)\n",
    "\n",
    "train_valid_ratio = 0.9\n",
    "indices = np.random.permutation(X_T.shape[0])\n",
    "train_idx, valid_idx = indices[:int(X_T.shape[0] * train_valid_ratio)], indices[int(X_T.shape[0] * train_valid_ratio):]\n",
    "x_train, x_valid = X_T[train_idx,:], X_T[valid_idx,:]\n",
    "y_train, y_valid = y_T[train_idx], y_T[valid_idx]\n",
    "\n",
    "\n",
    "\n",
    "## normalization\n",
    "x_train = x_train.reshape(-1,X_T.shape[2])\n",
    "x_valid = x_valid.reshape(-1,X_T.shape[2])\n",
    "### TEST\n",
    "\n",
    "eps = 0.00001\n",
    "x_train_mean = np.mean(x_train, axis=0)\n",
    "x_train_std = np.std(x_train, axis=0) + eps\n",
    "\n",
    "x_train = x_train - np.tile(x_train_mean,(len(x_train),1))\n",
    "x_train = x_train/np.tile(x_train_std,(len(x_train),1))\n",
    "\n",
    "\n",
    "x_valid = x_valid - np.tile(x_train_mean,(len(x_valid),1))\n",
    "x_valid = x_valid/np.tile(x_train_std,(len(x_valid),1))\n",
    "\n",
    "\n",
    "\n",
    "print(x_train.shape, x_valid.shape)\n",
    "print(y_train.shape, y_valid.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_x</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>671</th>\n",
       "      <th>672</th>\n",
       "      <th>673</th>\n",
       "      <th>674</th>\n",
       "      <th>675</th>\n",
       "      <th>676</th>\n",
       "      <th>677</th>\n",
       "      <th>678</th>\n",
       "      <th>679</th>\n",
       "      <th>0_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>-1.627674</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>0.587706</td>\n",
       "      <td>-1.784705</td>\n",
       "      <td>0.231947</td>\n",
       "      <td>-1.961844</td>\n",
       "      <td>0.192193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.292063</td>\n",
       "      <td>-0.122764</td>\n",
       "      <td>-0.116797</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>1.656475</td>\n",
       "      <td>0.081292</td>\n",
       "      <td>-2.759881</td>\n",
       "      <td>-0.492064</td>\n",
       "      <td>0.290814</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.648513</td>\n",
       "      <td>0.655456</td>\n",
       "      <td>0.641609</td>\n",
       "      <td>0.654688</td>\n",
       "      <td>0.617804</td>\n",
       "      <td>0.593207</td>\n",
       "      <td>0.540488</td>\n",
       "      <td>0.584837</td>\n",
       "      <td>0.505424</td>\n",
       "      <td>0.520936</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.284234</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.069458</td>\n",
       "      <td>-0.412960</td>\n",
       "      <td>-0.631829</td>\n",
       "      <td>0.822685</td>\n",
       "      <td>-0.347842</td>\n",
       "      <td>-1.260388</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.695477</td>\n",
       "      <td>0.698931</td>\n",
       "      <td>0.685169</td>\n",
       "      <td>0.697022</td>\n",
       "      <td>0.676922</td>\n",
       "      <td>0.629296</td>\n",
       "      <td>0.646371</td>\n",
       "      <td>0.597261</td>\n",
       "      <td>0.623705</td>\n",
       "      <td>0.647976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145237</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.070557</td>\n",
       "      <td>-0.393041</td>\n",
       "      <td>-0.621348</td>\n",
       "      <td>0.317003</td>\n",
       "      <td>-0.559164</td>\n",
       "      <td>-0.583922</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.469507</td>\n",
       "      <td>0.464847</td>\n",
       "      <td>0.555193</td>\n",
       "      <td>0.509946</td>\n",
       "      <td>0.490616</td>\n",
       "      <td>0.389731</td>\n",
       "      <td>0.464441</td>\n",
       "      <td>0.445280</td>\n",
       "      <td>0.331291</td>\n",
       "      <td>0.349867</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.264249</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116732</td>\n",
       "      <td>-0.071455</td>\n",
       "      <td>-0.002345</td>\n",
       "      <td>-0.583647</td>\n",
       "      <td>0.772675</td>\n",
       "      <td>0.334638</td>\n",
       "      <td>0.340296</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>0.411482</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>0.322869</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>0.271899</td>\n",
       "      <td>0.312345</td>\n",
       "      <td>0.560373</td>\n",
       "      <td>0.467749</td>\n",
       "      <td>-1.902926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.288322</td>\n",
       "      <td>-0.097396</td>\n",
       "      <td>-0.116214</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>1.302868</td>\n",
       "      <td>0.004954</td>\n",
       "      <td>-0.042360</td>\n",
       "      <td>0.403191</td>\n",
       "      <td>0.290814</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>-1.627674</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>0.420586</td>\n",
       "      <td>-1.784705</td>\n",
       "      <td>0.504371</td>\n",
       "      <td>-1.961844</td>\n",
       "      <td>0.317146</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294606</td>\n",
       "      <td>-0.113038</td>\n",
       "      <td>-0.114418</td>\n",
       "      <td>-0.071426</td>\n",
       "      <td>1.211042</td>\n",
       "      <td>0.896629</td>\n",
       "      <td>0.418542</td>\n",
       "      <td>1.800337</td>\n",
       "      <td>1.554991</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.812731</td>\n",
       "      <td>0.818055</td>\n",
       "      <td>0.842854</td>\n",
       "      <td>0.871256</td>\n",
       "      <td>0.855072</td>\n",
       "      <td>0.834104</td>\n",
       "      <td>0.799765</td>\n",
       "      <td>0.770528</td>\n",
       "      <td>0.722130</td>\n",
       "      <td>0.721522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.675728</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>-0.236436</td>\n",
       "      <td>-0.670551</td>\n",
       "      <td>0.442951</td>\n",
       "      <td>-0.619683</td>\n",
       "      <td>0.237803</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>-1.627674</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>0.562352</td>\n",
       "      <td>0.679876</td>\n",
       "      <td>0.680815</td>\n",
       "      <td>0.594939</td>\n",
       "      <td>0.575180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294606</td>\n",
       "      <td>0.974913</td>\n",
       "      <td>-0.022676</td>\n",
       "      <td>0.026038</td>\n",
       "      <td>1.350316</td>\n",
       "      <td>3.120934</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>-0.396814</td>\n",
       "      <td>1.784946</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.416429</td>\n",
       "      <td>0.296711</td>\n",
       "      <td>0.316295</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>0.301335</td>\n",
       "      <td>-1.703818</td>\n",
       "      <td>-1.784705</td>\n",
       "      <td>0.231947</td>\n",
       "      <td>-1.961844</td>\n",
       "      <td>-1.902926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294606</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>-2.259947</td>\n",
       "      <td>-0.056638</td>\n",
       "      <td>-2.098613</td>\n",
       "      <td>-0.282580</td>\n",
       "      <td>-0.344889</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.302277</td>\n",
       "      <td>0.369124</td>\n",
       "      <td>0.316295</td>\n",
       "      <td>0.322869</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>0.271899</td>\n",
       "      <td>0.388393</td>\n",
       "      <td>0.513146</td>\n",
       "      <td>-1.961844</td>\n",
       "      <td>0.317146</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294606</td>\n",
       "      <td>-0.091345</td>\n",
       "      <td>-0.094051</td>\n",
       "      <td>0.085030</td>\n",
       "      <td>0.435933</td>\n",
       "      <td>0.482282</td>\n",
       "      <td>0.532203</td>\n",
       "      <td>0.349199</td>\n",
       "      <td>1.331490</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.583659</td>\n",
       "      <td>0.411482</td>\n",
       "      <td>0.544261</td>\n",
       "      <td>0.605615</td>\n",
       "      <td>0.617804</td>\n",
       "      <td>0.494930</td>\n",
       "      <td>0.236297</td>\n",
       "      <td>0.494832</td>\n",
       "      <td>0.498807</td>\n",
       "      <td>0.349867</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.218131</td>\n",
       "      <td>0.086858</td>\n",
       "      <td>0.030840</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>-0.285843</td>\n",
       "      <td>0.498599</td>\n",
       "      <td>0.443472</td>\n",
       "      <td>-0.312873</td>\n",
       "      <td>0.654705</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.596665</td>\n",
       "      <td>0.556306</td>\n",
       "      <td>0.316295</td>\n",
       "      <td>0.467611</td>\n",
       "      <td>0.417392</td>\n",
       "      <td>0.538418</td>\n",
       "      <td>0.605736</td>\n",
       "      <td>0.535910</td>\n",
       "      <td>0.505424</td>\n",
       "      <td>0.483924</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.273450</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071461</td>\n",
       "      <td>0.205902</td>\n",
       "      <td>-0.600733</td>\n",
       "      <td>0.634227</td>\n",
       "      <td>-0.317821</td>\n",
       "      <td>-1.260388</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>0.441536</td>\n",
       "      <td>0.518187</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>0.471356</td>\n",
       "      <td>-1.703818</td>\n",
       "      <td>0.412875</td>\n",
       "      <td>-1.787534</td>\n",
       "      <td>-1.961844</td>\n",
       "      <td>0.349867</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.116802</td>\n",
       "      <td>-0.118728</td>\n",
       "      <td>-0.116552</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>-0.441498</td>\n",
       "      <td>-0.703762</td>\n",
       "      <td>0.585759</td>\n",
       "      <td>-0.082656</td>\n",
       "      <td>0.290814</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.551433</td>\n",
       "      <td>0.483894</td>\n",
       "      <td>0.460126</td>\n",
       "      <td>0.490910</td>\n",
       "      <td>0.521007</td>\n",
       "      <td>-1.703818</td>\n",
       "      <td>0.236297</td>\n",
       "      <td>0.445280</td>\n",
       "      <td>0.331291</td>\n",
       "      <td>0.271030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.287135</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>-0.441219</td>\n",
       "      <td>-0.577742</td>\n",
       "      <td>0.811283</td>\n",
       "      <td>-0.454717</td>\n",
       "      <td>-1.260388</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>-1.627674</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>0.526041</td>\n",
       "      <td>0.301335</td>\n",
       "      <td>-1.703818</td>\n",
       "      <td>-1.784705</td>\n",
       "      <td>-1.787534</td>\n",
       "      <td>0.331291</td>\n",
       "      <td>0.492352</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.289798</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>0.612995</td>\n",
       "      <td>0.881134</td>\n",
       "      <td>0.134710</td>\n",
       "      <td>0.350310</td>\n",
       "      <td>0.023580</td>\n",
       "      <td>-0.864230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.504468</td>\n",
       "      <td>0.547216</td>\n",
       "      <td>0.388210</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>0.374559</td>\n",
       "      <td>-1.703818</td>\n",
       "      <td>-1.784705</td>\n",
       "      <td>-1.787534</td>\n",
       "      <td>-1.961844</td>\n",
       "      <td>0.192193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.287109</td>\n",
       "      <td>-0.127755</td>\n",
       "      <td>0.174701</td>\n",
       "      <td>0.246363</td>\n",
       "      <td>-0.900208</td>\n",
       "      <td>0.567599</td>\n",
       "      <td>0.757628</td>\n",
       "      <td>-0.146509</td>\n",
       "      <td>0.935377</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.613551</td>\n",
       "      <td>0.592694</td>\n",
       "      <td>0.621786</td>\n",
       "      <td>0.395240</td>\n",
       "      <td>0.612385</td>\n",
       "      <td>0.562352</td>\n",
       "      <td>0.508926</td>\n",
       "      <td>0.484383</td>\n",
       "      <td>0.378233</td>\n",
       "      <td>0.520936</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225144</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.105970</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>-0.155377</td>\n",
       "      <td>-0.601057</td>\n",
       "      <td>0.801401</td>\n",
       "      <td>-0.174998</td>\n",
       "      <td>1.221990</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.416429</td>\n",
       "      <td>-1.627674</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>-1.703818</td>\n",
       "      <td>-1.784705</td>\n",
       "      <td>-1.787534</td>\n",
       "      <td>-1.961844</td>\n",
       "      <td>0.317146</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294354</td>\n",
       "      <td>0.242119</td>\n",
       "      <td>-0.010974</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>0.236642</td>\n",
       "      <td>3.112660</td>\n",
       "      <td>0.526096</td>\n",
       "      <td>0.173195</td>\n",
       "      <td>1.908239</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.623455</td>\n",
       "      <td>0.679439</td>\n",
       "      <td>0.658244</td>\n",
       "      <td>0.563281</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>0.575776</td>\n",
       "      <td>0.575427</td>\n",
       "      <td>0.597261</td>\n",
       "      <td>0.634739</td>\n",
       "      <td>0.514435</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294606</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>0.979210</td>\n",
       "      <td>-0.570929</td>\n",
       "      <td>-1.015836</td>\n",
       "      <td>-0.479240</td>\n",
       "      <td>-0.158977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.530582</td>\n",
       "      <td>0.441536</td>\n",
       "      <td>0.582413</td>\n",
       "      <td>0.467611</td>\n",
       "      <td>0.572296</td>\n",
       "      <td>0.529086</td>\n",
       "      <td>0.533408</td>\n",
       "      <td>0.575695</td>\n",
       "      <td>0.505424</td>\n",
       "      <td>0.317146</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244424</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>0.507567</td>\n",
       "      <td>-0.680035</td>\n",
       "      <td>0.778212</td>\n",
       "      <td>0.097793</td>\n",
       "      <td>-1.260388</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.775452</td>\n",
       "      <td>0.759593</td>\n",
       "      <td>0.758556</td>\n",
       "      <td>0.766452</td>\n",
       "      <td>0.718744</td>\n",
       "      <td>0.682541</td>\n",
       "      <td>0.665500</td>\n",
       "      <td>0.621724</td>\n",
       "      <td>0.648609</td>\n",
       "      <td>0.627653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581308</td>\n",
       "      <td>-0.110999</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>-0.788794</td>\n",
       "      <td>-0.686581</td>\n",
       "      <td>0.543605</td>\n",
       "      <td>-0.587545</td>\n",
       "      <td>-0.014700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.871748</td>\n",
       "      <td>0.848109</td>\n",
       "      <td>0.866006</td>\n",
       "      <td>0.872344</td>\n",
       "      <td>0.839438</td>\n",
       "      <td>0.845914</td>\n",
       "      <td>0.818687</td>\n",
       "      <td>0.831417</td>\n",
       "      <td>0.802379</td>\n",
       "      <td>0.801437</td>\n",
       "      <td>...</td>\n",
       "      <td>1.103657</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.067499</td>\n",
       "      <td>-0.489330</td>\n",
       "      <td>-0.678099</td>\n",
       "      <td>0.289327</td>\n",
       "      <td>-0.631474</td>\n",
       "      <td>-0.583922</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.446321</td>\n",
       "      <td>0.464847</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>0.509946</td>\n",
       "      <td>0.572296</td>\n",
       "      <td>0.625395</td>\n",
       "      <td>0.690856</td>\n",
       "      <td>0.692913</td>\n",
       "      <td>0.588845</td>\n",
       "      <td>0.608877</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294606</td>\n",
       "      <td>0.702069</td>\n",
       "      <td>-0.097574</td>\n",
       "      <td>-0.070237</td>\n",
       "      <td>0.197196</td>\n",
       "      <td>0.825300</td>\n",
       "      <td>0.331159</td>\n",
       "      <td>-0.529141</td>\n",
       "      <td>0.900184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>-1.627674</td>\n",
       "      <td>0.388210</td>\n",
       "      <td>0.630296</td>\n",
       "      <td>0.447783</td>\n",
       "      <td>0.389731</td>\n",
       "      <td>0.432878</td>\n",
       "      <td>0.231947</td>\n",
       "      <td>-1.961844</td>\n",
       "      <td>-1.902926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.292041</td>\n",
       "      <td>-0.019452</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>0.856068</td>\n",
       "      <td>1.588268</td>\n",
       "      <td>-2.177434</td>\n",
       "      <td>-0.314985</td>\n",
       "      <td>0.579479</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>-1.627674</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>0.389731</td>\n",
       "      <td>0.236297</td>\n",
       "      <td>0.231947</td>\n",
       "      <td>0.297985</td>\n",
       "      <td>0.317146</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293729</td>\n",
       "      <td>-0.027020</td>\n",
       "      <td>-0.114568</td>\n",
       "      <td>-0.071329</td>\n",
       "      <td>-1.155199</td>\n",
       "      <td>0.651102</td>\n",
       "      <td>-0.233343</td>\n",
       "      <td>0.038138</td>\n",
       "      <td>0.290814</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>-1.627674</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>-1.703818</td>\n",
       "      <td>-1.784705</td>\n",
       "      <td>-1.787534</td>\n",
       "      <td>-1.961844</td>\n",
       "      <td>-1.902926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281818</td>\n",
       "      <td>-0.104792</td>\n",
       "      <td>-0.105392</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>-1.389926</td>\n",
       "      <td>0.436466</td>\n",
       "      <td>-0.360411</td>\n",
       "      <td>1.092108</td>\n",
       "      <td>0.866994</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>-1.627674</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>0.374559</td>\n",
       "      <td>-1.703818</td>\n",
       "      <td>0.728763</td>\n",
       "      <td>0.307938</td>\n",
       "      <td>0.476328</td>\n",
       "      <td>0.442100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294606</td>\n",
       "      <td>0.027776</td>\n",
       "      <td>-0.114326</td>\n",
       "      <td>-0.002863</td>\n",
       "      <td>1.628503</td>\n",
       "      <td>0.646483</td>\n",
       "      <td>0.502757</td>\n",
       "      <td>-0.105091</td>\n",
       "      <td>1.194378</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.469507</td>\n",
       "      <td>-1.627674</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>0.653229</td>\n",
       "      <td>0.745851</td>\n",
       "      <td>0.798794</td>\n",
       "      <td>0.630451</td>\n",
       "      <td>-1.902926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294606</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>1.518790</td>\n",
       "      <td>-0.570929</td>\n",
       "      <td>0.313029</td>\n",
       "      <td>-0.507948</td>\n",
       "      <td>-0.158977</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.706660</td>\n",
       "      <td>0.687181</td>\n",
       "      <td>0.708912</td>\n",
       "      <td>0.697022</td>\n",
       "      <td>0.696182</td>\n",
       "      <td>0.682541</td>\n",
       "      <td>0.679876</td>\n",
       "      <td>0.667215</td>\n",
       "      <td>0.630451</td>\n",
       "      <td>0.661602</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127921</td>\n",
       "      <td>-0.076230</td>\n",
       "      <td>-0.007869</td>\n",
       "      <td>-0.071318</td>\n",
       "      <td>0.011229</td>\n",
       "      <td>-0.506670</td>\n",
       "      <td>0.709951</td>\n",
       "      <td>-0.535919</td>\n",
       "      <td>0.928275</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>0.296711</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>0.271899</td>\n",
       "      <td>0.356830</td>\n",
       "      <td>0.548822</td>\n",
       "      <td>0.437374</td>\n",
       "      <td>0.520936</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294606</td>\n",
       "      <td>-0.051455</td>\n",
       "      <td>-0.090995</td>\n",
       "      <td>-0.071312</td>\n",
       "      <td>1.181362</td>\n",
       "      <td>1.021424</td>\n",
       "      <td>0.601484</td>\n",
       "      <td>0.296712</td>\n",
       "      <td>1.920582</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47236</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>0.464847</td>\n",
       "      <td>0.430278</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>0.533450</td>\n",
       "      <td>0.346243</td>\n",
       "      <td>-1.784705</td>\n",
       "      <td>0.580361</td>\n",
       "      <td>0.297985</td>\n",
       "      <td>0.543760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294606</td>\n",
       "      <td>0.213092</td>\n",
       "      <td>-0.108939</td>\n",
       "      <td>-0.070752</td>\n",
       "      <td>1.263105</td>\n",
       "      <td>1.313763</td>\n",
       "      <td>0.494338</td>\n",
       "      <td>-0.232475</td>\n",
       "      <td>-0.096067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47237</th>\n",
       "      <td>0.596665</td>\n",
       "      <td>0.411482</td>\n",
       "      <td>0.603956</td>\n",
       "      <td>0.552280</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>0.464075</td>\n",
       "      <td>0.312345</td>\n",
       "      <td>0.408392</td>\n",
       "      <td>0.491788</td>\n",
       "      <td>0.428703</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294606</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>0.815457</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>0.282429</td>\n",
       "      <td>2.446861</td>\n",
       "      <td>0.450079</td>\n",
       "      <td>0.529216</td>\n",
       "      <td>2.017051</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47238</th>\n",
       "      <td>0.659089</td>\n",
       "      <td>0.661987</td>\n",
       "      <td>0.658244</td>\n",
       "      <td>0.624651</td>\n",
       "      <td>0.617804</td>\n",
       "      <td>0.593207</td>\n",
       "      <td>0.593755</td>\n",
       "      <td>0.604825</td>\n",
       "      <td>0.552366</td>\n",
       "      <td>0.575180</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.127355</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071115</td>\n",
       "      <td>-0.362814</td>\n",
       "      <td>-0.676174</td>\n",
       "      <td>0.073784</td>\n",
       "      <td>-0.507350</td>\n",
       "      <td>-1.260388</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47239</th>\n",
       "      <td>0.416429</td>\n",
       "      <td>0.441536</td>\n",
       "      <td>0.388210</td>\n",
       "      <td>0.395240</td>\n",
       "      <td>0.301335</td>\n",
       "      <td>0.480608</td>\n",
       "      <td>0.525838</td>\n",
       "      <td>0.231947</td>\n",
       "      <td>0.396080</td>\n",
       "      <td>0.413516</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.284754</td>\n",
       "      <td>-0.016391</td>\n",
       "      <td>-0.114315</td>\n",
       "      <td>-0.071378</td>\n",
       "      <td>0.553582</td>\n",
       "      <td>0.526626</td>\n",
       "      <td>0.138071</td>\n",
       "      <td>-0.012112</td>\n",
       "      <td>0.579479</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47240</th>\n",
       "      <td>0.568790</td>\n",
       "      <td>0.296711</td>\n",
       "      <td>0.316295</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>0.444520</td>\n",
       "      <td>0.356830</td>\n",
       "      <td>-1.787534</td>\n",
       "      <td>0.297985</td>\n",
       "      <td>0.192193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293361</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.007923</td>\n",
       "      <td>-0.355490</td>\n",
       "      <td>-0.269670</td>\n",
       "      <td>-0.203334</td>\n",
       "      <td>1.895568</td>\n",
       "      <td>-0.629146</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47241</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>0.411482</td>\n",
       "      <td>0.532041</td>\n",
       "      <td>0.630296</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>-1.703818</td>\n",
       "      <td>-1.784705</td>\n",
       "      <td>-1.787534</td>\n",
       "      <td>0.491788</td>\n",
       "      <td>-1.902926</td>\n",
       "      <td>...</td>\n",
       "      <td>1.105133</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>-1.406494</td>\n",
       "      <td>-0.940557</td>\n",
       "      <td>-0.411170</td>\n",
       "      <td>-0.585133</td>\n",
       "      <td>-0.694874</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47242</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>-1.627674</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>-1.703818</td>\n",
       "      <td>-1.784705</td>\n",
       "      <td>-1.787534</td>\n",
       "      <td>-1.961844</td>\n",
       "      <td>-1.902926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293839</td>\n",
       "      <td>0.612034</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>-1.059723</td>\n",
       "      <td>3.082996</td>\n",
       "      <td>0.284596</td>\n",
       "      <td>-0.229450</td>\n",
       "      <td>1.853833</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47243</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>-1.627674</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>0.547003</td>\n",
       "      <td>0.236297</td>\n",
       "      <td>0.307938</td>\n",
       "      <td>0.467749</td>\n",
       "      <td>-1.902926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.237956</td>\n",
       "      <td>-0.113765</td>\n",
       "      <td>-0.116535</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>0.185912</td>\n",
       "      <td>-0.381620</td>\n",
       "      <td>0.343496</td>\n",
       "      <td>0.883783</td>\n",
       "      <td>0.290814</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47244</th>\n",
       "      <td>0.750890</td>\n",
       "      <td>0.773543</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>0.773656</td>\n",
       "      <td>0.754579</td>\n",
       "      <td>0.732074</td>\n",
       "      <td>0.683659</td>\n",
       "      <td>0.699270</td>\n",
       "      <td>0.687830</td>\n",
       "      <td>0.711330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015175</td>\n",
       "      <td>-0.069446</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>-0.303298</td>\n",
       "      <td>-0.568271</td>\n",
       "      <td>0.626721</td>\n",
       "      <td>-0.592146</td>\n",
       "      <td>0.051756</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47245</th>\n",
       "      <td>0.837735</td>\n",
       "      <td>0.823592</td>\n",
       "      <td>0.847297</td>\n",
       "      <td>0.855343</td>\n",
       "      <td>0.866203</td>\n",
       "      <td>0.884017</td>\n",
       "      <td>0.862081</td>\n",
       "      <td>0.831879</td>\n",
       "      <td>0.819800</td>\n",
       "      <td>0.832264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756403</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071321</td>\n",
       "      <td>-0.212243</td>\n",
       "      <td>-0.639275</td>\n",
       "      <td>0.360848</td>\n",
       "      <td>-0.631107</td>\n",
       "      <td>-0.379743</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47246</th>\n",
       "      <td>0.760162</td>\n",
       "      <td>0.742028</td>\n",
       "      <td>0.752540</td>\n",
       "      <td>0.735416</td>\n",
       "      <td>0.714601</td>\n",
       "      <td>0.653229</td>\n",
       "      <td>0.671893</td>\n",
       "      <td>0.680815</td>\n",
       "      <td>0.636825</td>\n",
       "      <td>0.687714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.657115</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>-0.154208</td>\n",
       "      <td>-0.781703</td>\n",
       "      <td>0.787937</td>\n",
       "      <td>-0.573679</td>\n",
       "      <td>-1.260388</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47247</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>-1.627674</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>-1.703818</td>\n",
       "      <td>-1.784705</td>\n",
       "      <td>-1.787534</td>\n",
       "      <td>-1.961844</td>\n",
       "      <td>-1.902926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294606</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>0.978813</td>\n",
       "      <td>-0.570929</td>\n",
       "      <td>-2.825467</td>\n",
       "      <td>-0.637407</td>\n",
       "      <td>-0.158977</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47248</th>\n",
       "      <td>0.813493</td>\n",
       "      <td>0.802777</td>\n",
       "      <td>0.800136</td>\n",
       "      <td>0.822729</td>\n",
       "      <td>0.807085</td>\n",
       "      <td>0.804938</td>\n",
       "      <td>0.813687</td>\n",
       "      <td>0.830954</td>\n",
       "      <td>0.805310</td>\n",
       "      <td>0.799816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.314331</td>\n",
       "      <td>0.092347</td>\n",
       "      <td>1.578098</td>\n",
       "      <td>-0.070721</td>\n",
       "      <td>0.104838</td>\n",
       "      <td>-0.352265</td>\n",
       "      <td>0.637880</td>\n",
       "      <td>-0.621657</td>\n",
       "      <td>0.341421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47249</th>\n",
       "      <td>0.623455</td>\n",
       "      <td>0.628719</td>\n",
       "      <td>0.665658</td>\n",
       "      <td>0.684725</td>\n",
       "      <td>0.676922</td>\n",
       "      <td>0.640212</td>\n",
       "      <td>0.597896</td>\n",
       "      <td>0.633588</td>\n",
       "      <td>0.564564</td>\n",
       "      <td>0.599773</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079700</td>\n",
       "      <td>-0.085301</td>\n",
       "      <td>-0.108320</td>\n",
       "      <td>-0.071233</td>\n",
       "      <td>-0.113814</td>\n",
       "      <td>-0.587321</td>\n",
       "      <td>0.454759</td>\n",
       "      <td>-0.536198</td>\n",
       "      <td>-0.322607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47250</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>0.411482</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>-1.703818</td>\n",
       "      <td>-1.784705</td>\n",
       "      <td>0.231947</td>\n",
       "      <td>-1.961844</td>\n",
       "      <td>0.538469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294606</td>\n",
       "      <td>-0.107891</td>\n",
       "      <td>-0.116450</td>\n",
       "      <td>-0.071440</td>\n",
       "      <td>1.726500</td>\n",
       "      <td>0.763723</td>\n",
       "      <td>-0.455928</td>\n",
       "      <td>2.186124</td>\n",
       "      <td>1.051607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47251</th>\n",
       "      <td>0.680244</td>\n",
       "      <td>0.713435</td>\n",
       "      <td>0.779642</td>\n",
       "      <td>0.684725</td>\n",
       "      <td>0.705792</td>\n",
       "      <td>0.662050</td>\n",
       "      <td>0.616536</td>\n",
       "      <td>0.630740</td>\n",
       "      <td>0.621366</td>\n",
       "      <td>0.599773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074305</td>\n",
       "      <td>-0.055053</td>\n",
       "      <td>-0.103999</td>\n",
       "      <td>0.716500</td>\n",
       "      <td>0.603212</td>\n",
       "      <td>-0.400211</td>\n",
       "      <td>0.825660</td>\n",
       "      <td>-0.482300</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47252</th>\n",
       "      <td>0.374299</td>\n",
       "      <td>0.572410</td>\n",
       "      <td>0.483277</td>\n",
       "      <td>0.490910</td>\n",
       "      <td>0.521007</td>\n",
       "      <td>0.581907</td>\n",
       "      <td>0.356830</td>\n",
       "      <td>0.408392</td>\n",
       "      <td>0.378233</td>\n",
       "      <td>0.317146</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.262439</td>\n",
       "      <td>-0.018697</td>\n",
       "      <td>0.015911</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>-0.671787</td>\n",
       "      <td>0.208920</td>\n",
       "      <td>0.740560</td>\n",
       "      <td>-0.219398</td>\n",
       "      <td>0.510869</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47253</th>\n",
       "      <td>0.680244</td>\n",
       "      <td>0.648488</td>\n",
       "      <td>0.610246</td>\n",
       "      <td>0.509946</td>\n",
       "      <td>0.506901</td>\n",
       "      <td>0.608197</td>\n",
       "      <td>0.488923</td>\n",
       "      <td>0.445280</td>\n",
       "      <td>0.357125</td>\n",
       "      <td>0.442100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.289273</td>\n",
       "      <td>0.153207</td>\n",
       "      <td>-0.110362</td>\n",
       "      <td>-0.071092</td>\n",
       "      <td>-0.142731</td>\n",
       "      <td>0.740734</td>\n",
       "      <td>0.800366</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>0.290814</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47254</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>-1.627674</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>0.389731</td>\n",
       "      <td>0.356830</td>\n",
       "      <td>0.307938</td>\n",
       "      <td>0.425176</td>\n",
       "      <td>0.428703</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294581</td>\n",
       "      <td>-0.053572</td>\n",
       "      <td>0.321233</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>1.454972</td>\n",
       "      <td>2.748004</td>\n",
       "      <td>0.365579</td>\n",
       "      <td>0.825165</td>\n",
       "      <td>1.908239</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47255</th>\n",
       "      <td>0.693088</td>\n",
       "      <td>0.709493</td>\n",
       "      <td>0.713524</td>\n",
       "      <td>0.670783</td>\n",
       "      <td>0.693636</td>\n",
       "      <td>0.659189</td>\n",
       "      <td>0.635391</td>\n",
       "      <td>0.649276</td>\n",
       "      <td>0.614051</td>\n",
       "      <td>0.650026</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078515</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.070275</td>\n",
       "      <td>-0.127288</td>\n",
       "      <td>-0.654561</td>\n",
       "      <td>0.633956</td>\n",
       "      <td>-0.553274</td>\n",
       "      <td>-1.260388</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47256</th>\n",
       "      <td>0.640813</td>\n",
       "      <td>0.369124</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>0.322869</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>-1.703818</td>\n",
       "      <td>0.312345</td>\n",
       "      <td>0.352390</td>\n",
       "      <td>0.170794</td>\n",
       "      <td>-1.902926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.246268</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.066301</td>\n",
       "      <td>-2.761929</td>\n",
       "      <td>-0.561879</td>\n",
       "      <td>-0.822598</td>\n",
       "      <td>-0.447286</td>\n",
       "      <td>-0.493272</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47257</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>-1.627674</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>-1.703818</td>\n",
       "      <td>-1.784705</td>\n",
       "      <td>-1.787534</td>\n",
       "      <td>0.331291</td>\n",
       "      <td>0.538469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294606</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>2.324199</td>\n",
       "      <td>-0.570929</td>\n",
       "      <td>-2.803962</td>\n",
       "      <td>-0.644576</td>\n",
       "      <td>-0.158977</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47258</th>\n",
       "      <td>0.416429</td>\n",
       "      <td>0.369124</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>0.389731</td>\n",
       "      <td>0.499379</td>\n",
       "      <td>-1.787534</td>\n",
       "      <td>0.297985</td>\n",
       "      <td>-1.902926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294606</td>\n",
       "      <td>-0.074768</td>\n",
       "      <td>-0.115679</td>\n",
       "      <td>-0.071391</td>\n",
       "      <td>0.696654</td>\n",
       "      <td>0.713288</td>\n",
       "      <td>0.234323</td>\n",
       "      <td>0.904525</td>\n",
       "      <td>-0.379743</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47259</th>\n",
       "      <td>0.790685</td>\n",
       "      <td>0.774626</td>\n",
       "      <td>0.720079</td>\n",
       "      <td>0.776403</td>\n",
       "      <td>0.765564</td>\n",
       "      <td>0.732074</td>\n",
       "      <td>0.703972</td>\n",
       "      <td>0.710975</td>\n",
       "      <td>0.664302</td>\n",
       "      <td>0.724726</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.086379</td>\n",
       "      <td>0.322709</td>\n",
       "      <td>-0.086941</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>-0.094817</td>\n",
       "      <td>-0.270514</td>\n",
       "      <td>0.231105</td>\n",
       "      <td>-0.600127</td>\n",
       "      <td>0.323925</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47260</th>\n",
       "      <td>0.640813</td>\n",
       "      <td>0.644822</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>0.539982</td>\n",
       "      <td>0.447783</td>\n",
       "      <td>0.581907</td>\n",
       "      <td>0.547140</td>\n",
       "      <td>0.484383</td>\n",
       "      <td>0.425176</td>\n",
       "      <td>0.520936</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.224098</td>\n",
       "      <td>0.133884</td>\n",
       "      <td>-0.114428</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>0.506328</td>\n",
       "      <td>0.465218</td>\n",
       "      <td>0.577125</td>\n",
       "      <td>-0.310580</td>\n",
       "      <td>-0.014700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47261</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>-1.627674</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>0.464075</td>\n",
       "      <td>0.533408</td>\n",
       "      <td>0.383928</td>\n",
       "      <td>0.170794</td>\n",
       "      <td>0.192193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294308</td>\n",
       "      <td>-0.047402</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>1.888943</td>\n",
       "      <td>2.506693</td>\n",
       "      <td>-2.655367</td>\n",
       "      <td>-0.433058</td>\n",
       "      <td>1.908239</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47262</th>\n",
       "      <td>0.551433</td>\n",
       "      <td>0.513948</td>\n",
       "      <td>0.502193</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>0.490616</td>\n",
       "      <td>0.389731</td>\n",
       "      <td>0.412875</td>\n",
       "      <td>0.408392</td>\n",
       "      <td>0.511684</td>\n",
       "      <td>0.375246</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.289824</td>\n",
       "      <td>0.038368</td>\n",
       "      <td>0.039136</td>\n",
       "      <td>-0.071387</td>\n",
       "      <td>-0.265926</td>\n",
       "      <td>1.176268</td>\n",
       "      <td>0.805960</td>\n",
       "      <td>0.038740</td>\n",
       "      <td>1.402516</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47263</th>\n",
       "      <td>0.945275</td>\n",
       "      <td>0.945944</td>\n",
       "      <td>0.947574</td>\n",
       "      <td>0.930231</td>\n",
       "      <td>0.914589</td>\n",
       "      <td>0.894207</td>\n",
       "      <td>0.877096</td>\n",
       "      <td>0.849304</td>\n",
       "      <td>0.826418</td>\n",
       "      <td>0.849679</td>\n",
       "      <td>...</td>\n",
       "      <td>3.887318</td>\n",
       "      <td>-0.129958</td>\n",
       "      <td>-0.116963</td>\n",
       "      <td>-0.071472</td>\n",
       "      <td>-0.865221</td>\n",
       "      <td>-0.715394</td>\n",
       "      <td>0.800081</td>\n",
       "      <td>-0.639603</td>\n",
       "      <td>-0.583922</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47264</th>\n",
       "      <td>-1.611745</td>\n",
       "      <td>-1.627674</td>\n",
       "      <td>-1.594887</td>\n",
       "      <td>-1.600424</td>\n",
       "      <td>-1.644622</td>\n",
       "      <td>-1.703818</td>\n",
       "      <td>-1.784705</td>\n",
       "      <td>0.352390</td>\n",
       "      <td>-1.961844</td>\n",
       "      <td>0.271030</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280948</td>\n",
       "      <td>0.005796</td>\n",
       "      <td>-0.113783</td>\n",
       "      <td>-0.071363</td>\n",
       "      <td>-1.466162</td>\n",
       "      <td>0.801250</td>\n",
       "      <td>0.616389</td>\n",
       "      <td>0.132935</td>\n",
       "      <td>0.579479</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47265</th>\n",
       "      <td>0.446321</td>\n",
       "      <td>-1.627674</td>\n",
       "      <td>0.532041</td>\n",
       "      <td>0.563281</td>\n",
       "      <td>0.490616</td>\n",
       "      <td>0.444520</td>\n",
       "      <td>0.499379</td>\n",
       "      <td>0.307938</td>\n",
       "      <td>0.437374</td>\n",
       "      <td>-1.902926</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.294600</td>\n",
       "      <td>-0.092673</td>\n",
       "      <td>-0.101903</td>\n",
       "      <td>-0.071443</td>\n",
       "      <td>0.930364</td>\n",
       "      <td>1.437800</td>\n",
       "      <td>-2.605244</td>\n",
       "      <td>-0.518363</td>\n",
       "      <td>2.177741</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47266 rows  681 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0_x         1         2         3         4         5         6  \\\n",
       "0     -1.611745 -1.627674 -1.594887 -1.600424 -1.644622  0.587706 -1.784705   \n",
       "1      0.648513  0.655456  0.641609  0.654688  0.617804  0.593207  0.540488   \n",
       "2      0.695477  0.698931  0.685169  0.697022  0.676922  0.629296  0.646371   \n",
       "3      0.469507  0.464847  0.555193  0.509946  0.490616  0.389731  0.464441   \n",
       "4     -1.611745  0.411482  0.430278  0.322869 -1.644622  0.271899  0.312345   \n",
       "5     -1.611745 -1.627674 -1.594887 -1.600424 -1.644622  0.420586 -1.784705   \n",
       "6      0.812731  0.818055  0.842854  0.871256  0.855072  0.834104  0.799765   \n",
       "7     -1.611745 -1.627674 -1.594887 -1.600424 -1.644622  0.562352  0.679876   \n",
       "8      0.416429  0.296711  0.316295 -1.600424  0.301335 -1.703818 -1.784705   \n",
       "9      0.302277  0.369124  0.316295  0.322869 -1.644622  0.271899  0.388393   \n",
       "10     0.583659  0.411482  0.544261  0.605615  0.617804  0.494930  0.236297   \n",
       "11     0.596665  0.556306  0.316295  0.467611  0.417392  0.538418  0.605736   \n",
       "12    -1.611745  0.441536  0.518187 -1.600424  0.471356 -1.703818  0.412875   \n",
       "13     0.551433  0.483894  0.460126  0.490910  0.521007 -1.703818  0.236297   \n",
       "14    -1.611745 -1.627674 -1.594887  0.526041  0.301335 -1.703818 -1.784705   \n",
       "15     0.504468  0.547216  0.388210 -1.600424  0.374559 -1.703818 -1.784705   \n",
       "16     0.613551  0.592694  0.621786  0.395240  0.612385  0.562352  0.508926   \n",
       "17     0.416429 -1.627674 -1.594887 -1.600424 -1.644622 -1.703818 -1.784705   \n",
       "18     0.623455  0.679439  0.658244  0.563281 -1.644622  0.575776  0.575427   \n",
       "19     0.530582  0.441536  0.582413  0.467611  0.572296  0.529086  0.533408   \n",
       "20     0.775452  0.759593  0.758556  0.766452  0.718744  0.682541  0.665500   \n",
       "21     0.871748  0.848109  0.866006  0.872344  0.839438  0.845914  0.818687   \n",
       "22     0.446321  0.464847  0.430278  0.509946  0.572296  0.625395  0.690856   \n",
       "23    -1.611745 -1.627674  0.388210  0.630296  0.447783  0.389731  0.432878   \n",
       "24    -1.611745 -1.627674 -1.594887 -1.600424 -1.644622  0.389731  0.236297   \n",
       "25    -1.611745 -1.627674 -1.594887 -1.600424 -1.644622 -1.703818 -1.784705   \n",
       "26    -1.611745 -1.627674 -1.594887 -1.600424  0.374559 -1.703818  0.728763   \n",
       "27     0.469507 -1.627674  0.430278 -1.600424 -1.644622  0.653229  0.745851   \n",
       "28     0.706660  0.687181  0.708912  0.697022  0.696182  0.682541  0.679876   \n",
       "29    -1.611745  0.296711 -1.594887 -1.600424 -1.644622  0.271899  0.356830   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "47236 -1.611745  0.464847  0.430278 -1.600424  0.533450  0.346243 -1.784705   \n",
       "47237  0.596665  0.411482  0.603956  0.552280 -1.644622  0.464075  0.312345   \n",
       "47238  0.659089  0.661987  0.658244  0.624651  0.617804  0.593207  0.593755   \n",
       "47239  0.416429  0.441536  0.388210  0.395240  0.301335  0.480608  0.525838   \n",
       "47240  0.568790  0.296711  0.316295 -1.600424 -1.644622  0.444520  0.356830   \n",
       "47241 -1.611745  0.411482  0.532041  0.630296 -1.644622 -1.703818 -1.784705   \n",
       "47242 -1.611745 -1.627674 -1.594887 -1.600424 -1.644622 -1.703818 -1.784705   \n",
       "47243 -1.611745 -1.627674 -1.594887 -1.600424 -1.644622  0.547003  0.236297   \n",
       "47244  0.750890  0.773543  0.782000  0.773656  0.754579  0.732074  0.683659   \n",
       "47245  0.837735  0.823592  0.847297  0.855343  0.866203  0.884017  0.862081   \n",
       "47246  0.760162  0.742028  0.752540  0.735416  0.714601  0.653229  0.671893   \n",
       "47247 -1.611745 -1.627674 -1.594887 -1.600424 -1.644622 -1.703818 -1.784705   \n",
       "47248  0.813493  0.802777  0.800136  0.822729  0.807085  0.804938  0.813687   \n",
       "47249  0.623455  0.628719  0.665658  0.684725  0.676922  0.640212  0.597896   \n",
       "47250 -1.611745  0.411482 -1.594887 -1.600424 -1.644622 -1.703818 -1.784705   \n",
       "47251  0.680244  0.713435  0.779642  0.684725  0.705792  0.662050  0.616536   \n",
       "47252  0.374299  0.572410  0.483277  0.490910  0.521007  0.581907  0.356830   \n",
       "47253  0.680244  0.648488  0.610246  0.509946  0.506901  0.608197  0.488923   \n",
       "47254 -1.611745 -1.627674 -1.594887 -1.600424 -1.644622  0.389731  0.356830   \n",
       "47255  0.693088  0.709493  0.713524  0.670783  0.693636  0.659189  0.635391   \n",
       "47256  0.640813  0.369124 -1.594887  0.322869 -1.644622 -1.703818  0.312345   \n",
       "47257 -1.611745 -1.627674 -1.594887 -1.600424 -1.644622 -1.703818 -1.784705   \n",
       "47258  0.416429  0.369124 -1.594887 -1.600424 -1.644622  0.389731  0.499379   \n",
       "47259  0.790685  0.774626  0.720079  0.776403  0.765564  0.732074  0.703972   \n",
       "47260  0.640813  0.644822 -1.594887  0.539982  0.447783  0.581907  0.547140   \n",
       "47261 -1.611745 -1.627674 -1.594887 -1.600424 -1.644622  0.464075  0.533408   \n",
       "47262  0.551433  0.513948  0.502193 -1.600424  0.490616  0.389731  0.412875   \n",
       "47263  0.945275  0.945944  0.947574  0.930231  0.914589  0.894207  0.877096   \n",
       "47264 -1.611745 -1.627674 -1.594887 -1.600424 -1.644622 -1.703818 -1.784705   \n",
       "47265  0.446321 -1.627674  0.532041  0.563281  0.490616  0.444520  0.499379   \n",
       "\n",
       "              7         8         9 ...        671       672       673  \\\n",
       "0      0.231947 -1.961844  0.192193 ...  -0.292063 -0.122764 -0.116797   \n",
       "1      0.584837  0.505424  0.520936 ...  -0.284234 -0.129958 -0.116963   \n",
       "2      0.597261  0.623705  0.647976 ...  -0.145237 -0.129958 -0.116963   \n",
       "3      0.445280  0.331291  0.349867 ...  -0.264249 -0.129958 -0.116732   \n",
       "4      0.560373  0.467749 -1.902926 ...  -0.288322 -0.097396 -0.116214   \n",
       "5      0.504371 -1.961844  0.317146 ...  -0.294606 -0.113038 -0.114418   \n",
       "6      0.770528  0.722130  0.721522 ...   0.675728 -0.129958 -0.116963   \n",
       "7      0.680815  0.594939  0.575180 ...  -0.294606  0.974913 -0.022676   \n",
       "8      0.231947 -1.961844 -1.902926 ...  -0.294606 -0.129958 -0.116963   \n",
       "9      0.513146 -1.961844  0.317146 ...  -0.294606 -0.091345 -0.094051   \n",
       "10     0.494832  0.498807  0.349867 ...  -0.218131  0.086858  0.030840   \n",
       "11     0.535910  0.505424  0.483924 ...  -0.273450 -0.129958 -0.116963   \n",
       "12    -1.787534 -1.961844  0.349867 ...  -0.116802 -0.118728 -0.116552   \n",
       "13     0.445280  0.331291  0.271030 ...  -0.287135 -0.129958 -0.116963   \n",
       "14    -1.787534  0.331291  0.492352 ...  -0.289798 -0.129958 -0.116963   \n",
       "15    -1.787534 -1.961844  0.192193 ...  -0.287109 -0.127755  0.174701   \n",
       "16     0.484383  0.378233  0.520936 ...  -0.225144 -0.129958 -0.105970   \n",
       "17    -1.787534 -1.961844  0.317146 ...  -0.294354  0.242119 -0.010974   \n",
       "18     0.597261  0.634739  0.514435 ...  -0.294606 -0.129958 -0.116963   \n",
       "19     0.575695  0.505424  0.317146 ...  -0.244424 -0.129958 -0.116963   \n",
       "20     0.621724  0.648609  0.627653 ...   0.581308 -0.110999 -0.116963   \n",
       "21     0.831417  0.802379  0.801437 ...   1.103657 -0.129958 -0.116963   \n",
       "22     0.692913  0.588845  0.608877 ...  -0.294606  0.702069 -0.097574   \n",
       "23     0.231947 -1.961844 -1.902926 ...  -0.292041 -0.019452 -0.116963   \n",
       "24     0.231947  0.297985  0.317146 ...  -0.293729 -0.027020 -0.114568   \n",
       "25    -1.787534 -1.961844 -1.902926 ...  -0.281818 -0.104792 -0.105392   \n",
       "26     0.307938  0.476328  0.442100 ...  -0.294606  0.027776 -0.114326   \n",
       "27     0.798794  0.630451 -1.902926 ...  -0.294606 -0.129958 -0.116963   \n",
       "28     0.667215  0.630451  0.661602 ...  -0.127921 -0.076230 -0.007869   \n",
       "29     0.548822  0.437374  0.520936 ...  -0.294606 -0.051455 -0.090995   \n",
       "...         ...       ...       ... ...        ...       ...       ...   \n",
       "47236  0.580361  0.297985  0.543760 ...  -0.294606  0.213092 -0.108939   \n",
       "47237  0.408392  0.491788  0.428703 ...  -0.294606 -0.129958  0.815457   \n",
       "47238  0.604825  0.552366  0.575180 ...  -0.127355 -0.129958 -0.116963   \n",
       "47239  0.231947  0.396080  0.413516 ...  -0.284754 -0.016391 -0.114315   \n",
       "47240 -1.787534  0.297985  0.192193 ...  -0.293361 -0.129958 -0.116963   \n",
       "47241 -1.787534  0.491788 -1.902926 ...   1.105133 -0.129958 -0.116963   \n",
       "47242 -1.787534 -1.961844 -1.902926 ...  -0.293839  0.612034 -0.116963   \n",
       "47243  0.307938  0.467749 -1.902926 ...  -0.237956 -0.113765 -0.116535   \n",
       "47244  0.699270  0.687830  0.711330 ...   0.015175 -0.069446 -0.116963   \n",
       "47245  0.831879  0.819800  0.832264 ...   0.756403 -0.129958 -0.116963   \n",
       "47246  0.680815  0.636825  0.687714 ...   0.657115 -0.129958 -0.116963   \n",
       "47247 -1.787534 -1.961844 -1.902926 ...  -0.294606 -0.129958 -0.116963   \n",
       "47248  0.830954  0.805310  0.799816 ...   0.314331  0.092347  1.578098   \n",
       "47249  0.633588  0.564564  0.599773 ...  -0.079700 -0.085301 -0.108320   \n",
       "47250  0.231947 -1.961844  0.538469 ...  -0.294606 -0.107891 -0.116450   \n",
       "47251  0.630740  0.621366  0.599773 ...   0.074305 -0.055053 -0.103999   \n",
       "47252  0.408392  0.378233  0.317146 ...  -0.262439 -0.018697  0.015911   \n",
       "47253  0.445280  0.357125  0.442100 ...  -0.289273  0.153207 -0.110362   \n",
       "47254  0.307938  0.425176  0.428703 ...  -0.294581 -0.053572  0.321233   \n",
       "47255  0.649276  0.614051  0.650026 ...  -0.078515 -0.129958 -0.116963   \n",
       "47256  0.352390  0.170794 -1.902926 ...  -0.246268 -0.129958 -0.116963   \n",
       "47257 -1.787534  0.331291  0.538469 ...  -0.294606 -0.129958 -0.116963   \n",
       "47258 -1.787534  0.297985 -1.902926 ...  -0.294606 -0.074768 -0.115679   \n",
       "47259  0.710975  0.664302  0.724726 ...  -0.086379  0.322709 -0.086941   \n",
       "47260  0.484383  0.425176  0.520936 ...  -0.224098  0.133884 -0.114428   \n",
       "47261  0.383928  0.170794  0.192193 ...  -0.294308 -0.047402 -0.116963   \n",
       "47262  0.408392  0.511684  0.375246 ...  -0.289824  0.038368  0.039136   \n",
       "47263  0.849304  0.826418  0.849679 ...   3.887318 -0.129958 -0.116963   \n",
       "47264  0.352390 -1.961844  0.271030 ...  -0.280948  0.005796 -0.113783   \n",
       "47265  0.307938  0.437374 -1.902926 ...  -0.294600 -0.092673 -0.101903   \n",
       "\n",
       "            674       675       676       677       678       679  0_y  \n",
       "0     -0.071472  1.656475  0.081292 -2.759881 -0.492064  0.290814    0  \n",
       "1     -0.069458 -0.412960 -0.631829  0.822685 -0.347842 -1.260388    0  \n",
       "2     -0.070557 -0.393041 -0.621348  0.317003 -0.559164 -0.583922    0  \n",
       "3     -0.071455 -0.002345 -0.583647  0.772675  0.334638  0.340296    0  \n",
       "4     -0.071472  1.302868  0.004954 -0.042360  0.403191  0.290814    0  \n",
       "5     -0.071426  1.211042  0.896629  0.418542  1.800337  1.554991    0  \n",
       "6     -0.071472 -0.236436 -0.670551  0.442951 -0.619683  0.237803    0  \n",
       "7      0.026038  1.350316  3.120934  0.191919 -0.396814  1.784946    0  \n",
       "8     -0.071472 -2.259947 -0.056638 -2.098613 -0.282580 -0.344889    0  \n",
       "9      0.085030  0.435933  0.482282  0.532203  0.349199  1.331490    0  \n",
       "10    -0.071472 -0.285843  0.498599  0.443472 -0.312873  0.654705    0  \n",
       "11    -0.071461  0.205902 -0.600733  0.634227 -0.317821 -1.260388    1  \n",
       "12    -0.071472 -0.441498 -0.703762  0.585759 -0.082656  0.290814    0  \n",
       "13    -0.071472 -0.441219 -0.577742  0.811283 -0.454717 -1.260388    0  \n",
       "14     0.612995  0.881134  0.134710  0.350310  0.023580 -0.864230    0  \n",
       "15     0.246363 -0.900208  0.567599  0.757628 -0.146509  0.935377    0  \n",
       "16    -0.071472 -0.155377 -0.601057  0.801401 -0.174998  1.221990    0  \n",
       "17    -0.071472  0.236642  3.112660  0.526096  0.173195  1.908239    0  \n",
       "18    -0.071472  0.979210 -0.570929 -1.015836 -0.479240 -0.158977    0  \n",
       "19    -0.071472  0.507567 -0.680035  0.778212  0.097793 -1.260388    0  \n",
       "20    -0.071472 -0.788794 -0.686581  0.543605 -0.587545 -0.014700    0  \n",
       "21    -0.067499 -0.489330 -0.678099  0.289327 -0.631474 -0.583922    0  \n",
       "22    -0.070237  0.197196  0.825300  0.331159 -0.529141  0.900184    0  \n",
       "23    -0.071472  0.856068  1.588268 -2.177434 -0.314985  0.579479    0  \n",
       "24    -0.071329 -1.155199  0.651102 -0.233343  0.038138  0.290814    0  \n",
       "25    -0.071472 -1.389926  0.436466 -0.360411  1.092108  0.866994    1  \n",
       "26    -0.002863  1.628503  0.646483  0.502757 -0.105091  1.194378    0  \n",
       "27    -0.071472  1.518790 -0.570929  0.313029 -0.507948 -0.158977    0  \n",
       "28    -0.071318  0.011229 -0.506670  0.709951 -0.535919  0.928275    0  \n",
       "29    -0.071312  1.181362  1.021424  0.601484  0.296712  1.920582    0  \n",
       "...         ...       ...       ...       ...       ...       ...  ...  \n",
       "47236 -0.070752  1.263105  1.313763  0.494338 -0.232475 -0.096067    0  \n",
       "47237 -0.071472  0.282429  2.446861  0.450079  0.529216  2.017051    0  \n",
       "47238 -0.071115 -0.362814 -0.676174  0.073784 -0.507350 -1.260388    0  \n",
       "47239 -0.071378  0.553582  0.526626  0.138071 -0.012112  0.579479    0  \n",
       "47240 -0.007923 -0.355490 -0.269670 -0.203334  1.895568 -0.629146    0  \n",
       "47241 -0.071472 -1.406494 -0.940557 -0.411170 -0.585133 -0.694874    0  \n",
       "47242 -0.071472 -1.059723  3.082996  0.284596 -0.229450  1.853833    0  \n",
       "47243 -0.071472  0.185912 -0.381620  0.343496  0.883783  0.290814    1  \n",
       "47244 -0.071472 -0.303298 -0.568271  0.626721 -0.592146  0.051756    0  \n",
       "47245 -0.071321 -0.212243 -0.639275  0.360848 -0.631107 -0.379743    0  \n",
       "47246 -0.071472 -0.154208 -0.781703  0.787937 -0.573679 -1.260388    0  \n",
       "47247 -0.071472  0.978813 -0.570929 -2.825467 -0.637407 -0.158977    1  \n",
       "47248 -0.070721  0.104838 -0.352265  0.637880 -0.621657  0.341421    0  \n",
       "47249 -0.071233 -0.113814 -0.587321  0.454759 -0.536198 -0.322607    0  \n",
       "47250 -0.071440  1.726500  0.763723 -0.455928  2.186124  1.051607    0  \n",
       "47251  0.716500  0.603212 -0.400211  0.825660 -0.482300  0.000065    0  \n",
       "47252 -0.071472 -0.671787  0.208920  0.740560 -0.219398  0.510869    0  \n",
       "47253 -0.071092 -0.142731  0.740734  0.800366 -0.267578  0.290814    0  \n",
       "47254 -0.071472  1.454972  2.748004  0.365579  0.825165  1.908239    0  \n",
       "47255 -0.070275 -0.127288 -0.654561  0.633956 -0.553274 -1.260388    0  \n",
       "47256 -0.066301 -2.761929 -0.561879 -0.822598 -0.447286 -0.493272    0  \n",
       "47257 -0.071472  2.324199 -0.570929 -2.803962 -0.644576 -0.158977    1  \n",
       "47258 -0.071391  0.696654  0.713288  0.234323  0.904525 -0.379743    0  \n",
       "47259 -0.071472 -0.094817 -0.270514  0.231105 -0.600127  0.323925    0  \n",
       "47260 -0.071472  0.506328  0.465218  0.577125 -0.310580 -0.014700    0  \n",
       "47261 -0.071472  1.888943  2.506693 -2.655367 -0.433058  1.908239    0  \n",
       "47262 -0.071387 -0.265926  1.176268  0.805960  0.038740  1.402516    0  \n",
       "47263 -0.071472 -0.865221 -0.715394  0.800081 -0.639603 -0.583922    0  \n",
       "47264 -0.071363 -1.466162  0.801250  0.616389  0.132935  0.579479    0  \n",
       "47265 -0.071443  0.930364  1.437800 -2.605244 -0.518363  2.177741    1  \n",
       "\n",
       "[47266 rows x 681 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfT = pd.DataFrame(x_train,columns=None)\n",
    "dfA = pd.DataFrame(y_train,columns=None)\n",
    "dfM = dfT.merge(dfA,how='left',left_index=True,right_index=True)\n",
    "dfM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88010, 680) (5252, 680)\n",
      "(88010, 1) (5252, 1)\n"
     ]
    }
   ],
   "source": [
    "#only DENSE regression\n",
    "dfMM = dfM[dfM['0_y'] == 1 ]\n",
    "for i in range(8):\n",
    "    dfM = dfM.append(dfMM)\n",
    "dfT = dfM.drop(columns='0_y')\n",
    "dfA = dfM.filter(['0_y'])\n",
    "\n",
    "X_T = np.array(pd.concat([dfT],axis=1))\n",
    "y_T = np.array(pd.concat([dfA],axis=1))\n",
    "\n",
    "print(X_T.shape, x_valid.shape)\n",
    "print(y_T.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88082, 1, 658) (5252, 1, 658)\n",
      "(88082, 1, 1) (5252, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "##REGRESSION+CNN X & Y without flattern:\n",
    "X_T = np.expand_dims(X_T,axis=1)\n",
    "# print(X_T.shape)\n",
    "\n",
    "y_T = np.expand_dims(y_T,axis=1)\n",
    "# print(y_T.shape)\n",
    "\n",
    "x_valid = np.expand_dims(x_valid,axis=1)\n",
    "y_valid = np.expand_dims(y_valid,axis=1)\n",
    "\n",
    "print(X_T.shape, x_valid.shape)\n",
    "print(y_T.shape, y_valid.shape)\n",
    "# dfT = pd.DataFrame(x_train,columns=None)\n",
    "\n",
    "# print(dfT)\n",
    "# # dfTT = dfT.loc([dfT.658 ==1])\n",
    "# # dfT = dfT[dfT[659] == 1 ]\n",
    "# # dfT = dfT.astype(float)\n",
    "# # dfT = dfT.loc[dfT[658]==1. ]\n",
    "# # dfTT = dfTT.drop(columns=658)\n",
    "# for i in range(8):\n",
    "#     dfTT = dfT.append(dfT)\n",
    "#     print(dfTT.shape)\n",
    "# dfTX = dfTT.drop(columns=658)\n",
    "# dfTY = dfTT.filter([658])\n",
    "\n",
    "# X_T = np.array(pd.concat([dfTX],axis=1))\n",
    "# y_T = np.array(pd.concat([dfTY],axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87426, 1, 658) (5252, 1, 658)\n",
      "(87426, 1) (5252, 1)\n"
     ]
    }
   ],
   "source": [
    "##REGRESSION+CNN X & Y  flattern:\n",
    "X_T = np.expand_dims(X_T,axis=1)\n",
    "# print(X_T.shape)\n",
    "\n",
    "# y_T = np.expand_dims(y_T,axis=1)\n",
    "# print(y_T.shape)\n",
    "\n",
    "x_valid = np.expand_dims(x_valid,axis=1)\n",
    "# y_valid = np.expand_dims(y_valid,axis=1)\n",
    "\n",
    "print(X_T.shape, x_valid.shape)\n",
    "print(y_T.shape, y_valid.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>649</th>\n",
       "      <th>650</th>\n",
       "      <th>651</th>\n",
       "      <th>652</th>\n",
       "      <th>653</th>\n",
       "      <th>654</th>\n",
       "      <th>655</th>\n",
       "      <th>656</th>\n",
       "      <th>657</th>\n",
       "      <th>658</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.607957</td>\n",
       "      <td>-1.627399</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>0.511305</td>\n",
       "      <td>0.580310</td>\n",
       "      <td>0.668478</td>\n",
       "      <td>0.650075</td>\n",
       "      <td>0.649025</td>\n",
       "      <td>0.539881</td>\n",
       "      <td>0.617453</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137918</td>\n",
       "      <td>-0.062177</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>1.724257</td>\n",
       "      <td>-0.449732</td>\n",
       "      <td>0.454312</td>\n",
       "      <td>-0.370140</td>\n",
       "      <td>0.579528</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.607957</td>\n",
       "      <td>-1.627399</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>-1.595845</td>\n",
       "      <td>-1.643784</td>\n",
       "      <td>0.347490</td>\n",
       "      <td>-1.778815</td>\n",
       "      <td>-1.786890</td>\n",
       "      <td>-1.956558</td>\n",
       "      <td>-1.899808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.210614</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>0.417328</td>\n",
       "      <td>-0.965921</td>\n",
       "      <td>-0.686831</td>\n",
       "      <td>0.370406</td>\n",
       "      <td>-1.229952</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.598124</td>\n",
       "      <td>0.579476</td>\n",
       "      <td>0.622577</td>\n",
       "      <td>0.591911</td>\n",
       "      <td>0.580310</td>\n",
       "      <td>0.548051</td>\n",
       "      <td>0.566257</td>\n",
       "      <td>0.570600</td>\n",
       "      <td>0.580139</td>\n",
       "      <td>0.620127</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.186996</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.069362</td>\n",
       "      <td>0.424020</td>\n",
       "      <td>-0.673693</td>\n",
       "      <td>0.366344</td>\n",
       "      <td>-0.483746</td>\n",
       "      <td>-0.848626</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.506025</td>\n",
       "      <td>0.441419</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>0.511305</td>\n",
       "      <td>0.471573</td>\n",
       "      <td>0.421760</td>\n",
       "      <td>0.238228</td>\n",
       "      <td>0.428210</td>\n",
       "      <td>-1.956558</td>\n",
       "      <td>0.396392</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.269816</td>\n",
       "      <td>-0.082533</td>\n",
       "      <td>-0.115570</td>\n",
       "      <td>-0.070630</td>\n",
       "      <td>0.765997</td>\n",
       "      <td>0.043775</td>\n",
       "      <td>0.438054</td>\n",
       "      <td>0.752307</td>\n",
       "      <td>-0.379287</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.607957</td>\n",
       "      <td>-1.627399</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>-1.595845</td>\n",
       "      <td>-1.643784</td>\n",
       "      <td>-1.700526</td>\n",
       "      <td>0.614246</td>\n",
       "      <td>0.484192</td>\n",
       "      <td>0.358576</td>\n",
       "      <td>-1.899808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.243850</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>2.028875</td>\n",
       "      <td>-0.914609</td>\n",
       "      <td>-2.813876</td>\n",
       "      <td>-0.624023</td>\n",
       "      <td>-0.694283</td>\n",
       "      <td>2.889923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.607957</td>\n",
       "      <td>-1.627399</td>\n",
       "      <td>0.431237</td>\n",
       "      <td>-1.595845</td>\n",
       "      <td>0.507108</td>\n",
       "      <td>0.465205</td>\n",
       "      <td>0.554720</td>\n",
       "      <td>0.445104</td>\n",
       "      <td>0.459765</td>\n",
       "      <td>0.454423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.292731</td>\n",
       "      <td>0.482125</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>1.264305</td>\n",
       "      <td>2.970149</td>\n",
       "      <td>0.489925</td>\n",
       "      <td>-0.208938</td>\n",
       "      <td>1.907721</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.614993</td>\n",
       "      <td>0.691877</td>\n",
       "      <td>0.632952</td>\n",
       "      <td>0.668106</td>\n",
       "      <td>0.667615</td>\n",
       "      <td>0.594209</td>\n",
       "      <td>0.560641</td>\n",
       "      <td>0.512945</td>\n",
       "      <td>0.569462</td>\n",
       "      <td>0.465251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.224634</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>-0.362303</td>\n",
       "      <td>-0.618683</td>\n",
       "      <td>0.666980</td>\n",
       "      <td>-0.470970</td>\n",
       "      <td>-0.848626</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.920342</td>\n",
       "      <td>0.898791</td>\n",
       "      <td>0.879251</td>\n",
       "      <td>0.905412</td>\n",
       "      <td>0.916879</td>\n",
       "      <td>0.899657</td>\n",
       "      <td>0.848372</td>\n",
       "      <td>0.825405</td>\n",
       "      <td>0.768850</td>\n",
       "      <td>0.773581</td>\n",
       "      <td>...</td>\n",
       "      <td>1.106511</td>\n",
       "      <td>0.404124</td>\n",
       "      <td>6.367049</td>\n",
       "      <td>-0.070600</td>\n",
       "      <td>-0.576254</td>\n",
       "      <td>-0.178722</td>\n",
       "      <td>0.708942</td>\n",
       "      <td>-0.644264</td>\n",
       "      <td>0.867729</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.561971</td>\n",
       "      <td>0.547080</td>\n",
       "      <td>0.389206</td>\n",
       "      <td>0.511305</td>\n",
       "      <td>0.417626</td>\n",
       "      <td>0.445669</td>\n",
       "      <td>0.314127</td>\n",
       "      <td>0.231850</td>\n",
       "      <td>-1.956558</td>\n",
       "      <td>0.317648</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.250070</td>\n",
       "      <td>-0.115661</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>-0.336546</td>\n",
       "      <td>-0.666718</td>\n",
       "      <td>-0.481082</td>\n",
       "      <td>0.115131</td>\n",
       "      <td>-0.583378</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.607957</td>\n",
       "      <td>0.441419</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>-1.595845</td>\n",
       "      <td>-1.643784</td>\n",
       "      <td>0.273221</td>\n",
       "      <td>0.238228</td>\n",
       "      <td>0.307812</td>\n",
       "      <td>-1.956558</td>\n",
       "      <td>-1.899808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.288243</td>\n",
       "      <td>-0.035782</td>\n",
       "      <td>-0.043012</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>-0.099951</td>\n",
       "      <td>1.883814</td>\n",
       "      <td>0.467182</td>\n",
       "      <td>1.340317</td>\n",
       "      <td>1.149260</td>\n",
       "      <td>2.889923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.653560</td>\n",
       "      <td>0.624122</td>\n",
       "      <td>0.673324</td>\n",
       "      <td>0.679090</td>\n",
       "      <td>0.696333</td>\n",
       "      <td>0.660125</td>\n",
       "      <td>0.652592</td>\n",
       "      <td>0.674931</td>\n",
       "      <td>0.631449</td>\n",
       "      <td>0.634965</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.262520</td>\n",
       "      <td>1.006465</td>\n",
       "      <td>0.409100</td>\n",
       "      <td>-0.057594</td>\n",
       "      <td>0.159673</td>\n",
       "      <td>1.422975</td>\n",
       "      <td>0.372601</td>\n",
       "      <td>-0.540241</td>\n",
       "      <td>0.566741</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.609669</td>\n",
       "      <td>0.579476</td>\n",
       "      <td>0.604763</td>\n",
       "      <td>0.660082</td>\n",
       "      <td>0.628044</td>\n",
       "      <td>0.654171</td>\n",
       "      <td>0.610655</td>\n",
       "      <td>0.580136</td>\n",
       "      <td>0.506629</td>\n",
       "      <td>0.582954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.152740</td>\n",
       "      <td>0.141718</td>\n",
       "      <td>-0.052350</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>0.286771</td>\n",
       "      <td>-0.148517</td>\n",
       "      <td>0.539455</td>\n",
       "      <td>-0.539516</td>\n",
       "      <td>0.928174</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.490025</td>\n",
       "      <td>0.483770</td>\n",
       "      <td>0.431237</td>\n",
       "      <td>0.527376</td>\n",
       "      <td>-1.643784</td>\n",
       "      <td>-1.700526</td>\n",
       "      <td>0.238228</td>\n",
       "      <td>0.428210</td>\n",
       "      <td>-1.956558</td>\n",
       "      <td>0.350329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.231016</td>\n",
       "      <td>-0.074572</td>\n",
       "      <td>-0.093413</td>\n",
       "      <td>-0.032168</td>\n",
       "      <td>0.586972</td>\n",
       "      <td>-0.023267</td>\n",
       "      <td>0.386312</td>\n",
       "      <td>0.185528</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.738918</td>\n",
       "      <td>0.751676</td>\n",
       "      <td>0.754751</td>\n",
       "      <td>0.738418</td>\n",
       "      <td>0.716838</td>\n",
       "      <td>0.692388</td>\n",
       "      <td>0.666590</td>\n",
       "      <td>0.669006</td>\n",
       "      <td>0.641869</td>\n",
       "      <td>0.639510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.098001</td>\n",
       "      <td>-0.111748</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>-0.443750</td>\n",
       "      <td>-0.599724</td>\n",
       "      <td>0.806319</td>\n",
       "      <td>-0.575601</td>\n",
       "      <td>-0.379287</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.607957</td>\n",
       "      <td>-1.627399</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>-1.595845</td>\n",
       "      <td>-1.643784</td>\n",
       "      <td>0.688014</td>\n",
       "      <td>0.414460</td>\n",
       "      <td>-1.786890</td>\n",
       "      <td>-1.956558</td>\n",
       "      <td>-1.899808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293693</td>\n",
       "      <td>-0.070458</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>1.661716</td>\n",
       "      <td>0.774332</td>\n",
       "      <td>-2.863649</td>\n",
       "      <td>-0.658132</td>\n",
       "      <td>1.182687</td>\n",
       "      <td>2.889923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.471100</td>\n",
       "      <td>0.441419</td>\n",
       "      <td>0.532910</td>\n",
       "      <td>0.541296</td>\n",
       "      <td>0.544776</td>\n",
       "      <td>0.445669</td>\n",
       "      <td>0.314127</td>\n",
       "      <td>0.408230</td>\n",
       "      <td>0.438692</td>\n",
       "      <td>0.543994</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.291610</td>\n",
       "      <td>0.115111</td>\n",
       "      <td>-0.090733</td>\n",
       "      <td>-0.070990</td>\n",
       "      <td>0.658285</td>\n",
       "      <td>1.084217</td>\n",
       "      <td>0.412743</td>\n",
       "      <td>-0.205593</td>\n",
       "      <td>0.867729</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.471100</td>\n",
       "      <td>0.499871</td>\n",
       "      <td>0.389206</td>\n",
       "      <td>-1.595845</td>\n",
       "      <td>-1.643784</td>\n",
       "      <td>-1.700526</td>\n",
       "      <td>-1.778815</td>\n",
       "      <td>0.580136</td>\n",
       "      <td>0.172553</td>\n",
       "      <td>-1.899808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.245137</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>0.498054</td>\n",
       "      <td>-0.841691</td>\n",
       "      <td>0.042095</td>\n",
       "      <td>0.932143</td>\n",
       "      <td>-0.694283</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.447939</td>\n",
       "      <td>0.369021</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>0.613557</td>\n",
       "      <td>-1.643784</td>\n",
       "      <td>-1.700526</td>\n",
       "      <td>-1.778815</td>\n",
       "      <td>-1.786890</td>\n",
       "      <td>-1.956558</td>\n",
       "      <td>-1.899808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293693</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>-1.294753</td>\n",
       "      <td>-0.572524</td>\n",
       "      <td>0.568840</td>\n",
       "      <td>0.828600</td>\n",
       "      <td>-0.158615</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.607957</td>\n",
       "      <td>-1.627399</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>-1.595845</td>\n",
       "      <td>0.580310</td>\n",
       "      <td>0.465205</td>\n",
       "      <td>0.238228</td>\n",
       "      <td>0.428210</td>\n",
       "      <td>-1.956558</td>\n",
       "      <td>-1.899808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293693</td>\n",
       "      <td>-0.059555</td>\n",
       "      <td>-0.114954</td>\n",
       "      <td>-0.071167</td>\n",
       "      <td>0.666853</td>\n",
       "      <td>1.116566</td>\n",
       "      <td>-0.744922</td>\n",
       "      <td>0.522122</td>\n",
       "      <td>1.051454</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.799227</td>\n",
       "      <td>0.795797</td>\n",
       "      <td>0.812762</td>\n",
       "      <td>0.324514</td>\n",
       "      <td>0.612561</td>\n",
       "      <td>0.761161</td>\n",
       "      <td>0.871213</td>\n",
       "      <td>0.892494</td>\n",
       "      <td>0.865638</td>\n",
       "      <td>0.883021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293693</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>0.380103</td>\n",
       "      <td>-0.572524</td>\n",
       "      <td>0.830311</td>\n",
       "      <td>-0.631542</td>\n",
       "      <td>-0.158615</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.543047</td>\n",
       "      <td>0.640869</td>\n",
       "      <td>0.646793</td>\n",
       "      <td>0.660082</td>\n",
       "      <td>0.612561</td>\n",
       "      <td>0.622321</td>\n",
       "      <td>0.630619</td>\n",
       "      <td>0.560155</td>\n",
       "      <td>0.573132</td>\n",
       "      <td>0.575378</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056536</td>\n",
       "      <td>0.155921</td>\n",
       "      <td>-0.108733</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>0.503338</td>\n",
       "      <td>0.036443</td>\n",
       "      <td>0.480187</td>\n",
       "      <td>-0.486367</td>\n",
       "      <td>-0.226596</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.017993</td>\n",
       "      <td>0.999421</td>\n",
       "      <td>0.995552</td>\n",
       "      <td>0.999257</td>\n",
       "      <td>0.979782</td>\n",
       "      <td>0.962672</td>\n",
       "      <td>0.939774</td>\n",
       "      <td>0.927274</td>\n",
       "      <td>0.902080</td>\n",
       "      <td>0.903951</td>\n",
       "      <td>...</td>\n",
       "      <td>6.333814</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>-0.756435</td>\n",
       "      <td>-0.699903</td>\n",
       "      <td>0.345649</td>\n",
       "      <td>-0.655014</td>\n",
       "      <td>-1.259556</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.607957</td>\n",
       "      <td>0.586217</td>\n",
       "      <td>0.461058</td>\n",
       "      <td>0.511305</td>\n",
       "      <td>0.448007</td>\n",
       "      <td>0.445669</td>\n",
       "      <td>0.238228</td>\n",
       "      <td>-1.786890</td>\n",
       "      <td>-1.956558</td>\n",
       "      <td>0.492647</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293693</td>\n",
       "      <td>-0.093818</td>\n",
       "      <td>-0.105688</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>-0.028858</td>\n",
       "      <td>0.013613</td>\n",
       "      <td>0.404767</td>\n",
       "      <td>0.648433</td>\n",
       "      <td>1.391137</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.822036</td>\n",
       "      <td>0.788528</td>\n",
       "      <td>0.816189</td>\n",
       "      <td>0.807685</td>\n",
       "      <td>0.822726</td>\n",
       "      <td>0.784392</td>\n",
       "      <td>0.769520</td>\n",
       "      <td>0.758327</td>\n",
       "      <td>0.702495</td>\n",
       "      <td>0.707760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466681</td>\n",
       "      <td>-0.093129</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.066330</td>\n",
       "      <td>-0.651171</td>\n",
       "      <td>-0.640467</td>\n",
       "      <td>-0.076085</td>\n",
       "      <td>-0.633939</td>\n",
       "      <td>0.474658</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.375993</td>\n",
       "      <td>0.296622</td>\n",
       "      <td>0.745176</td>\n",
       "      <td>0.695160</td>\n",
       "      <td>0.606851</td>\n",
       "      <td>0.594209</td>\n",
       "      <td>-1.778815</td>\n",
       "      <td>-1.786890</td>\n",
       "      <td>0.252668</td>\n",
       "      <td>0.192842</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.281712</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.066329</td>\n",
       "      <td>-0.057971</td>\n",
       "      <td>-0.402819</td>\n",
       "      <td>-1.054917</td>\n",
       "      <td>-0.264972</td>\n",
       "      <td>-0.103748</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.561971</td>\n",
       "      <td>0.572269</td>\n",
       "      <td>0.669925</td>\n",
       "      <td>0.599636</td>\n",
       "      <td>0.521210</td>\n",
       "      <td>0.445669</td>\n",
       "      <td>0.566257</td>\n",
       "      <td>0.870327</td>\n",
       "      <td>0.798045</td>\n",
       "      <td>0.630231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020041</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.063960</td>\n",
       "      <td>0.867482</td>\n",
       "      <td>-0.633212</td>\n",
       "      <td>0.761197</td>\n",
       "      <td>-0.559179</td>\n",
       "      <td>-0.472911</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.506025</td>\n",
       "      <td>0.483770</td>\n",
       "      <td>0.519069</td>\n",
       "      <td>0.324514</td>\n",
       "      <td>-1.643784</td>\n",
       "      <td>0.465205</td>\n",
       "      <td>0.238228</td>\n",
       "      <td>0.383775</td>\n",
       "      <td>0.358576</td>\n",
       "      <td>0.350329</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101142</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>-0.686978</td>\n",
       "      <td>-0.639978</td>\n",
       "      <td>0.366259</td>\n",
       "      <td>-0.558222</td>\n",
       "      <td>-0.772316</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.532111</td>\n",
       "      <td>0.369021</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>0.324514</td>\n",
       "      <td>0.490828</td>\n",
       "      <td>0.421760</td>\n",
       "      <td>0.434424</td>\n",
       "      <td>0.307812</td>\n",
       "      <td>0.485556</td>\n",
       "      <td>0.192842</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.280952</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>0.713240</td>\n",
       "      <td>-0.642321</td>\n",
       "      <td>0.498457</td>\n",
       "      <td>1.033351</td>\n",
       "      <td>-0.694283</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.607957</td>\n",
       "      <td>-1.627399</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>-1.595845</td>\n",
       "      <td>-1.643784</td>\n",
       "      <td>-1.700526</td>\n",
       "      <td>0.358525</td>\n",
       "      <td>-1.786890</td>\n",
       "      <td>-1.956558</td>\n",
       "      <td>-1.899808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293693</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>1.804429</td>\n",
       "      <td>-0.572524</td>\n",
       "      <td>-2.678525</td>\n",
       "      <td>0.814062</td>\n",
       "      <td>-0.158615</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.678847</td>\n",
       "      <td>0.628567</td>\n",
       "      <td>0.616972</td>\n",
       "      <td>0.619877</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>0.576795</td>\n",
       "      <td>0.603100</td>\n",
       "      <td>0.542344</td>\n",
       "      <td>0.518807</td>\n",
       "      <td>0.543994</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179334</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>-0.602110</td>\n",
       "      <td>-0.640381</td>\n",
       "      <td>0.464321</td>\n",
       "      <td>-0.495179</td>\n",
       "      <td>-1.259556</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47236</th>\n",
       "      <td>0.633917</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.662773</td>\n",
       "      <td>0.675555</td>\n",
       "      <td>0.680054</td>\n",
       "      <td>0.654171</td>\n",
       "      <td>0.642156</td>\n",
       "      <td>0.565502</td>\n",
       "      <td>0.576688</td>\n",
       "      <td>0.582954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.210434</td>\n",
       "      <td>-0.116149</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>-0.046017</td>\n",
       "      <td>-0.596096</td>\n",
       "      <td>0.648432</td>\n",
       "      <td>-0.467318</td>\n",
       "      <td>-0.379287</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47237</th>\n",
       "      <td>0.543047</td>\n",
       "      <td>0.296622</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>0.655827</td>\n",
       "      <td>0.554841</td>\n",
       "      <td>0.390935</td>\n",
       "      <td>0.390026</td>\n",
       "      <td>0.408230</td>\n",
       "      <td>0.299533</td>\n",
       "      <td>0.192842</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293693</td>\n",
       "      <td>-0.069354</td>\n",
       "      <td>-0.115239</td>\n",
       "      <td>-0.071197</td>\n",
       "      <td>-0.505308</td>\n",
       "      <td>0.244595</td>\n",
       "      <td>0.821586</td>\n",
       "      <td>0.526118</td>\n",
       "      <td>0.290985</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47238</th>\n",
       "      <td>0.708004</td>\n",
       "      <td>0.734228</td>\n",
       "      <td>0.718646</td>\n",
       "      <td>0.746003</td>\n",
       "      <td>0.710435</td>\n",
       "      <td>0.676227</td>\n",
       "      <td>0.639384</td>\n",
       "      <td>0.535700</td>\n",
       "      <td>0.586745</td>\n",
       "      <td>0.590057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.369222</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.022425</td>\n",
       "      <td>0.162479</td>\n",
       "      <td>-0.813874</td>\n",
       "      <td>0.536445</td>\n",
       "      <td>-0.543352</td>\n",
       "      <td>-0.402866</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47239</th>\n",
       "      <td>-1.607957</td>\n",
       "      <td>0.556168</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>0.492298</td>\n",
       "      <td>0.374805</td>\n",
       "      <td>0.555991</td>\n",
       "      <td>0.478821</td>\n",
       "      <td>0.231850</td>\n",
       "      <td>0.299533</td>\n",
       "      <td>0.317648</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.232492</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.070097</td>\n",
       "      <td>0.365953</td>\n",
       "      <td>-0.889805</td>\n",
       "      <td>0.683139</td>\n",
       "      <td>0.384624</td>\n",
       "      <td>-1.259556</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47240</th>\n",
       "      <td>0.570279</td>\n",
       "      <td>0.369021</td>\n",
       "      <td>0.590921</td>\n",
       "      <td>0.439045</td>\n",
       "      <td>0.471573</td>\n",
       "      <td>0.390935</td>\n",
       "      <td>0.534756</td>\n",
       "      <td>0.352248</td>\n",
       "      <td>0.172553</td>\n",
       "      <td>0.375679</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.245358</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>0.298554</td>\n",
       "      <td>-0.299548</td>\n",
       "      <td>-0.323389</td>\n",
       "      <td>0.749098</td>\n",
       "      <td>-0.030080</td>\n",
       "      <td>-0.566139</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47241</th>\n",
       "      <td>-1.607957</td>\n",
       "      <td>0.296622</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>0.469036</td>\n",
       "      <td>0.507108</td>\n",
       "      <td>0.702600</td>\n",
       "      <td>0.603100</td>\n",
       "      <td>0.584609</td>\n",
       "      <td>0.561754</td>\n",
       "      <td>0.614716</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.134898</td>\n",
       "      <td>-0.024989</td>\n",
       "      <td>-0.059508</td>\n",
       "      <td>0.337163</td>\n",
       "      <td>0.827555</td>\n",
       "      <td>-0.167967</td>\n",
       "      <td>0.659311</td>\n",
       "      <td>-0.437250</td>\n",
       "      <td>0.731238</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47242</th>\n",
       "      <td>-1.607957</td>\n",
       "      <td>-1.627399</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>-1.595845</td>\n",
       "      <td>-1.643784</td>\n",
       "      <td>0.690223</td>\n",
       "      <td>0.606943</td>\n",
       "      <td>0.560155</td>\n",
       "      <td>0.379649</td>\n",
       "      <td>0.507817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293678</td>\n",
       "      <td>0.367326</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>1.696223</td>\n",
       "      <td>3.369392</td>\n",
       "      <td>0.435086</td>\n",
       "      <td>-0.054335</td>\n",
       "      <td>1.853338</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47243</th>\n",
       "      <td>-1.607957</td>\n",
       "      <td>-1.627399</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>-1.595845</td>\n",
       "      <td>-1.643784</td>\n",
       "      <td>-1.700526</td>\n",
       "      <td>-1.778815</td>\n",
       "      <td>-1.786890</td>\n",
       "      <td>-1.956558</td>\n",
       "      <td>-1.899808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.289852</td>\n",
       "      <td>-0.096814</td>\n",
       "      <td>-0.083695</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>-0.751949</td>\n",
       "      <td>1.532891</td>\n",
       "      <td>-2.860681</td>\n",
       "      <td>-0.652173</td>\n",
       "      <td>1.402214</td>\n",
       "      <td>2.889923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47244</th>\n",
       "      <td>0.471100</td>\n",
       "      <td>0.537125</td>\n",
       "      <td>0.545120</td>\n",
       "      <td>0.606829</td>\n",
       "      <td>0.521210</td>\n",
       "      <td>0.481722</td>\n",
       "      <td>0.571600</td>\n",
       "      <td>0.428210</td>\n",
       "      <td>0.459765</td>\n",
       "      <td>0.521198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162245</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.070618</td>\n",
       "      <td>0.525444</td>\n",
       "      <td>-0.817076</td>\n",
       "      <td>0.408919</td>\n",
       "      <td>-0.208133</td>\n",
       "      <td>-1.259556</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47245</th>\n",
       "      <td>0.872049</td>\n",
       "      <td>0.836079</td>\n",
       "      <td>0.861265</td>\n",
       "      <td>0.859307</td>\n",
       "      <td>0.831471</td>\n",
       "      <td>0.814989</td>\n",
       "      <td>0.775350</td>\n",
       "      <td>0.816267</td>\n",
       "      <td>0.779624</td>\n",
       "      <td>0.787113</td>\n",
       "      <td>...</td>\n",
       "      <td>4.655425</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>-0.425354</td>\n",
       "      <td>-0.976601</td>\n",
       "      <td>0.330739</td>\n",
       "      <td>-0.637836</td>\n",
       "      <td>-0.583378</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47246</th>\n",
       "      <td>0.762707</td>\n",
       "      <td>0.731014</td>\n",
       "      <td>0.751657</td>\n",
       "      <td>0.751350</td>\n",
       "      <td>0.712613</td>\n",
       "      <td>0.696590</td>\n",
       "      <td>0.677026</td>\n",
       "      <td>0.684146</td>\n",
       "      <td>0.651427</td>\n",
       "      <td>0.648088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042641</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.070027</td>\n",
       "      <td>-0.282523</td>\n",
       "      <td>-0.652044</td>\n",
       "      <td>0.228182</td>\n",
       "      <td>-0.588793</td>\n",
       "      <td>-1.259556</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47247</th>\n",
       "      <td>-1.607957</td>\n",
       "      <td>-1.627399</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>0.553575</td>\n",
       "      <td>0.533649</td>\n",
       "      <td>-1.700526</td>\n",
       "      <td>-1.778815</td>\n",
       "      <td>-1.786890</td>\n",
       "      <td>0.172553</td>\n",
       "      <td>-1.899808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.259212</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.083720</td>\n",
       "      <td>-0.017938</td>\n",
       "      <td>0.636876</td>\n",
       "      <td>-0.343432</td>\n",
       "      <td>-0.138371</td>\n",
       "      <td>1.274719</td>\n",
       "      <td>0.096451</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47248</th>\n",
       "      <td>0.577971</td>\n",
       "      <td>0.592549</td>\n",
       "      <td>0.616972</td>\n",
       "      <td>0.564559</td>\n",
       "      <td>0.580310</td>\n",
       "      <td>0.618118</td>\n",
       "      <td>0.576693</td>\n",
       "      <td>0.528627</td>\n",
       "      <td>0.518807</td>\n",
       "      <td>0.429073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.258207</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.072950</td>\n",
       "      <td>-0.071213</td>\n",
       "      <td>0.634436</td>\n",
       "      <td>-0.572492</td>\n",
       "      <td>0.604858</td>\n",
       "      <td>-0.238906</td>\n",
       "      <td>-0.440546</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47249</th>\n",
       "      <td>0.778886</td>\n",
       "      <td>0.727698</td>\n",
       "      <td>0.771598</td>\n",
       "      <td>0.759695</td>\n",
       "      <td>0.740817</td>\n",
       "      <td>0.688014</td>\n",
       "      <td>0.691896</td>\n",
       "      <td>0.699001</td>\n",
       "      <td>0.676113</td>\n",
       "      <td>0.680252</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295062</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.067797</td>\n",
       "      <td>-0.483037</td>\n",
       "      <td>-0.715690</td>\n",
       "      <td>0.241887</td>\n",
       "      <td>-0.605102</td>\n",
       "      <td>-1.259556</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47250</th>\n",
       "      <td>-1.607957</td>\n",
       "      <td>0.464727</td>\n",
       "      <td>0.431237</td>\n",
       "      <td>-1.595845</td>\n",
       "      <td>0.533649</td>\n",
       "      <td>0.347490</td>\n",
       "      <td>-1.778815</td>\n",
       "      <td>0.580136</td>\n",
       "      <td>0.299533</td>\n",
       "      <td>0.543994</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293693</td>\n",
       "      <td>0.177614</td>\n",
       "      <td>-0.108493</td>\n",
       "      <td>-0.070551</td>\n",
       "      <td>1.263224</td>\n",
       "      <td>1.320187</td>\n",
       "      <td>0.494840</td>\n",
       "      <td>-0.234010</td>\n",
       "      <td>-0.095731</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47251</th>\n",
       "      <td>0.532111</td>\n",
       "      <td>0.526120</td>\n",
       "      <td>0.611047</td>\n",
       "      <td>0.324514</td>\n",
       "      <td>0.471573</td>\n",
       "      <td>0.445669</td>\n",
       "      <td>0.478821</td>\n",
       "      <td>0.445104</td>\n",
       "      <td>0.438692</td>\n",
       "      <td>0.396392</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.276074</td>\n",
       "      <td>0.066676</td>\n",
       "      <td>-0.115526</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>0.092744</td>\n",
       "      <td>1.301658</td>\n",
       "      <td>0.830275</td>\n",
       "      <td>0.152005</td>\n",
       "      <td>0.579528</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47252</th>\n",
       "      <td>0.604057</td>\n",
       "      <td>0.564529</td>\n",
       "      <td>0.556042</td>\n",
       "      <td>0.511305</td>\n",
       "      <td>0.471573</td>\n",
       "      <td>0.496030</td>\n",
       "      <td>0.500795</td>\n",
       "      <td>0.428210</td>\n",
       "      <td>0.518807</td>\n",
       "      <td>0.465251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.275926</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>0.100108</td>\n",
       "      <td>-0.606244</td>\n",
       "      <td>0.588558</td>\n",
       "      <td>-0.044292</td>\n",
       "      <td>-0.694283</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47253</th>\n",
       "      <td>0.543047</td>\n",
       "      <td>0.572269</td>\n",
       "      <td>0.565922</td>\n",
       "      <td>0.541296</td>\n",
       "      <td>0.554841</td>\n",
       "      <td>0.555991</td>\n",
       "      <td>0.500795</td>\n",
       "      <td>0.484192</td>\n",
       "      <td>0.449708</td>\n",
       "      <td>0.465251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293693</td>\n",
       "      <td>0.332960</td>\n",
       "      <td>-0.109221</td>\n",
       "      <td>-0.070798</td>\n",
       "      <td>-0.972921</td>\n",
       "      <td>1.254329</td>\n",
       "      <td>0.782670</td>\n",
       "      <td>-0.356756</td>\n",
       "      <td>1.182687</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47254</th>\n",
       "      <td>0.604057</td>\n",
       "      <td>0.564529</td>\n",
       "      <td>0.666410</td>\n",
       "      <td>0.591911</td>\n",
       "      <td>0.594412</td>\n",
       "      <td>0.570299</td>\n",
       "      <td>0.500795</td>\n",
       "      <td>0.494637</td>\n",
       "      <td>0.459765</td>\n",
       "      <td>0.500485</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.257623</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071027</td>\n",
       "      <td>0.048152</td>\n",
       "      <td>-0.622235</td>\n",
       "      <td>0.412429</td>\n",
       "      <td>-0.251117</td>\n",
       "      <td>0.340446</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47255</th>\n",
       "      <td>0.797453</td>\n",
       "      <td>0.784695</td>\n",
       "      <td>0.789412</td>\n",
       "      <td>0.786428</td>\n",
       "      <td>0.777989</td>\n",
       "      <td>0.746692</td>\n",
       "      <td>0.733358</td>\n",
       "      <td>0.717427</td>\n",
       "      <td>0.683294</td>\n",
       "      <td>0.665305</td>\n",
       "      <td>...</td>\n",
       "      <td>2.443443</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.066186</td>\n",
       "      <td>-0.811380</td>\n",
       "      <td>-0.964799</td>\n",
       "      <td>0.245468</td>\n",
       "      <td>-0.624202</td>\n",
       "      <td>-0.583378</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47256</th>\n",
       "      <td>0.561971</td>\n",
       "      <td>0.513818</td>\n",
       "      <td>0.532910</td>\n",
       "      <td>0.564559</td>\n",
       "      <td>0.507108</td>\n",
       "      <td>0.496030</td>\n",
       "      <td>0.500795</td>\n",
       "      <td>0.494637</td>\n",
       "      <td>0.449708</td>\n",
       "      <td>0.429073</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225895</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.070085</td>\n",
       "      <td>-0.911154</td>\n",
       "      <td>-0.682311</td>\n",
       "      <td>0.209756</td>\n",
       "      <td>-0.373851</td>\n",
       "      <td>-1.259556</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47257</th>\n",
       "      <td>0.689502</td>\n",
       "      <td>0.632831</td>\n",
       "      <td>0.697122</td>\n",
       "      <td>0.756437</td>\n",
       "      <td>0.726715</td>\n",
       "      <td>0.671124</td>\n",
       "      <td>0.639384</td>\n",
       "      <td>0.680553</td>\n",
       "      <td>0.641869</td>\n",
       "      <td>0.654122</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004530</td>\n",
       "      <td>-0.030620</td>\n",
       "      <td>-0.105048</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>0.450228</td>\n",
       "      <td>-0.522647</td>\n",
       "      <td>0.579191</td>\n",
       "      <td>-0.565865</td>\n",
       "      <td>0.611203</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47258</th>\n",
       "      <td>0.657079</td>\n",
       "      <td>0.689476</td>\n",
       "      <td>0.704804</td>\n",
       "      <td>0.660082</td>\n",
       "      <td>0.708212</td>\n",
       "      <td>0.651065</td>\n",
       "      <td>0.655053</td>\n",
       "      <td>0.604590</td>\n",
       "      <td>0.583489</td>\n",
       "      <td>0.614716</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.153787</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>-0.241564</td>\n",
       "      <td>-0.630863</td>\n",
       "      <td>0.455924</td>\n",
       "      <td>-0.534454</td>\n",
       "      <td>-2.043131</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47259</th>\n",
       "      <td>-1.607957</td>\n",
       "      <td>-1.627399</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>-1.595845</td>\n",
       "      <td>-1.643784</td>\n",
       "      <td>-1.700526</td>\n",
       "      <td>-1.778815</td>\n",
       "      <td>-1.786890</td>\n",
       "      <td>-1.956558</td>\n",
       "      <td>-1.899808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293448</td>\n",
       "      <td>-0.021418</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>0.041805</td>\n",
       "      <td>2.818484</td>\n",
       "      <td>-2.802832</td>\n",
       "      <td>-0.605853</td>\n",
       "      <td>1.853338</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47260</th>\n",
       "      <td>-1.607957</td>\n",
       "      <td>-1.627399</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>-1.595845</td>\n",
       "      <td>0.448007</td>\n",
       "      <td>-1.700526</td>\n",
       "      <td>-1.778815</td>\n",
       "      <td>-1.786890</td>\n",
       "      <td>-1.956558</td>\n",
       "      <td>-1.899808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.292694</td>\n",
       "      <td>0.366898</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>-0.892153</td>\n",
       "      <td>2.903972</td>\n",
       "      <td>-0.330771</td>\n",
       "      <td>-0.215810</td>\n",
       "      <td>1.853338</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47261</th>\n",
       "      <td>0.304046</td>\n",
       "      <td>-1.627399</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>0.591911</td>\n",
       "      <td>-1.643784</td>\n",
       "      <td>0.347490</td>\n",
       "      <td>-1.778815</td>\n",
       "      <td>-1.786890</td>\n",
       "      <td>0.252668</td>\n",
       "      <td>-1.899808</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.291209</td>\n",
       "      <td>-0.110403</td>\n",
       "      <td>-0.116339</td>\n",
       "      <td>0.001786</td>\n",
       "      <td>-0.015287</td>\n",
       "      <td>0.104277</td>\n",
       "      <td>-0.182080</td>\n",
       "      <td>1.544385</td>\n",
       "      <td>0.093944</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47262</th>\n",
       "      <td>0.729025</td>\n",
       "      <td>0.700966</td>\n",
       "      <td>0.642382</td>\n",
       "      <td>0.660082</td>\n",
       "      <td>0.617978</td>\n",
       "      <td>0.673706</td>\n",
       "      <td>0.614246</td>\n",
       "      <td>0.649025</td>\n",
       "      <td>0.569462</td>\n",
       "      <td>0.614716</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.244730</td>\n",
       "      <td>0.328043</td>\n",
       "      <td>1.584311</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>-0.090351</td>\n",
       "      <td>0.864716</td>\n",
       "      <td>0.806120</td>\n",
       "      <td>-0.512362</td>\n",
       "      <td>0.928174</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47263</th>\n",
       "      <td>0.620057</td>\n",
       "      <td>0.707298</td>\n",
       "      <td>0.655091</td>\n",
       "      <td>0.636820</td>\n",
       "      <td>0.554841</td>\n",
       "      <td>0.647866</td>\n",
       "      <td>0.548462</td>\n",
       "      <td>0.560155</td>\n",
       "      <td>0.438692</td>\n",
       "      <td>0.396392</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196681</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.072985</td>\n",
       "      <td>-0.071268</td>\n",
       "      <td>0.007166</td>\n",
       "      <td>-0.619169</td>\n",
       "      <td>0.818133</td>\n",
       "      <td>-0.183311</td>\n",
       "      <td>0.619786</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47264</th>\n",
       "      <td>0.506025</td>\n",
       "      <td>-1.627399</td>\n",
       "      <td>-1.592148</td>\n",
       "      <td>0.324514</td>\n",
       "      <td>0.544776</td>\n",
       "      <td>-1.700526</td>\n",
       "      <td>0.590691</td>\n",
       "      <td>0.428210</td>\n",
       "      <td>0.412900</td>\n",
       "      <td>0.492647</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.293693</td>\n",
       "      <td>0.066543</td>\n",
       "      <td>-0.111538</td>\n",
       "      <td>-0.046598</td>\n",
       "      <td>0.095375</td>\n",
       "      <td>0.827524</td>\n",
       "      <td>0.495995</td>\n",
       "      <td>-0.167773</td>\n",
       "      <td>0.535242</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47265</th>\n",
       "      <td>0.591831</td>\n",
       "      <td>0.586217</td>\n",
       "      <td>0.389206</td>\n",
       "      <td>0.606829</td>\n",
       "      <td>0.374805</td>\n",
       "      <td>0.347490</td>\n",
       "      <td>0.500795</td>\n",
       "      <td>0.408230</td>\n",
       "      <td>0.358576</td>\n",
       "      <td>0.454423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.285364</td>\n",
       "      <td>-0.116413</td>\n",
       "      <td>-0.116500</td>\n",
       "      <td>-0.071118</td>\n",
       "      <td>-0.080821</td>\n",
       "      <td>-0.576811</td>\n",
       "      <td>0.767868</td>\n",
       "      <td>0.095694</td>\n",
       "      <td>0.197158</td>\n",
       "      <td>-0.346008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47266 rows  659 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0     -1.607957 -1.627399 -1.592148  0.511305  0.580310  0.668478  0.650075   \n",
       "1     -1.607957 -1.627399 -1.592148 -1.595845 -1.643784  0.347490 -1.778815   \n",
       "2      0.598124  0.579476  0.622577  0.591911  0.580310  0.548051  0.566257   \n",
       "3      0.506025  0.441419 -1.592148  0.511305  0.471573  0.421760  0.238228   \n",
       "4     -1.607957 -1.627399 -1.592148 -1.595845 -1.643784 -1.700526  0.614246   \n",
       "5     -1.607957 -1.627399  0.431237 -1.595845  0.507108  0.465205  0.554720   \n",
       "6      0.614993  0.691877  0.632952  0.668106  0.667615  0.594209  0.560641   \n",
       "7      0.920342  0.898791  0.879251  0.905412  0.916879  0.899657  0.848372   \n",
       "8      0.561971  0.547080  0.389206  0.511305  0.417626  0.445669  0.314127   \n",
       "9     -1.607957  0.441419 -1.592148 -1.595845 -1.643784  0.273221  0.238228   \n",
       "10     0.653560  0.624122  0.673324  0.679090  0.696333  0.660125  0.652592   \n",
       "11     0.609669  0.579476  0.604763  0.660082  0.628044  0.654171  0.610655   \n",
       "12     0.490025  0.483770  0.431237  0.527376 -1.643784 -1.700526  0.238228   \n",
       "13     0.738918  0.751676  0.754751  0.738418  0.716838  0.692388  0.666590   \n",
       "14    -1.607957 -1.627399 -1.592148 -1.595845 -1.643784  0.688014  0.414460   \n",
       "15     0.471100  0.441419  0.532910  0.541296  0.544776  0.445669  0.314127   \n",
       "16     0.471100  0.499871  0.389206 -1.595845 -1.643784 -1.700526 -1.778815   \n",
       "17     0.447939  0.369021 -1.592148  0.613557 -1.643784 -1.700526 -1.778815   \n",
       "18    -1.607957 -1.627399 -1.592148 -1.595845  0.580310  0.465205  0.238228   \n",
       "19     0.799227  0.795797  0.812762  0.324514  0.612561  0.761161  0.871213   \n",
       "20     0.543047  0.640869  0.646793  0.660082  0.612561  0.622321  0.630619   \n",
       "21     1.017993  0.999421  0.995552  0.999257  0.979782  0.962672  0.939774   \n",
       "22    -1.607957  0.586217  0.461058  0.511305  0.448007  0.445669  0.238228   \n",
       "23     0.822036  0.788528  0.816189  0.807685  0.822726  0.784392  0.769520   \n",
       "24     0.375993  0.296622  0.745176  0.695160  0.606851  0.594209 -1.778815   \n",
       "25     0.561971  0.572269  0.669925  0.599636  0.521210  0.445669  0.566257   \n",
       "26     0.506025  0.483770  0.519069  0.324514 -1.643784  0.465205  0.238228   \n",
       "27     0.532111  0.369021 -1.592148  0.324514  0.490828  0.421760  0.434424   \n",
       "28    -1.607957 -1.627399 -1.592148 -1.595845 -1.643784 -1.700526  0.358525   \n",
       "29     0.678847  0.628567  0.616972  0.619877  0.617978  0.576795  0.603100   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "47236  0.633917  0.609524  0.662773  0.675555  0.680054  0.654171  0.642156   \n",
       "47237  0.543047  0.296622 -1.592148  0.655827  0.554841  0.390935  0.390026   \n",
       "47238  0.708004  0.734228  0.718646  0.746003  0.710435  0.676227  0.639384   \n",
       "47239 -1.607957  0.556168 -1.592148  0.492298  0.374805  0.555991  0.478821   \n",
       "47240  0.570279  0.369021  0.590921  0.439045  0.471573  0.390935  0.534756   \n",
       "47241 -1.607957  0.296622 -1.592148  0.469036  0.507108  0.702600  0.603100   \n",
       "47242 -1.607957 -1.627399 -1.592148 -1.595845 -1.643784  0.690223  0.606943   \n",
       "47243 -1.607957 -1.627399 -1.592148 -1.595845 -1.643784 -1.700526 -1.778815   \n",
       "47244  0.471100  0.537125  0.545120  0.606829  0.521210  0.481722  0.571600   \n",
       "47245  0.872049  0.836079  0.861265  0.859307  0.831471  0.814989  0.775350   \n",
       "47246  0.762707  0.731014  0.751657  0.751350  0.712613  0.696590  0.677026   \n",
       "47247 -1.607957 -1.627399 -1.592148  0.553575  0.533649 -1.700526 -1.778815   \n",
       "47248  0.577971  0.592549  0.616972  0.564559  0.580310  0.618118  0.576693   \n",
       "47249  0.778886  0.727698  0.771598  0.759695  0.740817  0.688014  0.691896   \n",
       "47250 -1.607957  0.464727  0.431237 -1.595845  0.533649  0.347490 -1.778815   \n",
       "47251  0.532111  0.526120  0.611047  0.324514  0.471573  0.445669  0.478821   \n",
       "47252  0.604057  0.564529  0.556042  0.511305  0.471573  0.496030  0.500795   \n",
       "47253  0.543047  0.572269  0.565922  0.541296  0.554841  0.555991  0.500795   \n",
       "47254  0.604057  0.564529  0.666410  0.591911  0.594412  0.570299  0.500795   \n",
       "47255  0.797453  0.784695  0.789412  0.786428  0.777989  0.746692  0.733358   \n",
       "47256  0.561971  0.513818  0.532910  0.564559  0.507108  0.496030  0.500795   \n",
       "47257  0.689502  0.632831  0.697122  0.756437  0.726715  0.671124  0.639384   \n",
       "47258  0.657079  0.689476  0.704804  0.660082  0.708212  0.651065  0.655053   \n",
       "47259 -1.607957 -1.627399 -1.592148 -1.595845 -1.643784 -1.700526 -1.778815   \n",
       "47260 -1.607957 -1.627399 -1.592148 -1.595845  0.448007 -1.700526 -1.778815   \n",
       "47261  0.304046 -1.627399 -1.592148  0.591911 -1.643784  0.347490 -1.778815   \n",
       "47262  0.729025  0.700966  0.642382  0.660082  0.617978  0.673706  0.614246   \n",
       "47263  0.620057  0.707298  0.655091  0.636820  0.554841  0.647866  0.548462   \n",
       "47264  0.506025 -1.627399 -1.592148  0.324514  0.544776 -1.700526  0.590691   \n",
       "47265  0.591831  0.586217  0.389206  0.606829  0.374805  0.347490  0.500795   \n",
       "\n",
       "            7         8         9      ...          649       650       651  \\\n",
       "0      0.649025  0.539881  0.617453    ...    -0.137918 -0.062177 -0.116500   \n",
       "1     -1.786890 -1.956558 -1.899808    ...    -0.210614 -0.116413 -0.116500   \n",
       "2      0.570600  0.580139  0.620127    ...    -0.186996 -0.116413 -0.116500   \n",
       "3      0.428210 -1.956558  0.396392    ...    -0.269816 -0.082533 -0.115570   \n",
       "4      0.484192  0.358576 -1.899808    ...    -0.243850 -0.116413 -0.116500   \n",
       "5      0.445104  0.459765  0.454423    ...    -0.292731  0.482125  0.014724   \n",
       "6      0.512945  0.569462  0.465251    ...    -0.224634 -0.116413 -0.116500   \n",
       "7      0.825405  0.768850  0.773581    ...     1.106511  0.404124  6.367049   \n",
       "8      0.231850 -1.956558  0.317648    ...    -0.250070 -0.115661 -0.116500   \n",
       "9      0.307812 -1.956558 -1.899808    ...    -0.288243 -0.035782 -0.043012   \n",
       "10     0.674931  0.631449  0.634965    ...    -0.262520  1.006465  0.409100   \n",
       "11     0.580136  0.506629  0.582954    ...    -0.152740  0.141718 -0.052350   \n",
       "12     0.428210 -1.956558  0.350329    ...    -0.231016 -0.074572 -0.093413   \n",
       "13     0.669006  0.641869  0.639510    ...    -0.098001 -0.111748 -0.116500   \n",
       "14    -1.786890 -1.956558 -1.899808    ...    -0.293693 -0.070458 -0.116500   \n",
       "15     0.408230  0.438692  0.543994    ...    -0.291610  0.115111 -0.090733   \n",
       "16     0.580136  0.172553 -1.899808    ...    -0.245137 -0.116413 -0.116500   \n",
       "17    -1.786890 -1.956558 -1.899808    ...    -0.293693 -0.116413 -0.116500   \n",
       "18     0.428210 -1.956558 -1.899808    ...    -0.293693 -0.059555 -0.114954   \n",
       "19     0.892494  0.865638  0.883021    ...    -0.293693 -0.116413 -0.116500   \n",
       "20     0.560155  0.573132  0.575378    ...    -0.056536  0.155921 -0.108733   \n",
       "21     0.927274  0.902080  0.903951    ...     6.333814 -0.116413 -0.116500   \n",
       "22    -1.786890 -1.956558  0.492647    ...    -0.293693 -0.093818 -0.105688   \n",
       "23     0.758327  0.702495  0.707760    ...     0.466681 -0.093129 -0.116500   \n",
       "24    -1.786890  0.252668  0.192842    ...    -0.281712 -0.116413 -0.116500   \n",
       "25     0.870327  0.798045  0.630231    ...     0.020041 -0.116413 -0.116500   \n",
       "26     0.383775  0.358576  0.350329    ...    -0.101142 -0.116413 -0.116500   \n",
       "27     0.307812  0.485556  0.192842    ...    -0.280952 -0.116413 -0.116500   \n",
       "28    -1.786890 -1.956558 -1.899808    ...    -0.293693 -0.116413 -0.116500   \n",
       "29     0.542344  0.518807  0.543994    ...    -0.179334 -0.116413 -0.116500   \n",
       "...         ...       ...       ...    ...          ...       ...       ...   \n",
       "47236  0.565502  0.576688  0.582954    ...    -0.210434 -0.116149 -0.116500   \n",
       "47237  0.408230  0.299533  0.192842    ...    -0.293693 -0.069354 -0.115239   \n",
       "47238  0.535700  0.586745  0.590057    ...     0.369222 -0.116413 -0.116500   \n",
       "47239  0.231850  0.299533  0.317648    ...    -0.232492 -0.116413 -0.116500   \n",
       "47240  0.352248  0.172553  0.375679    ...    -0.245358 -0.116413 -0.116500   \n",
       "47241  0.584609  0.561754  0.614716    ...    -0.134898 -0.024989 -0.059508   \n",
       "47242  0.560155  0.379649  0.507817    ...    -0.293678  0.367326 -0.116500   \n",
       "47243 -1.786890 -1.956558 -1.899808    ...    -0.289852 -0.096814 -0.083695   \n",
       "47244  0.428210  0.459765  0.521198    ...    -0.162245 -0.116413 -0.116500   \n",
       "47245  0.816267  0.779624  0.787113    ...     4.655425 -0.116413 -0.116500   \n",
       "47246  0.684146  0.651427  0.648088    ...    -0.042641 -0.116413 -0.116500   \n",
       "47247 -1.786890  0.172553 -1.899808    ...    -0.259212 -0.116413 -0.083720   \n",
       "47248  0.528627  0.518807  0.429073    ...    -0.258207 -0.116413 -0.072950   \n",
       "47249  0.699001  0.676113  0.680252    ...     0.295062 -0.116413 -0.116500   \n",
       "47250  0.580136  0.299533  0.543994    ...    -0.293693  0.177614 -0.108493   \n",
       "47251  0.445104  0.438692  0.396392    ...    -0.276074  0.066676 -0.115526   \n",
       "47252  0.428210  0.518807  0.465251    ...    -0.275926 -0.116413 -0.116500   \n",
       "47253  0.484192  0.449708  0.465251    ...    -0.293693  0.332960 -0.109221   \n",
       "47254  0.494637  0.459765  0.500485    ...    -0.257623 -0.116413 -0.116500   \n",
       "47255  0.717427  0.683294  0.665305    ...     2.443443 -0.116413 -0.116500   \n",
       "47256  0.494637  0.449708  0.429073    ...    -0.225895 -0.116413 -0.116500   \n",
       "47257  0.680553  0.641869  0.654122    ...    -0.004530 -0.030620 -0.105048   \n",
       "47258  0.604590  0.583489  0.614716    ...    -0.153787 -0.116413 -0.116500   \n",
       "47259 -1.786890 -1.956558 -1.899808    ...    -0.293448 -0.021418 -0.116500   \n",
       "47260 -1.786890 -1.956558 -1.899808    ...    -0.292694  0.366898 -0.116500   \n",
       "47261 -1.786890  0.252668 -1.899808    ...    -0.291209 -0.110403 -0.116339   \n",
       "47262  0.649025  0.569462  0.614716    ...    -0.244730  0.328043  1.584311   \n",
       "47263  0.560155  0.438692  0.396392    ...    -0.196681 -0.116413 -0.072985   \n",
       "47264  0.428210  0.412900  0.492647    ...    -0.293693  0.066543 -0.111538   \n",
       "47265  0.408230  0.358576  0.454423    ...    -0.285364 -0.116413 -0.116500   \n",
       "\n",
       "            652       653       654       655       656       657       658  \n",
       "0     -0.071268  1.724257 -0.449732  0.454312 -0.370140  0.579528 -0.346008  \n",
       "1     -0.071268  0.417328 -0.965921 -0.686831  0.370406 -1.229952 -0.346008  \n",
       "2     -0.069362  0.424020 -0.673693  0.366344 -0.483746 -0.848626 -0.346008  \n",
       "3     -0.070630  0.765997  0.043775  0.438054  0.752307 -0.379287 -0.346008  \n",
       "4     -0.071268  2.028875 -0.914609 -2.813876 -0.624023 -0.694283  2.889923  \n",
       "5     -0.071268  1.264305  2.970149  0.489925 -0.208938  1.907721 -0.346008  \n",
       "6     -0.071268 -0.362303 -0.618683  0.666980 -0.470970 -0.848626 -0.346008  \n",
       "7     -0.070600 -0.576254 -0.178722  0.708942 -0.644264  0.867729 -0.346008  \n",
       "8     -0.071268 -0.336546 -0.666718 -0.481082  0.115131 -0.583378 -0.346008  \n",
       "9     -0.071268 -0.099951  1.883814  0.467182  1.340317  1.149260  2.889923  \n",
       "10    -0.057594  0.159673  1.422975  0.372601 -0.540241  0.566741 -0.346008  \n",
       "11    -0.071268  0.286771 -0.148517  0.539455 -0.539516  0.928174 -0.346008  \n",
       "12    -0.032168  0.586972 -0.023267  0.386312  0.185528  0.000360 -0.346008  \n",
       "13    -0.071268 -0.443750 -0.599724  0.806319 -0.575601 -0.379287 -0.346008  \n",
       "14    -0.071268  1.661716  0.774332 -2.863649 -0.658132  1.182687  2.889923  \n",
       "15    -0.070990  0.658285  1.084217  0.412743 -0.205593  0.867729 -0.346008  \n",
       "16    -0.071268  0.498054 -0.841691  0.042095  0.932143 -0.694283 -0.346008  \n",
       "17    -0.071268 -1.294753 -0.572524  0.568840  0.828600 -0.158615 -0.346008  \n",
       "18    -0.071167  0.666853  1.116566 -0.744922  0.522122  1.051454 -0.346008  \n",
       "19    -0.071268  0.380103 -0.572524  0.830311 -0.631542 -0.158615 -0.346008  \n",
       "20    -0.071268  0.503338  0.036443  0.480187 -0.486367 -0.226596 -0.346008  \n",
       "21    -0.071268 -0.756435 -0.699903  0.345649 -0.655014 -1.259556 -0.346008  \n",
       "22    -0.071268 -0.028858  0.013613  0.404767  0.648433  1.391137 -0.346008  \n",
       "23    -0.066330 -0.651171 -0.640467 -0.076085 -0.633939  0.474658 -0.346008  \n",
       "24    -0.066329 -0.057971 -0.402819 -1.054917 -0.264972 -0.103748 -0.346008  \n",
       "25    -0.063960  0.867482 -0.633212  0.761197 -0.559179 -0.472911 -0.346008  \n",
       "26    -0.071268 -0.686978 -0.639978  0.366259 -0.558222 -0.772316 -0.346008  \n",
       "27    -0.071268  0.713240 -0.642321  0.498457  1.033351 -0.694283 -0.346008  \n",
       "28    -0.071268  1.804429 -0.572524 -2.678525  0.814062 -0.158615 -0.346008  \n",
       "29    -0.071268 -0.602110 -0.640381  0.464321 -0.495179 -1.259556 -0.346008  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "47236 -0.071268 -0.046017 -0.596096  0.648432 -0.467318 -0.379287 -0.346008  \n",
       "47237 -0.071197 -0.505308  0.244595  0.821586  0.526118  0.290985 -0.346008  \n",
       "47238 -0.022425  0.162479 -0.813874  0.536445 -0.543352 -0.402866 -0.346008  \n",
       "47239 -0.070097  0.365953 -0.889805  0.683139  0.384624 -1.259556 -0.346008  \n",
       "47240  0.298554 -0.299548 -0.323389  0.749098 -0.030080 -0.566139 -0.346008  \n",
       "47241  0.337163  0.827555 -0.167967  0.659311 -0.437250  0.731238 -0.346008  \n",
       "47242 -0.071268  1.696223  3.369392  0.435086 -0.054335  1.853338 -0.346008  \n",
       "47243 -0.071268 -0.751949  1.532891 -2.860681 -0.652173  1.402214  2.889923  \n",
       "47244 -0.070618  0.525444 -0.817076  0.408919 -0.208133 -1.259556 -0.346008  \n",
       "47245 -0.071268 -0.425354 -0.976601  0.330739 -0.637836 -0.583378 -0.346008  \n",
       "47246 -0.070027 -0.282523 -0.652044  0.228182 -0.588793 -1.259556 -0.346008  \n",
       "47247 -0.017938  0.636876 -0.343432 -0.138371  1.274719  0.096451 -0.346008  \n",
       "47248 -0.071213  0.634436 -0.572492  0.604858 -0.238906 -0.440546 -0.346008  \n",
       "47249 -0.067797 -0.483037 -0.715690  0.241887 -0.605102 -1.259556 -0.346008  \n",
       "47250 -0.070551  1.263224  1.320187  0.494840 -0.234010 -0.095731 -0.346008  \n",
       "47251 -0.071268  0.092744  1.301658  0.830275  0.152005  0.579528 -0.346008  \n",
       "47252 -0.071268  0.100108 -0.606244  0.588558 -0.044292 -0.694283 -0.346008  \n",
       "47253 -0.070798 -0.972921  1.254329  0.782670 -0.356756  1.182687 -0.346008  \n",
       "47254 -0.071027  0.048152 -0.622235  0.412429 -0.251117  0.340446 -0.346008  \n",
       "47255 -0.066186 -0.811380 -0.964799  0.245468 -0.624202 -0.583378 -0.346008  \n",
       "47256 -0.070085 -0.911154 -0.682311  0.209756 -0.373851 -1.259556 -0.346008  \n",
       "47257 -0.071268  0.450228 -0.522647  0.579191 -0.565865  0.611203 -0.346008  \n",
       "47258 -0.071268 -0.241564 -0.630863  0.455924 -0.534454 -2.043131 -0.346008  \n",
       "47259 -0.071268  0.041805  2.818484 -2.802832 -0.605853  1.853338 -0.346008  \n",
       "47260 -0.071268 -0.892153  2.903972 -0.330771 -0.215810  1.853338 -0.346008  \n",
       "47261  0.001786 -0.015287  0.104277 -0.182080  1.544385  0.093944 -0.346008  \n",
       "47262 -0.071268 -0.090351  0.864716  0.806120 -0.512362  0.928174 -0.346008  \n",
       "47263 -0.071268  0.007166 -0.619169  0.818133 -0.183311  0.619786 -0.346008  \n",
       "47264 -0.046598  0.095375  0.827524  0.495995 -0.167773  0.535242 -0.346008  \n",
       "47265 -0.071118 -0.080821 -0.576811  0.767868  0.095694  0.197158 -0.346008  \n",
       "\n",
       "[47266 rows x 659 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dfT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 659) (5252, 659)\n",
      "(0, 1) (5252, 1)\n"
     ]
    }
   ],
   "source": [
    "# print(X_T.shape, x_valid.shape)\n",
    "# print(y_T.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNNbin_cat:\n",
    "y_T = to_categorical(df.ans.values, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87754, 1, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## dense\n",
    "y_T = np.expand_dims(y_T,axis=1)\n",
    "y_T.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "class GetBest(Callback):\n",
    "    \"\"\"Get the best model at the end of training.\n",
    "\t# Arguments\n",
    "        monitor: quantity to monitor.\n",
    "        verbose: verbosity mode, 0 or 1.\n",
    "        mode: one of {auto, min, max}.\n",
    "            The decision\n",
    "            to overwrite the current stored weights is made\n",
    "            based on either the maximization or the\n",
    "            minimization of the monitored quantity. For `val_acc`,\n",
    "            this should be `max`, for `val_loss` this should\n",
    "            be `min`, etc. In `auto` mode, the direction is\n",
    "            automatically inferred from the name of the monitored quantity.\n",
    "        period: Interval (number of epochs) between checkpoints.\n",
    "\t# Example\n",
    "\t\tcallbacks = [GetBest(monitor='val_acc', verbose=1, mode='max')]\n",
    "\t\tmode.fit(X, y, validation_data=(X_eval, Y_eval),\n",
    "                 callbacks=callbacks)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, monitor='val_loss', verbose=0,\n",
    "                 mode='auto', period=1):\n",
    "        super(GetBest, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.verbose = verbose\n",
    "        self.period = period\n",
    "        self.best_epochs = 0\n",
    "        self.epochs_since_last_save = 0\n",
    "\n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn('GetBest mode %s is unknown, '\n",
    "                          'fallback to auto mode.' % (mode),\n",
    "                          RuntimeWarning)\n",
    "            mode = 'auto'\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
    "                self.monitor_op = np.greater\n",
    "                self.best = -np.Inf\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "                self.best = np.Inf\n",
    "                \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epochs_since_last_save += 1\n",
    "        if self.epochs_since_last_save >= self.period:\n",
    "            self.epochs_since_last_save = 0\n",
    "            #filepath = self.filepath.format(epoch=epoch + 1, **logs)\n",
    "            current = logs.get(self.monitor)\n",
    "            if current is None:\n",
    "                warnings.warn('Can pick best model only with %s available, '\n",
    "                              'skipping.' % (self.monitor), RuntimeWarning)\n",
    "            else:\n",
    "                if self.monitor_op(current, self.best):\n",
    "                    if self.verbose > 0:\n",
    "                        print('\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n",
    "                              ' storing weights.'\n",
    "                              % (epoch + 1, self.monitor, self.best,\n",
    "                                 current))\n",
    "                    self.best = current\n",
    "                    self.best_epochs = epoch + 1\n",
    "                    self.best_weights = self.model.get_weights()\n",
    "                else:\n",
    "                    if self.verbose > 0:\n",
    "                        print('\\nEpoch %05d: %s did not improve' %\n",
    "                              (epoch + 1, self.monitor))            \n",
    "                    \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.verbose > 0:\n",
    "            print('Using epoch %05d with %s: %0.5f' % (self.best_epochs, self.monitor,\n",
    "                                                       self.best))\n",
    "        self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 8192)              5578752   \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 8192)              32768     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4096)              33558528  \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 4)                 16        \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 50,391,717\n",
      "Trainable params: 50,358,951\n",
      "Non-trainable params: 32,766\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "'''\n",
    "model.add(Conv1D(16,2,padding='same',input_shape=(1,658)))\n",
    "model.add(Conv1D(32,2,padding='same'))\n",
    "model.add(Conv1D(32,2,padding='same'))\n",
    "model.add(Conv1D(64,2,padding='same'))\n",
    "model.add(Conv1D(64,2,padding='same'))  #AF?\n",
    "model.add(Flatten())\n",
    "'''\n",
    "#dropout 0.001 0.5 decay\n",
    "# l2: 0.0001~0.01 decay\n",
    "model.add(Dense(8192,kernel_initializer='lecun_normal',input_dim=680,kernel_regularizer=regularizers.l2(0.00001))) #, kernel_regularizer=regularizers.l2(0.0001)\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('selu'))\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero'))\n",
    "model.add(Dense(4096,kernel_initializer='lecun_normal', kernel_regularizer=regularizers.l2(0.00001)))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dense(2048,kernel_initializer='lecun_normal', kernel_regularizer=regularizers.l2(0.00001)))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero'))\n",
    "model.add(Dense(1024,kernel_initializer='lecun_normal', kernel_regularizer=regularizers.l2(0.00001)))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dense(512,kernel_initializer='lecun_normal', kernel_regularizer=regularizers.l2(0.00001))) #,input_dim = 442\n",
    "model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero')) #, weights=None\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "# model.add(Dense(512,activation='relu',kernel_initializer='lecun_uniform'))#,activation='relu'))\n",
    "model.add(Dense(256,kernel_initializer='lecun_normal',kernel_regularizer=regularizers.l2(0.00001)))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "# model.add(Dense(256,activation='relu',kernel_initializer='lecun_uniform'))\n",
    "model.add(Dense(128,kernel_initializer='lecun_normal',kernel_regularizer=regularizers.l2(0.00001)))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dense(64,kernel_initializer='lecun_normal',kernel_regularizer=regularizers.l2(0.00001)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dense(32,kernel_initializer='lecun_normal',kernel_regularizer=regularizers.l2(0.00001)))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dropout(0.005))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dense(16,kernel_initializer='lecun_normal',kernel_regularizer=regularizers.l2(0.00001)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dense(8,kernel_initializer='lecun_normal',kernel_regularizer=regularizers.l2(0.00001)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dense(4,kernel_initializer='lecun_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('selu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dense(2,kernel_initializer='lecun_normal'))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "# model.add(Dense(128,kernel_initializer='lecun_uniform'))\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dense(1,kernel_initializer='uniform')) #,activation='sigmoid\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('sigmoid'))\n",
    "ADAM = Adam()\n",
    "model.compile(loss='binary_crossentropy', optimizer=ADAM, metrics=['accuracy']) #binary_crossentropy\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 8192)              5578752   \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 8192)              32768     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4096)              33558528  \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 2048)              8192      \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "activation_25 (Activation)   (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 4)                 16        \n",
      "_________________________________________________________________\n",
      "activation_26 (Activation)   (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "activation_27 (Activation)   (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 3         \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "activation_28 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 50,391,717\n",
      "Trainable params: 50,358,951\n",
      "Non-trainable params: 32,766\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "'''\n",
    "model.add(Conv1D(16,2,padding='same',input_shape=(1,658)))\n",
    "model.add(Conv1D(32,2,padding='same'))\n",
    "model.add(Conv1D(32,2,padding='same'))\n",
    "model.add(Conv1D(64,2,padding='same'))\n",
    "model.add(Conv1D(64,2,padding='same'))  #AF?\n",
    "model.add(Flatten())\n",
    "'''\n",
    "#dropout 0.001 0.5 decay\n",
    "# l2: 0.0001~0.01 decay\n",
    "model.add(Dense(8192,kernel_initializer='lecun_normal',input_dim=680,kernel_regularizer=regularizers.l2(0.00001))) #, kernel_regularizer=regularizers.l2(0.0001)\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('selu'))\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero'))\n",
    "model.add(Dense(4096,kernel_initializer='lecun_normal', kernel_regularizer=regularizers.l2(0.00001)))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(Dense(2048,kernel_initializer='lecun_normal', kernel_regularizer=regularizers.l2(0.00001)))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero'))\n",
    "model.add(Dense(1024,kernel_initializer='lecun_normal', kernel_regularizer=regularizers.l2(0.00001)))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dense(512,kernel_initializer='lecun_normal', kernel_regularizer=regularizers.l2(0.00001))) #,input_dim = 442\n",
    "model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero')) #, weights=None\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "# model.add(Dense(512,activation='relu',kernel_initializer='lecun_uniform'))#,activation='relu'))\n",
    "model.add(Dense(256,kernel_initializer='lecun_normal',kernel_regularizer=regularizers.l2(0.00001)))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "# model.add(Dense(256,activation='relu',kernel_initializer='lecun_uniform'))\n",
    "model.add(Dense(128,kernel_initializer='lecun_normal',kernel_regularizer=regularizers.l2(0.00001)))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dense(64,kernel_initializer='lecun_normal',kernel_regularizer=regularizers.l2(0.00001)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dense(32,kernel_initializer='lecun_normal',kernel_regularizer=regularizers.l2(0.00001)))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dropout(0.005))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dense(16,kernel_initializer='lecun_normal',kernel_regularizer=regularizers.l2(0.00001)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dense(8,kernel_initializer='lecun_normal',kernel_regularizer=regularizers.l2(0.00001)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dense(4,kernel_initializer='lecun_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('selu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dense(2,kernel_initializer='lecun_normal'))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Activation('selu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "# model.add(Dense(128,kernel_initializer='lecun_uniform'))\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dense(1,kernel_initializer='uniform')) #,activation='sigmoid\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('sigmoid'))\n",
    "ADAM = Adam()\n",
    "model.compile(loss='binary_crossentropy', optimizer=ADAM, metrics=['accuracy']) #binary_crossentropy\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 88010 samples, validate on 5252 samples\n",
      "Epoch 1/500\n",
      "88010/88010 [==============================] - 136s 2ms/step - loss: 0.5778 - acc: 0.7791 - val_loss: 0.5348 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53478, storing weights.\n",
      "Epoch 2/500\n",
      "88010/88010 [==============================] - 133s 2ms/step - loss: 0.5040 - acc: 0.7982 - val_loss: 0.5729 - val_acc: 0.8041\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/500\n",
      "88010/88010 [==============================] - 133s 2ms/step - loss: 0.4924 - acc: 0.8109 - val_loss: 0.4589 - val_acc: 0.8559\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.53478 to 0.45895, storing weights.\n",
      "Epoch 4/500\n",
      "88010/88010 [==============================] - 138s 2ms/step - loss: 0.4707 - acc: 0.8231 - val_loss: 0.4704 - val_acc: 0.8197\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/500\n",
      "88010/88010 [==============================] - 133s 2ms/step - loss: 0.4418 - acc: 0.8346 - val_loss: 0.4724 - val_acc: 0.8269\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/500\n",
      "88010/88010 [==============================] - 131s 1ms/step - loss: 0.4164 - acc: 0.8449 - val_loss: 0.4148 - val_acc: 0.8547\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.45895 to 0.41475, storing weights.\n",
      "Epoch 7/500\n",
      "88010/88010 [==============================] - 139s 2ms/step - loss: 0.3976 - acc: 0.8522 - val_loss: 0.4929 - val_acc: 0.7831\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/500\n",
      "88010/88010 [==============================] - 135s 2ms/step - loss: 0.3831 - acc: 0.8600 - val_loss: 0.4254 - val_acc: 0.8107\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/500\n",
      "88010/88010 [==============================] - 134s 2ms/step - loss: 0.3702 - acc: 0.8686 - val_loss: 0.3892 - val_acc: 0.8336\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.41475 to 0.38918, storing weights.\n",
      "Epoch 10/500\n",
      "88010/88010 [==============================] - 132s 1ms/step - loss: 0.3594 - acc: 0.8745 - val_loss: 0.3862 - val_acc: 0.8420\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.38918 to 0.38617, storing weights.\n",
      "Epoch 11/500\n",
      "88010/88010 [==============================] - 134s 2ms/step - loss: 0.3495 - acc: 0.8810 - val_loss: 0.4151 - val_acc: 0.8372\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/500\n",
      "88010/88010 [==============================] - 137s 2ms/step - loss: 0.3411 - acc: 0.8877 - val_loss: 0.3605 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.38617 to 0.36046, storing weights.\n",
      "Epoch 13/500\n",
      "88010/88010 [==============================] - 134s 2ms/step - loss: 0.3397 - acc: 0.8891 - val_loss: 0.3510 - val_acc: 0.8646\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.36046 to 0.35103, storing weights.\n",
      "Epoch 14/500\n",
      "88010/88010 [==============================] - 134s 2ms/step - loss: 0.3331 - acc: 0.8912 - val_loss: 0.4466 - val_acc: 0.8250\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/500\n",
      "88010/88010 [==============================] - 135s 2ms/step - loss: 0.3248 - acc: 0.8979 - val_loss: 0.3730 - val_acc: 0.8791\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/500\n",
      "88010/88010 [==============================] - 138s 2ms/step - loss: 0.3172 - acc: 0.9022 - val_loss: 0.3972 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/500\n",
      "88010/88010 [==============================] - 135s 2ms/step - loss: 0.3145 - acc: 0.9046 - val_loss: 0.4024 - val_acc: 0.8513\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/500\n",
      "88010/88010 [==============================] - 137s 2ms/step - loss: 0.3123 - acc: 0.9066 - val_loss: 0.3758 - val_acc: 0.8785\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/500\n",
      "88010/88010 [==============================] - 135s 2ms/step - loss: 0.3020 - acc: 0.9126 - val_loss: 0.5209 - val_acc: 0.8201\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/500\n",
      "88010/88010 [==============================] - 136s 2ms/step - loss: 0.3014 - acc: 0.9143 - val_loss: 0.3897 - val_acc: 0.8642\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/500\n",
      "88010/88010 [==============================] - 138s 2ms/step - loss: 0.2953 - acc: 0.9184 - val_loss: 0.3751 - val_acc: 0.8707\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/500\n",
      "88010/88010 [==============================] - 135s 2ms/step - loss: 0.2955 - acc: 0.9182 - val_loss: 0.3503 - val_acc: 0.8964\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.35103 to 0.35033, storing weights.\n",
      "Epoch 23/500\n",
      "88010/88010 [==============================] - 134s 2ms/step - loss: 0.2901 - acc: 0.9214 - val_loss: 0.3508 - val_acc: 0.8890\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/500\n",
      "88010/88010 [==============================] - 135s 2ms/step - loss: 0.2859 - acc: 0.9242 - val_loss: 0.3176 - val_acc: 0.9189\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.35033 to 0.31763, storing weights.\n",
      "Epoch 25/500\n",
      "88010/88010 [==============================] - 139s 2ms/step - loss: 0.2818 - acc: 0.9271 - val_loss: 0.3102 - val_acc: 0.9174\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.31763 to 0.31020, storing weights.\n",
      "Epoch 26/500\n",
      "88010/88010 [==============================] - 135s 2ms/step - loss: 0.2794 - acc: 0.9291 - val_loss: 0.3447 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/500\n",
      "88010/88010 [==============================] - 137s 2ms/step - loss: 0.2759 - acc: 0.9304 - val_loss: 0.3380 - val_acc: 0.9071\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/500\n",
      "88010/88010 [==============================] - 141s 2ms/step - loss: 0.2736 - acc: 0.9317 - val_loss: 0.3110 - val_acc: 0.9195\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/500\n",
      "88010/88010 [==============================] - 139s 2ms/step - loss: 0.2698 - acc: 0.9337 - val_loss: 0.3033 - val_acc: 0.9189\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.31020 to 0.30329, storing weights.\n",
      "Epoch 30/500\n",
      "88010/88010 [==============================] - 136s 2ms/step - loss: 0.2695 - acc: 0.9345 - val_loss: 0.3057 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/500\n",
      "88010/88010 [==============================] - 143s 2ms/step - loss: 0.2646 - acc: 0.9378 - val_loss: 0.3143 - val_acc: 0.9195\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/500\n",
      "88010/88010 [==============================] - 139s 2ms/step - loss: 0.2623 - acc: 0.9390 - val_loss: 0.2936 - val_acc: 0.9252\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.30329 to 0.29357, storing weights.\n",
      "Epoch 33/500\n",
      "88010/88010 [==============================] - 136s 2ms/step - loss: 0.2621 - acc: 0.9390 - val_loss: 0.3449 - val_acc: 0.9058\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/500\n",
      "88010/88010 [==============================] - 138s 2ms/step - loss: 0.2585 - acc: 0.9411 - val_loss: 0.2992 - val_acc: 0.9267\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/500\n",
      "88010/88010 [==============================] - 139s 2ms/step - loss: 0.2554 - acc: 0.9422 - val_loss: 0.3452 - val_acc: 0.9050\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/500\n",
      "88010/88010 [==============================] - 136s 2ms/step - loss: 0.2516 - acc: 0.9447 - val_loss: 0.2998 - val_acc: 0.9261\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/500\n",
      "88010/88010 [==============================] - 139s 2ms/step - loss: 0.2517 - acc: 0.9436 - val_loss: 0.2861 - val_acc: 0.9343\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.29357 to 0.28611, storing weights.\n",
      "Epoch 38/500\n",
      "88010/88010 [==============================] - 142s 2ms/step - loss: 0.2494 - acc: 0.9455 - val_loss: 0.3043 - val_acc: 0.9254\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/500\n",
      "88010/88010 [==============================] - 135s 2ms/step - loss: 0.2452 - acc: 0.9473 - val_loss: 0.3109 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/500\n",
      "88010/88010 [==============================] - 139s 2ms/step - loss: 0.2491 - acc: 0.9457 - val_loss: 0.3066 - val_acc: 0.9244\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/500\n",
      "88010/88010 [==============================] - 139s 2ms/step - loss: 0.2451 - acc: 0.9483 - val_loss: 0.3391 - val_acc: 0.9065\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/500\n",
      "88010/88010 [==============================] - 134s 2ms/step - loss: 0.2443 - acc: 0.9489 - val_loss: 0.3326 - val_acc: 0.9141\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/500\n",
      "88010/88010 [==============================] - 134s 2ms/step - loss: 0.2427 - acc: 0.9496 - val_loss: 0.2968 - val_acc: 0.9328\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88010/88010 [==============================] - 137s 2ms/step - loss: 0.2427 - acc: 0.9492 - val_loss: 0.3187 - val_acc: 0.9189\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/500\n",
      "88010/88010 [==============================] - 137s 2ms/step - loss: 0.2416 - acc: 0.9491 - val_loss: 0.3422 - val_acc: 0.9132\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/500\n",
      "88010/88010 [==============================] - 134s 2ms/step - loss: 0.2384 - acc: 0.9509 - val_loss: 0.3341 - val_acc: 0.9092\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/500\n",
      "88010/88010 [==============================] - 136s 2ms/step - loss: 0.2368 - acc: 0.9510 - val_loss: 0.3057 - val_acc: 0.9275\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/500\n",
      "88010/88010 [==============================] - 133s 2ms/step - loss: 0.2357 - acc: 0.9522 - val_loss: 0.3445 - val_acc: 0.9029\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/500\n",
      "88010/88010 [==============================] - 131s 1ms/step - loss: 0.2356 - acc: 0.9519 - val_loss: 0.3033 - val_acc: 0.9221\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/500\n",
      "88010/88010 [==============================] - 134s 2ms/step - loss: 0.2328 - acc: 0.9541 - val_loss: 0.3193 - val_acc: 0.9185\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/500\n",
      "88010/88010 [==============================] - 134s 2ms/step - loss: 0.2324 - acc: 0.9539 - val_loss: 0.3444 - val_acc: 0.9189\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/500\n",
      "88010/88010 [==============================] - 135s 2ms/step - loss: 0.2318 - acc: 0.9544 - val_loss: 0.3004 - val_acc: 0.9343\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/500\n",
      "88010/88010 [==============================] - 133s 2ms/step - loss: 0.2315 - acc: 0.9540 - val_loss: 0.3105 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/500\n",
      "88010/88010 [==============================] - 136s 2ms/step - loss: 0.2317 - acc: 0.9550 - val_loss: 0.3254 - val_acc: 0.9240\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/500\n",
      "88010/88010 [==============================] - 137s 2ms/step - loss: 0.2308 - acc: 0.9542 - val_loss: 0.3342 - val_acc: 0.9168\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/500\n",
      "88010/88010 [==============================] - 135s 2ms/step - loss: 0.2288 - acc: 0.9563 - val_loss: 0.3032 - val_acc: 0.9290\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/500\n",
      "88010/88010 [==============================] - 137s 2ms/step - loss: 0.2269 - acc: 0.9565 - val_loss: 0.3458 - val_acc: 0.9061\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/500\n",
      "88010/88010 [==============================] - 134s 2ms/step - loss: 0.2252 - acc: 0.9576 - val_loss: 0.3092 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/500\n",
      "88010/88010 [==============================] - 136s 2ms/step - loss: 0.2251 - acc: 0.9573 - val_loss: 0.2923 - val_acc: 0.9307\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/500\n",
      "88010/88010 [==============================] - 135s 2ms/step - loss: 0.2247 - acc: 0.9574 - val_loss: 0.3264 - val_acc: 0.9219\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/500\n",
      "88010/88010 [==============================] - 138s 2ms/step - loss: 0.2232 - acc: 0.9580 - val_loss: 0.3141 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/500\n",
      "88010/88010 [==============================] - 138s 2ms/step - loss: 0.2250 - acc: 0.9571 - val_loss: 0.3620 - val_acc: 0.8958\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Using epoch 00037 with val_loss: 0.28611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2460529908>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32#128\n",
    "early_stopping_monitor = EarlyStopping(patience=25) #10\n",
    "callback = GetBest(monitor='val_loss', verbose=1, mode='auto')\n",
    "# CW = {0:8.312,1:1.}\n",
    "model.fit(X_T, y_T, batch_size=batch_size, epochs=500, validation_data=(x_valid,y_valid), callbacks=[callback,early_stopping_monitor],shuffle=True)#,class_weight=CW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5252, 1)\n",
      "Valid AUC: 0.9224128518612779\n",
      "confusion matrix of valid: [[ 366  181]\n",
      " [ 164 4541]]\n",
      "(88010, 1)\n",
      "Train AUC: 0.9964232766337272\n",
      "confusion matrix of train: [[43884  1953]\n",
      " [  613 41560]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "valid_pred = model.predict(x_valid)\n",
    "# valid_pred = np.reshape(-1,1)\n",
    "print(valid_pred.shape)\n",
    "# y_valid = np.reshape(-1,1)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_valid, valid_pred, pos_label=1)\n",
    "valid_auc = metrics.auc(fpr, tpr)\n",
    "print('Valid AUC:', valid_auc)\n",
    "\n",
    "conf_mat = confusion_matrix(y_valid,np.round(valid_pred),labels=[1,0])\n",
    "print('confusion matrix of valid:',conf_mat)\n",
    "\n",
    "valid_pred = model.predict(X_T)\n",
    "# valid_pred = np.reshape(-1,1)\n",
    "print(valid_pred.shape)\n",
    "# y_valid = np.reshape(-1,1)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_T, valid_pred, pos_label=1)\n",
    "valid_auc = metrics.auc(fpr, tpr)\n",
    "print('Train AUC:', valid_auc)\n",
    "\n",
    "conf_mat = confusion_matrix(y_T,np.round(valid_pred),labels=[1,0])\n",
    "print('confusion matrix of train:',conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 87778 samples, validate on 5252 samples\n",
      "Epoch 1/500\n",
      "87778/87778 [==============================] - 135s 2ms/step - loss: 0.5624 - acc: 0.7803 - val_loss: 0.5345 - val_acc: 0.7854\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.53453, storing weights.\n",
      "Epoch 2/500\n",
      "87778/87778 [==============================] - 136s 2ms/step - loss: 0.4889 - acc: 0.8041 - val_loss: 0.5742 - val_acc: 0.8206\n",
      "\n",
      "Epoch 00002: val_loss did not improve\n",
      "Epoch 3/500\n",
      "87778/87778 [==============================] - 133s 2ms/step - loss: 0.4885 - acc: 0.8103 - val_loss: 0.5403 - val_acc: 0.7803\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/500\n",
      "87778/87778 [==============================] - 137s 2ms/step - loss: 0.4793 - acc: 0.8188 - val_loss: 0.4049 - val_acc: 0.8393\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.53453 to 0.40490, storing weights.\n",
      "Epoch 5/500\n",
      "87778/87778 [==============================] - 136s 2ms/step - loss: 0.4541 - acc: 0.8289 - val_loss: 0.4023 - val_acc: 0.8374\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.40490 to 0.40226, storing weights.\n",
      "Epoch 6/500\n",
      "87778/87778 [==============================] - 138s 2ms/step - loss: 0.4293 - acc: 0.8350 - val_loss: 0.4623 - val_acc: 0.8134\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/500\n",
      "87778/87778 [==============================] - 136s 2ms/step - loss: 0.4060 - acc: 0.8462 - val_loss: 0.3915 - val_acc: 0.8766\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.40226 to 0.39149, storing weights.\n",
      "Epoch 8/500\n",
      "87778/87778 [==============================] - 134s 2ms/step - loss: 0.3890 - acc: 0.8559 - val_loss: 0.3777 - val_acc: 0.8294\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.39149 to 0.37768, storing weights.\n",
      "Epoch 9/500\n",
      "87778/87778 [==============================] - 135s 2ms/step - loss: 0.3737 - acc: 0.8645 - val_loss: 0.3845 - val_acc: 0.8549\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/500\n",
      "87778/87778 [==============================] - 130s 1ms/step - loss: 0.3642 - acc: 0.8698 - val_loss: 0.3557 - val_acc: 0.8675\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.37768 to 0.35568, storing weights.\n",
      "Epoch 11/500\n",
      "87778/87778 [==============================] - 130s 1ms/step - loss: 0.3555 - acc: 0.8757 - val_loss: 0.3567 - val_acc: 0.8905\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/500\n",
      "87778/87778 [==============================] - 131s 1ms/step - loss: 0.3471 - acc: 0.8826 - val_loss: 0.3448 - val_acc: 0.8789\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.35568 to 0.34479, storing weights.\n",
      "Epoch 13/500\n",
      "87778/87778 [==============================] - 132s 2ms/step - loss: 0.3431 - acc: 0.8909 - val_loss: 0.3401 - val_acc: 0.8928\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.34479 to 0.34007, storing weights.\n",
      "Epoch 14/500\n",
      "87778/87778 [==============================] - 133s 2ms/step - loss: 0.3314 - acc: 0.8992 - val_loss: 0.3067 - val_acc: 0.9088\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.34007 to 0.30672, storing weights.\n",
      "Epoch 15/500\n",
      "87778/87778 [==============================] - 128s 1ms/step - loss: 0.3210 - acc: 0.9035 - val_loss: 0.3351 - val_acc: 0.9069\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/500\n",
      "87778/87778 [==============================] - 129s 1ms/step - loss: 0.3157 - acc: 0.9103 - val_loss: 0.3300 - val_acc: 0.9155\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/500\n",
      "87778/87778 [==============================] - 132s 2ms/step - loss: 0.3113 - acc: 0.9137 - val_loss: 0.3908 - val_acc: 0.8747\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/500\n",
      "87778/87778 [==============================] - 130s 1ms/step - loss: 0.3061 - acc: 0.9184 - val_loss: 0.3108 - val_acc: 0.9276\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/500\n",
      "87778/87778 [==============================] - 127s 1ms/step - loss: 0.3084 - acc: 0.9190 - val_loss: 0.3495 - val_acc: 0.8882\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/500\n",
      "87778/87778 [==============================] - 129s 1ms/step - loss: 0.2950 - acc: 0.9234 - val_loss: 0.3305 - val_acc: 0.9065\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/500\n",
      "87778/87778 [==============================] - 129s 1ms/step - loss: 0.3044 - acc: 0.9245 - val_loss: 0.3112 - val_acc: 0.9231\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/500\n",
      "87778/87778 [==============================] - 132s 2ms/step - loss: 0.3037 - acc: 0.9260 - val_loss: 0.4173 - val_acc: 0.9069\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/500\n",
      "87778/87778 [==============================] - 128s 1ms/step - loss: 0.3286 - acc: 0.9271 - val_loss: 0.3321 - val_acc: 0.9040\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/500\n",
      "87778/87778 [==============================] - 134s 2ms/step - loss: 0.2806 - acc: 0.9331 - val_loss: 0.2812 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.30672 to 0.28124, storing weights.\n",
      "Epoch 25/500\n",
      "87778/87778 [==============================] - 131s 1ms/step - loss: 0.3006 - acc: 0.9303 - val_loss: 1.1202 - val_acc: 0.7740\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/500\n",
      "87778/87778 [==============================] - 131s 1ms/step - loss: 0.3019 - acc: 0.9312 - val_loss: 0.3042 - val_acc: 0.9216\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/500\n",
      "87778/87778 [==============================] - 131s 1ms/step - loss: 0.3092 - acc: 0.9310 - val_loss: 0.3190 - val_acc: 0.9187\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/500\n",
      "87778/87778 [==============================] - 131s 1ms/step - loss: 0.2912 - acc: 0.9349 - val_loss: 0.2987 - val_acc: 0.9345\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/500\n",
      "87778/87778 [==============================] - 140s 2ms/step - loss: 0.2746 - acc: 0.9383 - val_loss: 0.3020 - val_acc: 0.9259\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/500\n",
      "87778/87778 [==============================] - 133s 2ms/step - loss: 0.2788 - acc: 0.9385 - val_loss: 0.2929 - val_acc: 0.9288\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/500\n",
      "87778/87778 [==============================] - 132s 1ms/step - loss: 0.2872 - acc: 0.9386 - val_loss: 0.2993 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/500\n",
      "87778/87778 [==============================] - 133s 2ms/step - loss: 0.2879 - acc: 0.9379 - val_loss: 0.3163 - val_acc: 0.9077\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/500\n",
      "87778/87778 [==============================] - 134s 2ms/step - loss: 0.2776 - acc: 0.9396 - val_loss: 0.3286 - val_acc: 0.9240\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/500\n",
      "87778/87778 [==============================] - 135s 2ms/step - loss: 0.2947 - acc: 0.9376 - val_loss: 0.3476 - val_acc: 0.9191\n",
      "\n",
      "Epoch 00034: val_loss did not improve\n",
      "Epoch 35/500\n",
      "87778/87778 [==============================] - 132s 2ms/step - loss: 0.2704 - acc: 0.9434 - val_loss: 0.3153 - val_acc: 0.9318\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/500\n",
      "87778/87778 [==============================] - 136s 2ms/step - loss: 0.2750 - acc: 0.9419 - val_loss: 0.3107 - val_acc: 0.9139\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/500\n",
      "87778/87778 [==============================] - 138s 2ms/step - loss: 0.2873 - acc: 0.9411 - val_loss: 0.3062 - val_acc: 0.9286\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/500\n",
      "87778/87778 [==============================] - 136s 2ms/step - loss: 0.2697 - acc: 0.9444 - val_loss: 0.3111 - val_acc: 0.9198\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/500\n",
      "87778/87778 [==============================] - 137s 2ms/step - loss: 0.2710 - acc: 0.9439 - val_loss: 0.3093 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/500\n",
      "87778/87778 [==============================] - 135s 2ms/step - loss: 0.2794 - acc: 0.9443 - val_loss: 0.3058 - val_acc: 0.9320\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/500\n",
      "87778/87778 [==============================] - 133s 2ms/step - loss: 0.2692 - acc: 0.9452 - val_loss: 0.2956 - val_acc: 0.9368\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/500\n",
      "87778/87778 [==============================] - 137s 2ms/step - loss: 0.2541 - acc: 0.9494 - val_loss: 0.3052 - val_acc: 0.9273\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/500\n",
      "87778/87778 [==============================] - 135s 2ms/step - loss: 0.2786 - acc: 0.9451 - val_loss: 0.3348 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/500\n",
      "87778/87778 [==============================] - 134s 2ms/step - loss: 0.2627 - acc: 0.9489 - val_loss: 0.4497 - val_acc: 0.9109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/500\n",
      "87778/87778 [==============================] - 138s 2ms/step - loss: 0.2823 - acc: 0.9447 - val_loss: 0.3261 - val_acc: 0.9236\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/500\n",
      "87778/87778 [==============================] - 137s 2ms/step - loss: 0.2709 - acc: 0.9476 - val_loss: 0.3158 - val_acc: 0.9250\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/500\n",
      "87778/87778 [==============================] - 136s 2ms/step - loss: 0.2886 - acc: 0.9440 - val_loss: 0.3441 - val_acc: 0.9347\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/500\n",
      "87778/87778 [==============================] - 134s 2ms/step - loss: 0.2704 - acc: 0.9496 - val_loss: 0.3070 - val_acc: 0.9353\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/500\n",
      "87778/87778 [==============================] - 131s 1ms/step - loss: 0.2584 - acc: 0.9493 - val_loss: 0.3389 - val_acc: 0.9335\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/500\n",
      "87778/87778 [==============================] - 137s 2ms/step - loss: 0.2602 - acc: 0.9503 - val_loss: 0.3185 - val_acc: 0.9337\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/500\n",
      "87778/87778 [==============================] - 140s 2ms/step - loss: 0.2480 - acc: 0.9531 - val_loss: 0.3122 - val_acc: 0.9181\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/500\n",
      "87778/87778 [==============================] - 135s 2ms/step - loss: 0.2507 - acc: 0.9526 - val_loss: 0.3078 - val_acc: 0.9362\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/500\n",
      "87778/87778 [==============================] - 134s 2ms/step - loss: 0.2482 - acc: 0.9524 - val_loss: 0.3036 - val_acc: 0.9334\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/500\n",
      "87778/87778 [==============================] - 133s 2ms/step - loss: 0.2526 - acc: 0.9510 - val_loss: 0.3128 - val_acc: 0.9233\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/500\n",
      "87778/87778 [==============================] - 134s 2ms/step - loss: 0.2574 - acc: 0.9518 - val_loss: 0.3021 - val_acc: 0.9320\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/500\n",
      "87778/87778 [==============================] - 136s 2ms/step - loss: 0.2350 - acc: 0.9568 - val_loss: 0.3093 - val_acc: 0.9280\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/500\n",
      "87778/87778 [==============================] - 138s 2ms/step - loss: 0.2392 - acc: 0.9548 - val_loss: 0.3069 - val_acc: 0.9240\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/500\n",
      "87778/87778 [==============================] - 137s 2ms/step - loss: 0.2418 - acc: 0.9548 - val_loss: 0.3064 - val_acc: 0.9288\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/500\n",
      "87778/87778 [==============================] - 138s 2ms/step - loss: 0.2529 - acc: 0.9531 - val_loss: 0.3200 - val_acc: 0.9219\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/500\n",
      "87778/87778 [==============================] - 132s 2ms/step - loss: 0.2595 - acc: 0.9523 - val_loss: 0.3079 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/500\n",
      "87778/87778 [==============================] - 132s 1ms/step - loss: 0.2540 - acc: 0.9541 - val_loss: 0.3034 - val_acc: 0.9292\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/500\n",
      "87778/87778 [==============================] - 131s 1ms/step - loss: 0.2334 - acc: 0.9574 - val_loss: 0.3099 - val_acc: 0.9271\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/500\n",
      "87778/87778 [==============================] - 133s 2ms/step - loss: 0.2772 - acc: 0.9496 - val_loss: 0.4793 - val_acc: 0.9269\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/500\n",
      "87778/87778 [==============================] - 127s 1ms/step - loss: 0.3230 - acc: 0.9481 - val_loss: 0.3221 - val_acc: 0.9383\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/500\n",
      "87778/87778 [==============================] - 130s 1ms/step - loss: 0.2602 - acc: 0.9570 - val_loss: 0.3082 - val_acc: 0.9322\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/500\n",
      "87778/87778 [==============================] - 129s 1ms/step - loss: 0.2321 - acc: 0.9581 - val_loss: 0.3199 - val_acc: 0.9195\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/500\n",
      "87778/87778 [==============================] - 131s 1ms/step - loss: 0.2407 - acc: 0.9568 - val_loss: 0.3721 - val_acc: 0.8966\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/500\n",
      "87778/87778 [==============================] - 131s 1ms/step - loss: 0.2372 - acc: 0.9582 - val_loss: 0.3032 - val_acc: 0.9202\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/500\n",
      "87778/87778 [==============================] - 131s 1ms/step - loss: 0.2323 - acc: 0.9589 - val_loss: 0.3115 - val_acc: 0.9284\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/500\n",
      "87778/87778 [==============================] - 129s 1ms/step - loss: 0.2297 - acc: 0.9601 - val_loss: 0.2997 - val_acc: 0.9330\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/500\n",
      "87778/87778 [==============================] - 129s 1ms/step - loss: 0.2284 - acc: 0.9598 - val_loss: 0.3096 - val_acc: 0.9263\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/500\n",
      "87778/87778 [==============================] - 130s 1ms/step - loss: 0.2323 - acc: 0.9575 - val_loss: 0.3177 - val_acc: 0.9309\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/500\n",
      "87778/87778 [==============================] - 126s 1ms/step - loss: 0.2267 - acc: 0.9607 - val_loss: 0.3038 - val_acc: 0.9313\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/500\n",
      "87778/87778 [==============================] - 126s 1ms/step - loss: 0.2253 - acc: 0.9613 - val_loss: 0.3247 - val_acc: 0.9315\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Using epoch 00024 with val_loss: 0.28124\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6baecfe1d0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32#128 \n",
    "early_stopping_monitor = EarlyStopping(patience=50) #,monitor='val_acc',mode='auto' #15\n",
    "# keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "callback = GetBest(monitor='val_loss', verbose=1, mode='auto')\n",
    "# CW = {0:8.312,1:1.}\n",
    "model.fit(X_T, y_T, batch_size=batch_size, epochs=500, validation_data=(x_valid,y_valid), callbacks=[callback,early_stopping_monitor],shuffle=True)#,class_weight=CW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5252, 1)\n",
      "Valid AUC: 0.9332187559405001\n",
      "confusion matrix of valid: [[ 378  198]\n",
      " [ 163 4513]]\n",
      "(87778, 1)\n",
      "Train AUC: 0.991448492984879\n",
      "confusion matrix of train: [[41499  4077]\n",
      " [  935 41267]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "valid_pred = model.predict(x_valid)\n",
    "# valid_pred = np.reshape(-1,1)\n",
    "print(valid_pred.shape)\n",
    "# y_valid = np.reshape(-1,1)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_valid, valid_pred, pos_label=1)\n",
    "valid_auc = metrics.auc(fpr, tpr)\n",
    "print('Valid AUC:', valid_auc)\n",
    "\n",
    "conf_mat = confusion_matrix(y_valid,np.round(valid_pred),labels=[1,0])\n",
    "print('confusion matrix of valid:',conf_mat)\n",
    "\n",
    "valid_pred = model.predict(X_T)\n",
    "# valid_pred = np.reshape(-1,1)\n",
    "print(valid_pred.shape)\n",
    "# y_valid = np.reshape(-1,1)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_T, valid_pred, pos_label=1)\n",
    "valid_auc = metrics.auc(fpr, tpr)\n",
    "print('Train AUC:', valid_auc)\n",
    "\n",
    "conf_mat = confusion_matrix(y_T,np.round(valid_pred),labels=[1,0])\n",
    "print('confusion matrix of train:',conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save_weights('model/0.9332_regression_DNN_W.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/0.9332_regression_DNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H_00_count</th>\n",
       "      <th>H_01_count</th>\n",
       "      <th>H_02_count</th>\n",
       "      <th>H_03_count</th>\n",
       "      <th>H_04_count</th>\n",
       "      <th>H_05_count</th>\n",
       "      <th>H_06_count</th>\n",
       "      <th>H_07_count</th>\n",
       "      <th>H_08_count</th>\n",
       "      <th>H_09_count</th>\n",
       "      <th>...</th>\n",
       "      <th>qts_step_max</th>\n",
       "      <th>march_count</th>\n",
       "      <th>april_count</th>\n",
       "      <th>may_count</th>\n",
       "      <th>march_ratio</th>\n",
       "      <th>april_ratio</th>\n",
       "      <th>may_ratio</th>\n",
       "      <th>count</th>\n",
       "      <th>y</th>\n",
       "      <th>train/test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>58990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>134</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>94</td>\n",
       "      <td>...</td>\n",
       "      <td>159445</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>791</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>791</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>54</td>\n",
       "      <td>32</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>31</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>14115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>617</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>617</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>124</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>83936</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>242</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>242</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>98093</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>73044</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>87148</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>338</td>\n",
       "      <td>343</td>\n",
       "      <td>301</td>\n",
       "      <td>232</td>\n",
       "      <td>196</td>\n",
       "      <td>163</td>\n",
       "      <td>179</td>\n",
       "      <td>186</td>\n",
       "      <td>168</td>\n",
       "      <td>164</td>\n",
       "      <td>...</td>\n",
       "      <td>1479</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>103967</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>255</td>\n",
       "      <td>272</td>\n",
       "      <td>242</td>\n",
       "      <td>228</td>\n",
       "      <td>230</td>\n",
       "      <td>231</td>\n",
       "      <td>236</td>\n",
       "      <td>234</td>\n",
       "      <td>244</td>\n",
       "      <td>207</td>\n",
       "      <td>...</td>\n",
       "      <td>6015</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5230</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>68</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>43441</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>147640</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>20972</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>258252</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>320129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>93587</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>74270</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>233</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>233</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>103277</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>73</td>\n",
       "      <td>82</td>\n",
       "      <td>35</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>237107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>590</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>51</td>\n",
       "      <td>57</td>\n",
       "      <td>47</td>\n",
       "      <td>70</td>\n",
       "      <td>67</td>\n",
       "      <td>72</td>\n",
       "      <td>71</td>\n",
       "      <td>43</td>\n",
       "      <td>35</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>17697</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1144</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>74189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>45</td>\n",
       "      <td>39</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>5768</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>81</td>\n",
       "      <td>38</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>176090</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>56</td>\n",
       "      <td>71</td>\n",
       "      <td>48</td>\n",
       "      <td>47</td>\n",
       "      <td>50</td>\n",
       "      <td>53</td>\n",
       "      <td>39</td>\n",
       "      <td>49</td>\n",
       "      <td>44</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>7965</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1118</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>46607</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>128</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>260821</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>108</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>76</td>\n",
       "      <td>66</td>\n",
       "      <td>48</td>\n",
       "      <td>38</td>\n",
       "      <td>27</td>\n",
       "      <td>22</td>\n",
       "      <td>37</td>\n",
       "      <td>39</td>\n",
       "      <td>37</td>\n",
       "      <td>33</td>\n",
       "      <td>...</td>\n",
       "      <td>10673</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1036</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>142878</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>57353</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>92102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81807</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>33091</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>136</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81811</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>14126</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81813</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>...</td>\n",
       "      <td>20998</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>591</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81817</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>70573</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>334</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>334</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81818</th>\n",
       "      <td>29</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "      <td>31</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>35500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>959</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81819</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>58095</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>151</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81823</th>\n",
       "      <td>327</td>\n",
       "      <td>356</td>\n",
       "      <td>298</td>\n",
       "      <td>284</td>\n",
       "      <td>322</td>\n",
       "      <td>351</td>\n",
       "      <td>308</td>\n",
       "      <td>353</td>\n",
       "      <td>261</td>\n",
       "      <td>232</td>\n",
       "      <td>...</td>\n",
       "      <td>2809</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6836</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81824</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2842</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81826</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81833</th>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>...</td>\n",
       "      <td>11116</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>457</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81834</th>\n",
       "      <td>18</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>8882</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>490</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81837</th>\n",
       "      <td>41</td>\n",
       "      <td>53</td>\n",
       "      <td>55</td>\n",
       "      <td>49</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>44</td>\n",
       "      <td>43</td>\n",
       "      <td>36</td>\n",
       "      <td>42</td>\n",
       "      <td>...</td>\n",
       "      <td>18496</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>903</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81839</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>10556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81840</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>96816</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81843</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>86264</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81844</th>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>19760</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>566</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>566</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81845</th>\n",
       "      <td>53</td>\n",
       "      <td>44</td>\n",
       "      <td>29</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>9587</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>740</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81847</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>89268</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81850</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>12090</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81851</th>\n",
       "      <td>73</td>\n",
       "      <td>48</td>\n",
       "      <td>44</td>\n",
       "      <td>42</td>\n",
       "      <td>55</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>51</td>\n",
       "      <td>43</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>4035</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1206</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81859</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>64249</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81865</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>249108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81866</th>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>39038</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>149</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81869</th>\n",
       "      <td>110</td>\n",
       "      <td>75</td>\n",
       "      <td>62</td>\n",
       "      <td>53</td>\n",
       "      <td>43</td>\n",
       "      <td>41</td>\n",
       "      <td>53</td>\n",
       "      <td>40</td>\n",
       "      <td>58</td>\n",
       "      <td>35</td>\n",
       "      <td>...</td>\n",
       "      <td>8978</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1494</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81872</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>76263</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>183</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81875</th>\n",
       "      <td>263</td>\n",
       "      <td>238</td>\n",
       "      <td>218</td>\n",
       "      <td>198</td>\n",
       "      <td>203</td>\n",
       "      <td>224</td>\n",
       "      <td>226</td>\n",
       "      <td>190</td>\n",
       "      <td>202</td>\n",
       "      <td>200</td>\n",
       "      <td>...</td>\n",
       "      <td>2824</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5325</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81877</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>16209</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>237</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81887</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>48</td>\n",
       "      <td>84</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>95272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81888</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>185741</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81893</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>102559</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101</td>\n",
       "      <td>NaN</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29376 rows  444 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       H_00_count  H_01_count  H_02_count  H_03_count  H_04_count  H_05_count  \\\n",
       "0               0           0           0           0           0           2   \n",
       "5               0           0           0           0           0           0   \n",
       "8              54          32          18          18          20          21   \n",
       "15              1           0           1           0          60         124   \n",
       "19              0           0           0           0           0           2   \n",
       "22             21           4           9           0           5           2   \n",
       "26              3           8           1           9          16          12   \n",
       "27            338         343         301         232         196         163   \n",
       "31              0           0           0           0           0           0   \n",
       "34            255         272         242         228         230         231   \n",
       "35              0          28          68          10           8           2   \n",
       "37              3          17           2           1           0           1   \n",
       "39              5           4           6          10          21           5   \n",
       "41              0           0           0           0           0           0   \n",
       "47              1           8           9           2           2           2   \n",
       "52             14           6           4           3           0          14   \n",
       "53              4           0           2           0           0           0   \n",
       "56              5           0           0           0           0           0   \n",
       "59              0           0           0           0           0          29   \n",
       "60             51          57          47          70          67          72   \n",
       "61             13          16           1          15           7           3   \n",
       "62             65          66          45          39          30          32   \n",
       "63             81          38         135           0           7           3   \n",
       "64             56          71          48          47          50          53   \n",
       "65             13           9           6           3           5           6   \n",
       "66              0           0           0           0           0           0   \n",
       "68             76          66          48          38          27          22   \n",
       "69              1           2           3          10           1           1   \n",
       "70              0           4           0           8          12           2   \n",
       "71              2           0           0           4           0          21   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "81807          13           8          17           7           8           5   \n",
       "81811          21          21          27          23          20           8   \n",
       "81813          32          32          29          23          21          33   \n",
       "81817           0           0           0          12           5          19   \n",
       "81818          29          33          22          26          31          24   \n",
       "81819           0           0           0           0           0           0   \n",
       "81823         327         356         298         284         322         351   \n",
       "81824          10           9           0           0           0           0   \n",
       "81826           0           0           0           0           0           0   \n",
       "81833          12          16          13           8          16          13   \n",
       "81834          18          25          12          24          11          15   \n",
       "81837          41          53          55          49          51          51   \n",
       "81839           1           0           0           1           0           0   \n",
       "81840          36          36          19          21           7           7   \n",
       "81843          25           0           9           0           0           0   \n",
       "81844          13          15          15          13          19          11   \n",
       "81845          53          44          29          16          30          33   \n",
       "81847           1           0           7           1           7          18   \n",
       "81850          10           9           8          10           5           2   \n",
       "81851          73          48          44          42          55          44   \n",
       "81859           4           1           4           6           1          17   \n",
       "81865           0           0           0           0           6           6   \n",
       "81866          31          25          14           8           5           3   \n",
       "81869         110          75          62          53          43          41   \n",
       "81872           0          19           0           0          12           0   \n",
       "81875         263         238         218         198         203         224   \n",
       "81877           7          11           7          11          12           8   \n",
       "81887          14           0          30          48          84          23   \n",
       "81888           7           9          11           7           9          17   \n",
       "81893           0           0           6           1           0           0   \n",
       "\n",
       "       H_06_count  H_07_count  H_08_count  H_09_count     ...      \\\n",
       "0              20          25          27          12     ...       \n",
       "5               0           0          39          94     ...       \n",
       "8              31          30          31          28     ...       \n",
       "15             41           1           3           1     ...       \n",
       "19              4           2           9           3     ...       \n",
       "22              0           0           1           2     ...       \n",
       "26              3          14           3           6     ...       \n",
       "27            179         186         168         164     ...       \n",
       "31              1           0           0           0     ...       \n",
       "34            236         234         244         207     ...       \n",
       "35              4           5          14           2     ...       \n",
       "37              3           1           8           6     ...       \n",
       "39             13          16           7          20     ...       \n",
       "41              0           7          14           1     ...       \n",
       "47              0           4          21          10     ...       \n",
       "52             28          25          31          35     ...       \n",
       "53              1           0           3           2     ...       \n",
       "56              8           0           0           0     ...       \n",
       "59             73          82          35          48     ...       \n",
       "60             71          43          35          64     ...       \n",
       "61              2           0           1           0     ...       \n",
       "62             29          31          41          40     ...       \n",
       "63              0           5           1           4     ...       \n",
       "64             39          49          44          54     ...       \n",
       "65              2           3           3           4     ...       \n",
       "66              0           0           0           0     ...       \n",
       "68             37          39          37          33     ...       \n",
       "69              2           1           3           1     ...       \n",
       "70              2           1          14          24     ...       \n",
       "71              3           5           8           8     ...       \n",
       "...           ...         ...         ...         ...     ...       \n",
       "81807           2           3           2           2     ...       \n",
       "81811          24          18          19          34     ...       \n",
       "81813          25          31          20          31     ...       \n",
       "81817          21          14          45          32     ...       \n",
       "81818          28          26          25          65     ...       \n",
       "81819           0           0           0           0     ...       \n",
       "81823         308         353         261         232     ...       \n",
       "81824           0           0           0           0     ...       \n",
       "81826           0           0           0           0     ...       \n",
       "81833          18          14          22          19     ...       \n",
       "81834          12          24          27          25     ...       \n",
       "81837          44          43          36          42     ...       \n",
       "81839           0           0           0           0     ...       \n",
       "81840           5           5          10           6     ...       \n",
       "81843           0           0          42           0     ...       \n",
       "81844          17          34          28          26     ...       \n",
       "81845          28          29          28          41     ...       \n",
       "81847           1          38           2          15     ...       \n",
       "81850           9          11          14          13     ...       \n",
       "81851          44          51          43          53     ...       \n",
       "81859           7          22           2          11     ...       \n",
       "81865          24          20           5          12     ...       \n",
       "81866           4           4           2           1     ...       \n",
       "81869          53          40          58          35     ...       \n",
       "81872           0           0           0          10     ...       \n",
       "81875         226         190         202         200     ...       \n",
       "81877           7          12          10          13     ...       \n",
       "81887          24           4          33          62     ...       \n",
       "81888           5           3           3           9     ...       \n",
       "81893           0          11          18           0     ...       \n",
       "\n",
       "       qts_step_max  march_count  april_count  may_count  march_ratio  \\\n",
       "0             58990            0            0        134          0.0   \n",
       "5            159445            0            0        791          0.0   \n",
       "8             14115            0            0        617          0.0   \n",
       "15            83936            0            0        242          0.0   \n",
       "19            98093            0            0         73          0.0   \n",
       "22            73044            0            0         70          0.0   \n",
       "26            87148            0            0        100          0.0   \n",
       "27             1479            0            0       5800          0.0   \n",
       "31           103967            0            0         49          0.0   \n",
       "34             6015            0            0       5230          0.0   \n",
       "35            43441            0            0        163          0.0   \n",
       "37           147640            0            0         44          0.0   \n",
       "39            20972            0            0        299          0.0   \n",
       "41           258252            0            0         22          0.0   \n",
       "47           320129            0            0         60          0.0   \n",
       "52            93587            0            0        206          0.0   \n",
       "53            74270            0            0        233          0.0   \n",
       "56           103277            0            0         29          0.0   \n",
       "59           237107            0            0        590          0.0   \n",
       "60            17697            0            0       1144          0.0   \n",
       "61            74189            0            0         64          0.0   \n",
       "62             5768            0            0       1087          0.0   \n",
       "63           176090            0            0        280          0.0   \n",
       "64             7965            0            0       1118          0.0   \n",
       "65            46607            0            0        128          0.0   \n",
       "66           260821            0            0        108          0.0   \n",
       "68            10673            0            0       1036          0.0   \n",
       "69           142878            0            0         40          0.0   \n",
       "70            57353            0            0        156          0.0   \n",
       "71            92102            0            0         77          0.0   \n",
       "...             ...          ...          ...        ...          ...   \n",
       "81807         33091            0            0        136          0.0   \n",
       "81811         14126            0            0        697          0.0   \n",
       "81813         20998            0            0        591          0.0   \n",
       "81817         70573            0            0        334          0.0   \n",
       "81818         35500            0            0        959          0.0   \n",
       "81819         58095            0            0        151          0.0   \n",
       "81823          2809            0            0       6836          0.0   \n",
       "81824          2842            0            0         23          0.0   \n",
       "81826             3            0            0         39          0.0   \n",
       "81833         11116            0            0        457          0.0   \n",
       "81834          8882            0            0        490          0.0   \n",
       "81837         18496            0            0        903          0.0   \n",
       "81839         10556            0            0          2          0.0   \n",
       "81840         96816            0            0        167          0.0   \n",
       "81843         86264            0            0         76          0.0   \n",
       "81844         19760            0            0        566          0.0   \n",
       "81845          9587            0            0        740          0.0   \n",
       "81847         89268            0            0        129          0.0   \n",
       "81850         12090            0            0        250          0.0   \n",
       "81851          4035            0            0       1206          0.0   \n",
       "81859         64249            0            0        147          0.0   \n",
       "81865        249108            0            0         94          0.0   \n",
       "81866         39038            0            0        149          0.0   \n",
       "81869          8978            0            0       1494          0.0   \n",
       "81872         76263            0            0        183          0.0   \n",
       "81875          2824            0            0       5325          0.0   \n",
       "81877         16209            0            0        237          0.0   \n",
       "81887         95272            0            0        460          0.0   \n",
       "81888        185741            0            0        126          0.0   \n",
       "81893        102559            0            0        101          0.0   \n",
       "\n",
       "       april_ratio  may_ratio  count   y  train/test  \n",
       "0              0.0        1.0    134 NaN        test  \n",
       "5              0.0        1.0    791 NaN        test  \n",
       "8              0.0        1.0    617 NaN        test  \n",
       "15             0.0        1.0    242 NaN        test  \n",
       "19             0.0        1.0     73 NaN        test  \n",
       "22             0.0        1.0     70 NaN        test  \n",
       "26             0.0        1.0    100 NaN        test  \n",
       "27             0.0        1.0   5800 NaN        test  \n",
       "31             0.0        1.0     49 NaN        test  \n",
       "34             0.0        1.0   5230 NaN        test  \n",
       "35             0.0        1.0    163 NaN        test  \n",
       "37             0.0        1.0     44 NaN        test  \n",
       "39             0.0        1.0    299 NaN        test  \n",
       "41             0.0        1.0     22 NaN        test  \n",
       "47             0.0        1.0     60 NaN        test  \n",
       "52             0.0        1.0    206 NaN        test  \n",
       "53             0.0        1.0    233 NaN        test  \n",
       "56             0.0        1.0     29 NaN        test  \n",
       "59             0.0        1.0    590 NaN        test  \n",
       "60             0.0        1.0   1144 NaN        test  \n",
       "61             0.0        1.0     64 NaN        test  \n",
       "62             0.0        1.0   1087 NaN        test  \n",
       "63             0.0        1.0    280 NaN        test  \n",
       "64             0.0        1.0   1118 NaN        test  \n",
       "65             0.0        1.0    128 NaN        test  \n",
       "66             0.0        1.0    108 NaN        test  \n",
       "68             0.0        1.0   1036 NaN        test  \n",
       "69             0.0        1.0     40 NaN        test  \n",
       "70             0.0        1.0    156 NaN        test  \n",
       "71             0.0        1.0     77 NaN        test  \n",
       "...            ...        ...    ...  ..         ...  \n",
       "81807          0.0        1.0    136 NaN        test  \n",
       "81811          0.0        1.0    697 NaN        test  \n",
       "81813          0.0        1.0    591 NaN        test  \n",
       "81817          0.0        1.0    334 NaN        test  \n",
       "81818          0.0        1.0    959 NaN        test  \n",
       "81819          0.0        1.0    151 NaN        test  \n",
       "81823          0.0        1.0   6836 NaN        test  \n",
       "81824          0.0        1.0     23 NaN        test  \n",
       "81826          0.0        1.0     39 NaN        test  \n",
       "81833          0.0        1.0    457 NaN        test  \n",
       "81834          0.0        1.0    490 NaN        test  \n",
       "81837          0.0        1.0    903 NaN        test  \n",
       "81839          0.0        1.0      2 NaN        test  \n",
       "81840          0.0        1.0    167 NaN        test  \n",
       "81843          0.0        1.0     76 NaN        test  \n",
       "81844          0.0        1.0    566 NaN        test  \n",
       "81845          0.0        1.0    740 NaN        test  \n",
       "81847          0.0        1.0    129 NaN        test  \n",
       "81850          0.0        1.0    250 NaN        test  \n",
       "81851          0.0        1.0   1206 NaN        test  \n",
       "81859          0.0        1.0    147 NaN        test  \n",
       "81865          0.0        1.0     94 NaN        test  \n",
       "81866          0.0        1.0    149 NaN        test  \n",
       "81869          0.0        1.0   1494 NaN        test  \n",
       "81872          0.0        1.0    183 NaN        test  \n",
       "81875          0.0        1.0   5325 NaN        test  \n",
       "81877          0.0        1.0    237 NaN        test  \n",
       "81887          0.0        1.0    460 NaN        test  \n",
       "81888          0.0        1.0    126 NaN        test  \n",
       "81893          0.0        1.0    101 NaN        test  \n",
       "\n",
       "[29376 rows x 444 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/output.csv')\n",
    "df = df[df['train/test'] == 'test' ]\n",
    "df = df.drop(columns='fid')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52518, 2)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51934886],\n",
       "       [0.51934886],\n",
       "       [0.51934886],\n",
       "       ...,\n",
       "       [0.51934886],\n",
       "       [0.51934886],\n",
       "       [0.51934886]], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_206 (Conv1D)          (None, 1, 16)             14160     \n",
      "_________________________________________________________________\n",
      "conv1d_207 (Conv1D)          (None, 1, 32)             1056      \n",
      "_________________________________________________________________\n",
      "conv1d_208 (Conv1D)          (None, 1, 32)             2080      \n",
      "_________________________________________________________________\n",
      "conv1d_209 (Conv1D)          (None, 1, 64)             4160      \n",
      "_________________________________________________________________\n",
      "conv1d_210 (Conv1D)          (None, 1, 64)             8256      \n",
      "_________________________________________________________________\n",
      "flatten_31 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_207 (Dense)            (None, 2048)              133120    \n",
      "_________________________________________________________________\n",
      "dense_208 (Dense)            (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_209 (Dense)            (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_28 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_210 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_211 (Dense)            (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_212 (Dense)            (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,950,161\n",
      "Trainable params: 2,950,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Conv1D(16,2,padding='same',input_shape=(1,442)))\n",
    "model.add(Conv1D(32,2,padding='same'))\n",
    "# model.add(MaxPooling1D(1))\n",
    "model.add(Conv1D(32,2,padding='same'))\n",
    "model.add(Conv1D(64,2,padding='same'))\n",
    "model.add(Conv1D(64,2,padding='same'))\n",
    "# model.add(MaxPooling1D(11))\n",
    "# model.add(GlobalAveragePooling1D())\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(2048,input_shape=(1,97)))\n",
    "model.add(Dense(2048,activation='relu'))\n",
    "# model.add(LeakyReLU())\n",
    "# model.add(PReLU(alpha_initializer='zero', weights=None))\n",
    "model.add(Dense(1024))\n",
    "# model.add(LeakyReLU())\n",
    "# model.add(Dense(1024,activation='selu'))\n",
    "model.add(Dense(512))\n",
    "model.add(LeakyReLU())\n",
    "# model.add(Dense(512,activation='selu'))\n",
    "model.add(Dense(256))\n",
    "# model.add(LeakyReLU())\n",
    "# model.add(Dense(256,activation='selu'))\n",
    "model.add(Dense(128))\n",
    "# model.add(LeakyReLU())\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) #binary_crossentropy\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regression model relu + mse + mae + rmsprp =>valid mae=0.1067  seluleakyRelu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classifcation softmax+binary_crossentropy+accuracy+adam => valid ACC=0.8933"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " => sigmoid + binary_crossentropy (regression) =>1.6818/0.8957\n",
    "\n",
    " => softmax + categorical_crossentropy () => 1.7201/0.8933\n",
    "\n",
    "optimizer  Adam  Rsprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "valid append"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test data predict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
